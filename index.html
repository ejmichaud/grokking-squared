<!DOCTYPE html>
<html>

<!----------------------------------------------------------------

    "There is a remarkably close parallel between the problems of 
    the physicist and those of the cryptographer. The system on which 
    a message is enciphered corresponds to the laws of the universe, 
    the intercepted messages to the evidence available, the keys for 
    a day or a message to important constants which have to be 
    determined. The correspondence is very close, but the subject 
    matter of cryptography is very easily dealt with by discrete 
    machinery, physics not so easily."
        - Alan Turing

    ... or is it?
----------------------------------------------------------------->


<head>
    <title>An Analysis of Grokking</title>
    
    <!--------------------------------------------------
                Import and Define CSS Things!
    ---------------------------------------------------->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- <link rel="stylesheet" href="/assets/style.css"> -->
    <!-- <link rel="stylesheet" href="/assets/navbar-and-footer.css"> -->
    <link rel="stylesheet" href="assets/structure.css">

    <link rel="apple-touch-icon" sizes="180x180" href="assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="assetes/favicon/site.webmanifest">
    
    <!--------------------------------------------------
        KaTeX - Render LaTeX Expressions in Document
        https://github.com/Khan/KaTeX/blob/master/contrib/auto-render/README.md
    ---------------------------------------------------->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css" integrity="sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei" crossorigin="anonymous">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js" integrity="sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/contrib/auto-render.min.js" integrity="sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body,
            {
              delimiters: [
                  {left: "$$", right: "$$", display: true},
                  {left: "\\[", right: "\\]", display: true},
                  {left: "$", right: "$", display: false},
                  {left: "\\(", right: "\\)", display: false}
              ]
            }
      );
    });
    </script>
</head>

<body>


<!------------------------
      Webpage Content
(insert in middle of body)
-------------------------->
<div class="content"> 

<h1>Understanding <i>grokking</i> in terms of representation learning dynamics <div style=color:red;>[in progress]</div></h1>

<subtitle>
    This post is based on the preprint <a href="https://arxiv.org/abs/2205.10343">Towards Understanding 
        Grokking: An Effective Theory of Representation Learning</a>, by <a href="https://kindxiaoming.github.io/">Ziming
            Liu</a>, <a href="https://okitouni.github.io/">Ouail Kitouni</a>, <a href="https://nolte.dev/">Niklas Nolte</a>, 
            <a href="https://ericjmichaud.com">Eric J. Michaud</a>, <a href="https://space.mit.edu/home/tegmark/">Max Tegmark</a>, 
            and <a href="https://physics.mit.edu/faculty/michael-williams/">Mike Williams</a>. It aims to be a bit easier to read
            than the full paper and also includes some videos and additional discussion. It was written by Eric.<br><br>

            <!-- <b>Epistemic Status</b>:  -->


            You are seeing v0 of this post, last updated 2022-06-17. It could change over time as we continue to think about grokking.
</subtitle>

<p>
Last year, some researchers at OpenAI released a short paper called 
    <a href="https://arxiv.org/abs/2201.02177">Grokking: Generalization Beyond Overfitting on 
    Small Algorithmic Datasets</a>. In this paper, they documented a curious phenomenon where 
    their neural networks would generalize long <i>after</i> overfitting their training dataset.
    Typically, when training neural networks, performance on the training dataset and validation dataset 
    improve together early in training, and if you continue training past a certain point the model will overfit the 
    training data and its performance on the validation data will decay. The OpenAI paper showed that
    the opposite can sometimes happen. Neural networks, in certain settings, can memorize their training dataset <i>first</i>,
    and then only much later "grok" the task, generalizing late in training. Here is a key 
    figure from the original paper, showing the prototypical grokking learning curve shape:
    
    <br><br>
    <img src="assets/figures/originalgrok.png">
    <figcaption>From Figure 1 of Power et al.: <a href="https://arxiv.org/abs/2201.02177">https://arxiv.org/abs/2201.02177</a>. The model 
    memorizes the training data quickly, but then ~100k steps later begins to generalize, hitting ~100% validation accuracy 
    around a million training steps.</figcaption> 
    <br>

    Grokking is not a universal phenomenon in deep learning. The setup of Power et al. was fairly unusual. Most notably, they studied 
    the problem of learning what they called "algorithmic datasets". By this they just mean learning a binary operation, usually a group operation 
    like modular addition, multiplication, etc. Denoting such a binary operation as "$\circ$", a model takes as input $a, b$ and must output 
    $c$, where $a \circ b = c$. 
    They use a decoder-only transformer which takes as input 
    the six-token seqeunce < a > < $ \circ $ > < b > < = > < ? > < eos > and produces a probability distribution over the tokens the operation
    is defined on ($a, b, c, \ldots$). The embeddings of $a, b, \ldots$ are trainable. The model is trained on a 
    subset of the pairs $((a, b), c)$ of the binary operation 
    table. Evidently, this setup exhibits some atypical learning dynamics.

    <br><br>

    This paper captured people's attention for a few reasons.
    First, it simply adds to the existing constellation of surprising facts about 
    deep learning generalization
    (<a href="https://arxiv.org/abs/1912.02292">double descent</a> being another).
    Second, it may give some practitioners the (probably false) hope that if their network is struggling to learn, maybe they can just keep 
    training and the network will magically "grok" and perform well. Third, grokking is an example of an <i>unexpected model capability gain</i>. 
    It seems that <a href="https://www.lesswrong.com/posts/Lp4Q9kSGsJHLfoHX3/more-is-different-for-ai">More is Different for AI</a>.
    With large language models, for instance, it is difficult to predict how performance on downstream tasks will improve as they are scaled up –
    large improvements sometimes occur suddenly when models hit a certain size. In the case of grokking, neural network performance can 
    unexpectedly improve, not as models are scaled up, but rather as they are trained for longer. These unexpected model capability gains 
    are interesting from an AI safety perspective since they suggest that it may be difficult to anticipate the properties of 
    future models, including their potential for harm. Understanding these phase transitions in model behavior could prove 
    important for safety. 
    <br><br>
    
    
    What should we aim to understand about grokking -- generalization, beyond overfitting, on algoritihmic datasets? Here are a 
    few key questions:
    <ol>
        <li>"Algorithmic datasets" are weird. <b>How do models generalize on them at all?</b> If you point out some dogs to a human
            child, pretty soon they can "generalize" and identify other dogs. But if you showed a child half the entries of 
            some binary op table, would you expect them to be able to "generalize" and fill in the other half? If the table was 
            for something familiar like addition or multiplication, they might recognize the pattern. But our networks have no 
            pre-existing knowledge about these operations. So it just seems bizarre that they would generalize at all. What even is 
            the <i>right</i> way to fill in missing entries in a binary op table, without knowing what operation it is? Maybe the only
            option is to evoke Kolmogorov complexity and identify the computationally simplest operation that matches the entries that
             are given to you (training data)? How do neural networks solve this problem?
        </li>
        <li>
            In their paper on grokking, Power et al. observed that as they reduced the size of the training dataset, the time it took 
            before their models grokked (generalized) increased dramatically. This critical training data fraction was somewhere between 30-40%
            of the full table. <b>What is behind this dependence on training data fraction. </b>
        </li>
        <li>
            Why do neural networks <i>first</i> memorize their training dataset and only later generalize? <b>Why don't they generalize early,
            as is usually the case?</b>
        </li>
    </ol>
    In our paper <a href="https://arxiv.org/abs/2205.10343">Towards Understanding Grokking: An Effective Theory of Representation Learning
    </a>, we sought answers to these questions. We approach the problem via an analysis of representation learning. 
    I don't think that we fully answer the three questions, but I do think that we have both (1) some interesting empirical observations and 
    (2) an interesting style of theoretical analysis. 
    
    <!-- Our explanations center around an analysis of representation learning. I don't think 
    we fully answer the questions I've listed above. Indeed our work is still fairly preliminary, is limited in scope, and definitely 
    won't be the final word on grokking.  -->
    <!-- This work is still 
    fairly preliminary. It could be wrong in important ways, and is definitely limited in scope -- it is certainly not the final word 
    on grokking. The formatting and style constraints of the paper also made it not the clearest possible articulation of our ideas. The 
    explanations that follow will hopefully be clarifying. -->

    <!-- Our analysis is still 
    fairly preliminary, and could be wrong in important ways. It also is somewhat limited in scope, and definitely doesn't explain 
    everything one might want to understand about grokking. But we thought it was ready to be shared and critiqued.  -->

    <h2>Structured Representations Associated with Generalization</h2>
    
    We begin with an empirical observation. With an architecture similar to what Power et al. used<sup><a href="#fn1" id="ref1">1</a></sup>, we 
    train on the task of modular addition ($p = 59$) and observe grokking. Across training, we do a PCA on the token embeddings and visualize their first 
    two principle components.<sup><a href="#fn2" id="ref2">2</a></sup> See the figure below:

    <br><br>
    <img src="assets/figures/ringplot.jpg" id="fullwidth">
    <figcaption>Figure 1 of <a href="https://arxiv.org/abs/2205.10343">our paper</a>, visualizing how embeddings change over training. The task here is modular addition ($p = 59$). Here 
        we show the embeddings simply projected onto the first two principle components. It appears that a distinct ring shape is learned 
        late in training.
    </figcaption>
    <br>

    We see something extremely cool when we do this: generalization is associated with a distinctive ring 
    structure in the embeddings. Furthermore, the embeddings are very precisely ordered along the ring, looping back to 0 after 58. 
    Here is a video showing how the embeddings change throughout training:
    
    <!-- generalizes a bit before the embeddings arrange
    themselves in a tight ring, but does seem to happen as the embeddings  -->
    
    <br><br>
    <video controls>Your browser does not support the &lt;video&gt; tag.
        <source src="assets/figures/modular-addition59-deep_pca12.mp4"/>
    </video>
    <figcaption>
        Visualization of first two principle components of learned embeddings throughout training. In this video, we do the PCA at each step. The
        sudden jumps are an artifact of this and not discontinuous changes in the embeddings themselves. 
    </figcaption>
    <br>

    Note that the model hits 100% validation accuracy at around step 3700. Right before this, there seems to be some quick movement where 
    the embeddings are repelled from the center into a loose ring. At the instant when the model fully generalizes, 
    there seems to be a global ordering of the embeddings but more locally they do 
    not seem to be ordered very precisely. The ring becomes more precise, and the embeddings become fully ordered, 
    if one continues training for a while. Instead of doing a PCA at each 
    step, it is useful to visualize the embeddings along the top two components <i>computed at the end of training.</i>:
    
    <br><br>
    <video controls>Your browser does not support the &lt;video&gt; tag.
        <source src="assets/figures/modular-addition59-deep_pca12_fixed.mp4"/>
    </video>
    <figcaption>
        Visualization of first two principle components of learned embeddings throughout training. In this video, we compute the PCA at the very
        end of training and use these components to visualize the embeddings throughout training. 
    </figcaption>
    <br>

    It appears that the two principle components at the end of training are those for which the randomly-initialized embeddings 
    were already globally somewhat ordered. From this video, it appears that generalization occurs when the embeddings 
    correctly order themselves additionally at the local level. If you track how "19" moves throughout training, for instance, you see that 
    at initialization it is positioned after (counterclockwise) 20, 22, arguably 25, and on top of 23. When the network has generalized, 19 
    is "correctly" positioned between 18 and 20. Not all the embeddings appear to be ordered correctly at the moment of full generalization (see 
    4, 10, 17, 33, 42, and 44), though if one continues to train for longer they appear to become fully ordered.
    <br><br>

    It is interesting how this circular, ordered layout resembles how we humans visualize modular addition as "clock math". 
    However, different orderings are sometimes learned with different seeds (e.g. consecutive embeddings might be spaced about 1/4th of
     the circle apart instead of 1/59th), so we should be careful about drawing too much from this resemblance. 
    <u>What we do draw from these experiments is that there appears to be a connection between generalization and the embeddings arranging
    themselves in some structured way.</u> We do not claim that this structure is fully captured by our 2-dimensional PCA plots (the 
    fact that the network first generalizes when a few embeddings are still "out of order" suggests that other dimensions are at play, or that the PCA 
    axes are not quite the most relevant projection), though we do find them very suggestive...
    <br><br>

    <h2>Our Theory (speculative, a bit beyond the scope of the paper)</h2>

    For any given feedforward neural network, the "early layers" and the "late layers" are 
    engaged in a kind of cooperative game. The "early layers" transform the network's input into some representation,
    and the "late layers" perform some computation on this representation to produce the network's output. The job of the "early layers"
    is to service the "late layers" with a usable representation. The job of the "late layers" is to compute the correct output from 
    this representation -- to not bungle the good work that the "early layers" have done in giving them a workable representation!
    <!-- the "early layers" have given them and to compute the correct output from the representation.  -->
    <br><br>
    
    For <i>algorithmic datasets</i>,
    we conjecture that there are special, structured representations that the "early layers" can learn which allow the "late layers" to
    internally implement an operation isomorphic to the "algorithmic" operation itself (modular addition, multiplication, etc.). 
    Thus on algorithmic datasets, the setting of grokking, the reason why the network 
    is able to generalize at all is because internally it is performing something akin to the true target operation. It is not just "fitting"
    the dataset, but rather its internal structure corresponds in some way to the process that generated the data. 
    <!-- computation isomorphic to the binary operation  -->
    <!-- that the whole network is tasked with learning.  -->
    <!-- But doing this requires really special representations of inputs to be learned by the "early layers". -->
    <br><br>

    There is a difficult coordination problem that the "early layers" and the "late layers" have to solve for this special 
    internal operation to be learned. In particular, if the "late layers" learn much faster than the "early layers", then they will 
    quickly fit bad, random, approximately static representations (given by the "early layers" at initialization), resulting in overfitting. 
    On the other hand, if the "early layers" learn much faster than the "late layers", then they will quickly find (weird) representations 
    which when thrown through the bad, random, approximately static "later layers" will produce the desired outputs (inverting the "later layers").
     This will also result in 
    overfitting.
    
    <!-- If
    the "late layers" learn much slower than the "early layers", then the "early layers" will try to quickly find representations 

    
    which
    when inputed into the bad, approximately static "late layers" -->
    <br><br> 
    <!-- But if they coordinate well, then generalization can happen at about the same time the network "fits" the training data.  -->
    Generalization requires that the "early layers" and "late layers" coordinate well. 
    In the case of grokking, there is a coordination failure at first. The network learns unstructured representations and fits them. But it 
    can't do this perfectly (the training loss is not exactly zero), giving some training signal for the "early layers" and "late layers"
    to work with. We suggest that the "later layers" will be less complex, and able to achieve lower loss, if they learn to perform something 
    akin to the underlying binary operation. The reason is that, if some part of the network internally implements the binary operation, then the 
    downstream layers need only fit $p$ representations instead of $\mathcal{O}(p^2)$ points. Regularization schemes like weight decay, reported to be helpful
    in producing generalization in the original grokking paper, provide a training signal towards lower-complexity "late layers" which 
    perform the target operation internally.
<!--     
    "late layers" first fit random representations from the "early layers". But they can't do 
    this perfectly (the training loss is not exactly zero), giving some training signal for the "early layers" and "late layers"
    to work with. We suggest that the "later layers" will be less complex, and able to achieve lower loss, if they learn to perform something 
    akin to the underlying binary operation. The reason is that, if some part of the network internally implements the binary operation, then the rest 
    of the network only needs to fit $p$ representations instead of $\mathcal{O}(p^2)$ points. Regularization schemes like weight decay, reported to be helpful
    in producing generalization in the original grokking paper, provide an additional training signal towards lower-complexity "late layers" which 
    perform the operation.  -->
    <br><br>

    Theoretically, there are as many ways of arbitrarily dividing a network into "early layers" and "late layers" as there are 
    layers in the network. For the Transformer setup, we consider the learned embeddings to be the "early layers" and the 
    whole decoder to be the "late layers". 
    Based on the picture presented above, we should expect that the decoder learning rate, relative to the learning rate that the 
    embeddings are trained with, and the decoder regularization (weight decay here) should control grokking. Indeed, doing a grid search 
    over decoder learning rate and weight decay, we find a variety of learning behaviors, with grokking occurring within a certain 
    strip of learning rates. 

    <br><br>
    <img src="assets/figures/phasediagram_transformer.jpg">
    <figcaption>From Figure 6 of <a href="https://arxiv.org/abs/2205.10343">our paper</a>, a <i>phase diagram</i> of learning dynamics.
        <b>Memorization</b> means the network overfits but doesn't generalize (within $10^5$ steps), <b>confusion</b> means the network 
        neither overfit nor generalized, <b>comprehension</b> means that the network generalized within 1000 steps of fitting the training set,
        <b>grokking</b> means the network generalized at least 1000 steps after fitting the training set. See Table 1 of our paper for full definitions. 
    </figcaption>
    <br>

    <h2>A Toy Model and Effective Theory</h2>

    The basic picture we've laid out is that maybe models (1) find a good representation of inputs (2) internally perform something akin to 
    the target binary operation with these representaitons and (3) map the result to the desired output. It would be cool to mechanistically
    understand our models to check this, though we haven't done this yet. Instead, we developed a toy model capturing this basic structure and 
    studied its properties. 
    <br><br>

    We consider learning the binary operation of addition (not modulo anything -- it's not closed, so if inputs are from $0, \ldots, p-1$, 
    outputs are from $0, \ldots, 2p-2$). Our toy model takes as input the symbols $a, b$, 
    maps them to (trainable) embeddings $\bm{E}_a, \bm{E}_b \in \mathbb{R}^{d_{\text{in}}}$, adds these together, then maps the result to targets with an MLP decoder:
    $$ (a, b) \mapsto {\rm Dec}(\bm{E}_a + \bm{E}_b). $$
    
    This kind of toy model is nice since we see that generalization comes directly from learning structured representations. 
    In particular, if the model learns to place the embeddings
    $\bm{E}_0, \bm{E}_1, \bm{E}_2, \ldots, \bm{E}_{p-1}$ evenly spaced along a single line, i.e. $\bm{E}_k = \bm{v} + k \bm{w}$ for any 
    $\bm{v}, \bm{w} \in \mathbb{R}^{d_\text{in}}$, then if the model achieves zero error on training sample $((i, j), i+j)$, it will 
    generalize to any other sample $((m, n), m + n)$ for which $m + n = i + j$. This is because the input to the decoder will be 
    the same for these two samples. To say it again: in this toy model, generalization comes directly from learning structured representations.
    <br><br>

    What do the learning dynamics of the embeddings $\bm{E}_{*}$ look like for this toy model? What determines whether the model 
    learns to arrange the $\bm{E}_{*}$ along a line? We develop an effective theory for the learning dynamics of embeddings. 
    The way that the $\bm{E}_{*}$ evolve over training, in practice, will depend on the decoder and the relative learning rate 
    between the $\bm{E}_{*}$ and ${\rm Dec}$, as well as other optimization hyperparameters and regularization. 
    As discussed earlier, if the decoder learning rate is much faster than the learning 
    rate for the embeddings, the model will likely overfit to the embeddings at initialization. Unfortunately, in the effective 
    theory presented in this first paper, we have not incorporated any facts about the decoder into our effective theory. This 
    means that we can't yet analytically predict how learning hyperparameters determine grokking -- we can't 
    compute the phase diagram of learning behavior, shown above, analytically. However, our effective theory does seem to 
    predict the dependence of grokking on the training set size, question #2 mentioned earlier.
    <br><br>
    
    We model the learning dynamics of the $\bm{E}_{*}$ as evolving under an <i>effective loss</i> $\ell_\text{eff}$. This effective loss 
    basically measures how well/poorly the embeddings are arranged along a line. We define it as:
    $$ \ell_\text{eff} = \frac{1}{Z_0} \sum_{(i, j, m, n) \in P_0(D)} |(\bm{E}_i + \bm{E}_j) - (\bm{E}_m + \bm{E}_n)|^2 $$
    where $Z_0 = \sum_k |\bm{E}_k|^2 $, and $P_0(D)$ is defined as:
    $$ 
        P_0(D) = \{ (i, j, m, n) :
             (i, j), (m, n) \in D,  i + j = m + n \} 
       
    $$
    where $D$ is the training dataset. $P_0(D)$ consists of pairs of elements in the training dataset which have the same sum. 
    We see that if the $\bm{E}_{*}$ are evenly-spaced on a line, then $\bm{E}_i + \bm{E}_j = \bm{E}_m + \bm{E}_n$ for
    $(i, j, m, n) \in P_0(D)$, and thus $\ell_\text{eff} = 0$. The learning dynamics under this loss are:
    
    $$ \frac{d\bm{E}_i}{dt} = -\eta \frac{\partial \ell_\text{eff}}{\partial \bm{E}_i} $$
    
    First, we find that, despite all simplifications and assumptions of the effective theory, that the dynamics of 
    the $\bm{E}_{*}$ (or rather normalized versions of them) under the effective loss seems fairly similar to the dynamics 
    under real training with a decoder. Below we show a comparison of the two dynamics for 1D embeddings $d_\text{in} = 1$
   
    <br><br>
    <img src="assets/figures/emb1dtimeseriescompare.jpg">
    <figcaption>From Figure 4 of <a href="https://arxiv.org/abs/2205.10343">our paper</a>, comparing the learning dynamics of 
        the $\bm{E}_{*}$ for the effective theory vs true dynamics where the loss is backpropagated through some decoder
        which is itself changing over time.
    </figcaption>
    <br>

    Second, when we vary the size of the training dataset $D$, we find that the effective theory predicts the likelihood that 
    the model will generalize -- that it will learn to arrange the embeddings along a line -- decreases rapidly once you go below 
    a train set fraction of 0.4:

    <br><br>
    <img src="assets/figures/probofgeneralizingcompare.jpg">
    <figcaption>From Figure 4 of <a href="https://arxiv.org/abs/2205.10343">our paper</a>, comparing the empirical 
        probability of generalizing (for the toy model) for real training runs vs under the dynamics 
        of the effective loss. 
    </figcaption>
    <br>
    
    Admittedly, what we are more interested in is the <i>time</i> that it takes to generalize, not the <i>probability</i> of 
    generalizing at all. I won't give the full argument here, but from our effective theory one can perform an analysis 
    of the eigenvalues of a certain matrix $H$ which governs the dynamics, and find that the first nonzero eigenvalue
    $\lambda_3$ corresponds to the speed at which learning happens, and that $\lambda_3$ is quite sensitive to the train 
    set fraction.
    
    <h2>Related Work</h2>

    Some writers had made informal posts with conjectures about grokking before our paper. Beren Millidge had 
    <a href="https://beren.io/2022-01-11-Grokking-Grokking/">an interesting post</a> where he discussed learning, during 
    overfitting, as a random walk on an "optimal manifold" in parameter space, which eventually hits a region which generalizes. He 
    discusses weight decay as a prior, when one views SGD as MCMC, and makes a connection to Solomonoff induction. Rohin Shah also made 
    some <a href="https://www.alignmentforum.org/posts/zvWqPmQasssaAWkrj/an-159-building-agents-that-know-how-to-experiment-by#DEEP_LEARNING_">
    conjectures in the Alignment Newsletter</a> about how functions which memorize will likely be more complicated and therefore 
    provide worse loss for a given level of expressivitiy than functions which generalize. Then if the training during overfitting is 
    mostly a random walk, once it finds a function which generalizes it will stay there because loss will be lower. <br><br>

    A more involved explanation was offered by Quintin Pope in his LessWrong post <a href="https://www.lesswrong.com/posts/JFibrXBewkSDmixuo/hypothesis-gradient-descent-prefers-general-circuits">
    Hypothesis: gradient descent prefers general circuits</a>. He envisions the training process as one in which "shallow" circuits are slowly
    <i>unified</i> into more "general" circuits which generalize. I find this description to be pretty intuitively appealing, though it has not 
    been backed up yet by a deep mechanistic circuits-style analysis of real networks. Perhaps our observations about structure emerging in 
    the PCA of embeddings will be useful in identifying these general circuits. As discussed earlier, a lot is involved in a network learning 
    a general circuit, one which effectively implements the target operation. A necessary condition seems to be that good representations are 
    learned, and we have modeled this aspect of learning a general circuit, of representation learning, with a simplified effective theory. 
    This allowed us to analytically study the effect of the training data fraction on learning. 
    But we do not yet have a full account of how these general circuits are formed.

   <h2>Taking Stock</h2>
    In terms of our original three questions about grokking, I think that we have a partial answer to (1), a reasonable answer to (2), and 
    a partial answer to (3).<br><br>
    
    In terms of question (1), about why generalization happens at all, I think our observations about 
    structured representations being associated with generalization suggests that the network is doing something very clever internally. 
    It is probably doing something like internally implementing the target operation (otherwise how would the network generalize),
     but we don't yet understand mechanistically how it does this. By the way, embeddings arranged on a ring seem pretty typical 
     for many tasks, but the order changes. Here is non-modular addition, instead of mod 59:

     <br><br>
     <img src="assets/figures/ringplot-addition.png" id="fullwidth">
     <figcaption>Visualization of how embeddings change over training, this time for standard addition rather than modular addition. Numbers within 
         each bundle of the ring often have the same sum. 
     </figcaption>
     <br>
    Interestingly, within each bundle on the ring are numbers belonging to the same "parallelogram". At one o'clock we find 
    the embeddings for 2, 13, 24, 35, 46, and 57, for which 2 + 57 = 13 + 46 = 24 + 35. Pretty neat. 
    <br><br>

    For question (2), we studied a toy model of grokking where the relationship between structured representations and generalization is clear.
    We modeled the learning dynamics of these representations under an effective loss, and found that this model captures the dependence 
    on training set size pretty well. 

    For question (3), we have some heuristic arguments about learning speed between different parts of a network, but I don't feel that 
    we fully understand why grokking appears on algorithmic datasets and not on more standard tasks. It likely has something to do with how 
    a particularly special circuit must be learned for generalization to occur on algorithmic datasets...<br><br>

    Always more to think about!
    
    <!-- with this effective theory of the training dynamics, we can vary  
    
    when we model the dynamics of the embeddings $E_{*}$ under this loss, that they  -->


    <!-- simpler and achieve lower loss.  -->

    <!-- , and hopefully discover some simpler solution where they cooperate and perform the operation internally. Internally
    implementing the operation leads to simpler "later layers" since only $p$ representations need to be fit (representations after the binary 
    operation has been performed internally) instead of $\mathcal{O}(p^2)$.  -->
    
    <!-- solution is 
    simpler for the 
    
    This does not 
    have to happen all at once. 
    
    and eventually the "early layers" produce a representation which 
    can be  -->




    
    <!-- <br><br><br><br>
    Consider partition a feedforward network into two parts -- the "early layers" and the "late layers". At what 
    layer you draw this line is arbitrary, though there are likely more and less interesting ways 
    one can think of the early layers and the late layers as being engaged 
    
    of the parts of the network before 
    this layer and the parts of the network after this layer are engaged in a kind of cooperative game. 
    The first part  -->



    <!-- is that  -->
<!-- 
    <br><br><br><br>
    
    but it may still be suggestive.


    This has 
    a striking resemblance to how we visualize modular addition as "clock math". While this resemblance is perhaps suggestive, 
    one has to be a bit 
    careful in taking it too seriously, since we find that other orderings are sometimes learned with different seeds. These orderings
    are still structured, but have a different spacing (e.g. consecutive embeddings are spaced about 1/4th of the circle apart instead of 1/59th).
 -->

    
    <!-- However, if instead of 
    
    At this stage, the embeddings seem only globally ordered, and not
    locally. There does seem to be a quick and the super precise ring forms if one continues training for much longer. 
    -->


    <!---------------------------------
                FOOTNOTES
    ---------------------------------->
    <div id="bar"></div> 
    <h2>Footnotes</h2>
    
    <sup id="fn1">1. Our transformers have a sequence length of just two, "< a > < b >", rather than having a longer sequence length 
    in which all the other tokens are constant for all samples.<a href="#ref1" title="Jump back to footnote 1 in the text.">↩</a></sup>
    <br>
    <sup id="fn2">2. The original paper on grokking from Power et al. showed a visualization of embeddings with t-SNE,
        but we find the PCA projections to be more interesting. We also track how structure in the embeddings changes over training,
        whereas they just showed embeddings of the trained network.<a href="#ref2" title="Jump back to footnote 2 in the text.">↩</a></sup>
    
    
    
</div>

    
</body>
</html>
