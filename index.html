<!DOCTYPE html>
<html>

<!----------------------------------------------------------------

    "There is a remarkably close parallel between the problems of 
    the physicist and those of the cryptographer. The system on which 
    a message is enciphered corresponds to the laws of the universe, 
    the intercepted messages to the evidence available, the keys for 
    a day or a message to important constants which have to be 
    determined. The correspondence is very close, but the subject 
    matter of cryptography is very easily dealt with by discrete 
    machinery, physics not so easily."
        - Alan Turing

    ... or is it?
----------------------------------------------------------------->


<head>
    <title>Thoughts on Grokking</title>
    
    <!--------------------------------------------------
                Import and Define CSS Things!
    ---------------------------------------------------->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- <link rel="stylesheet" href="/assets/style.css"> -->
    <!-- <link rel="stylesheet" href="/assets/navbar-and-footer.css"> -->
    <link rel="stylesheet" href="assets/structure.css">

    <link rel="apple-touch-icon" sizes="180x180" href="assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="assetes/favicon/site.webmanifest">
    
    <!--------------------------------------------------
        KaTeX - Render LaTeX Expressions in Document
        https://github.com/Khan/KaTeX/blob/master/contrib/auto-render/README.md
    ---------------------------------------------------->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css" integrity="sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei" crossorigin="anonymous">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js" integrity="sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/contrib/auto-render.min.js" integrity="sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body,
            {
              delimiters: [
                  {left: "$$", right: "$$", display: true},
                  {left: "\\[", right: "\\]", display: true},
                  {left: "$", right: "$", display: false},
                  {left: "\\(", right: "\\)", display: false}
              ]
            }
      );
    });
    </script>
</head>

<body>


<!------------------------
      Webpage Content
(insert in middle of body)
-------------------------->
<div class="content"> 

<h1>Understanding <i>grokking</i> in terms of representation learning dynamics <div style=color:red;>[in progress]</div></h1>

<subtitle>
    This post accompanies the preprint <a href="https://arxiv.org/abs/2205.10343">Towards Understanding 
        Grokking: An Effective Theory of Representation Learning</a>, by <a href="https://kindxiaoming.github.io/">Ziming
            Liu</a>, <a href="https://okitouni.github.io/">Ouail Kitouni</a>, <a href="https://nolte.dev/">Niklas Nolte</a>, 
            <a href="https://ericjmichaud.com">Eric J. Michaud</a>, <a href="https://space.mit.edu/home/tegmark/">Max Tegmark</a>, 
            and <a href="https://physics.mit.edu/faculty/michael-williams/">Mike Williams</a>. It aims to be a bit pedagogically
            friendlier than the full paper and also includes some videos and additional discussion. It was written mostly by Eric.<br><br>

            You are seeing v0 of this post, last updated 2022-06-01. It could change considerably over time. 
</subtitle>

<p>
Last year, some researchers at OpenAI released a short paper called 
    <a href="https://arxiv.org/abs/2201.02177">Grokking: Generalization Beyond Overfitting on 
    Small Algorithmic Datasets</a>. In this paper, they documented a curious phenomenon where 
    their neural networks would generalize long <i>after</i> overfitting their training dataset.
    Typically in machine learning, performance on the training dataset and validation dataset 
    improve together early in training, and if you continue training past a certain point the model will overfit the 
    training data and its performance on the validation data will decay. The OpenAI paper showed that
    the opposite can sometimes happen. Neural networks, in certain settings, can memorize their training dataset first,
    and then only much later "grok" the task, generalizing late in training. Here is a key 
    figure from the original paper, showing the prototypical grokking learning curve shape:
    
    <br><br>
    <img src="assets/figures/originalgrok.png">
    <figcaption>From Figure 1 of Power et al.: <a href="https://arxiv.org/abs/2201.02177">https://arxiv.org/abs/2201.02177</a>. The model 
    memorizes the training data quickly, but then ~100k steps later begins to generalize.</figcaption> 
    <br>

    Grokking is not a universal phenomenon in deep learning. The setup of Power et al. was fairly unusual. Most notably, they studied 
    the problem of learning what they called "algorithmic datasets". By this they just mean learning a binary operation, usually a group operation 
    like modular addition, multiplication, etc. Denoting such a binary operation as "$\circ$", a model takes as input $a, b$ and must output 
    $c$, where $a \circ b = c$. 
    They use a decoder-only transformer which takes as input 
    the six-token seqeunce < a > < $ \circ $ > < b > < = > < ? > < eos > and produces a probability distribution over the tokens the operation
    is defined on ($a, b, c, \ldots$). The embeddings of $a, b, \ldots$ are trainable. The model is trained on a 
    subset of the pairs $((a, b), c)$ of the binary operation 
    table. Evidently, this setup exhibits some atypical learning dynamics.

    <br><br>

    This paper captured people's attention for a few reasons.
    First, it simply adds to the existing constellation of surprising facts about 
    deep learning generalization
    (<a href="https://arxiv.org/abs/1912.02292">double descent</a> being another).
    Second, it may give some practitioners the (probably false) hope that if their network is struggling to learn, maybe they can just keep 
    training and the network will magically "grok" and perform well. Third, grokking is an example of an <i>unexpected model capability gain</i>. 
    It seems that <a href="https://www.lesswrong.com/posts/Lp4Q9kSGsJHLfoHX3/more-is-different-for-ai">More is Different for AI</a>.
    With large language models, for instance, it is difficult to predict how performance on downstream tasks will improve as they are scaled up –
    large improvements sometimes occur suddenly when models hit a certain size. In the case of grokking, neural network performance can 
    unexpectedly improve, not as models are scaled up, but rather as they are trained for longer. These unexpected model capability gains 
    are interesting from an AI safety perspective since they suggest that it may be difficult to anticipate the properties of 
    future models, such as their potential for harm. Understanding these phase transitions in model behavior could prove 
    important for safety. 
    <br><br>
    
    
    What should we aim to understand about grokking -- generalization, beyond overfitting, on algoritihmic datasets? Here are a 
    few key questions:
    <ol>
        <li>"Algorithmic datasets" are weird. <b>How do models generalize on them at all?</b> If you point out some dogs to a human
            child, pretty soon they can "generalize" and identify other dogs. But if you showed a child half the entries of 
            some binary op table, would you expect them to be able to "generalize" and fill in the other half? If the table was 
            for something familiar like addition or multiplication, they might recognize the pattern. But our networks have no 
            pre-existing knowledge about these operations. So it just seems bizarre that they would generalize at all. What even is 
            the <i>right</i> way to fill in missing entries in a binary op table, without knowing what operation it is? Maybe the only
            option is to evoke Kolmogorov complexity and identify the computationally simplest operation that matches the entries that
             are given to you (training data)? How do neural networks solve this problem?
        </li>
        <li>
            In their paper on grokking, Power et al. observed that as they reduced the size of the training dataset, the time it took 
            before their models grokked (generalized) increased dramatically. This critical training data fraction was somewhere between 30-40%
            of the full table. <b>What is behind this dependence on training data fraction. </b>
        </li>
        <li>
            Why do neural networks <i>first</i> memorize their training dataset and only later generalize? <b>Why don't they generalize early,
            as is usually the case?</b>
        </li>
    </ol>
    In our paper <a href="https://arxiv.org/abs/2205.10343">Towards Understanding Grokking: An Effective Theory of Representation Learning
    </a>, we sought answers to these questions. Our explanations center around an analysis of representation learning. This work is still 
    fairly preliminary. It could be wrong in important ways, and is definitely limited in scope -- it is certainly not the final word 
    on grokking. The formatting and style constraints of the paper also made it not the clearest possible articulation of our ideas. The 
    explanations that follow will hopefully be clarifying.
    
    <!-- Our analysis is still 
    fairly preliminary, and could be wrong in important ways. It also is somewhat limited in scope, and definitely doesn't explain 
    everything one might want to understand about grokking. But we thought it was ready to be shared and critiqued.  -->

    <h2>Structured Representations Associated with Generalization</h2>
    
    We begin with an empirical observation. With an architecture similar to what Power et al. used<sup><a href="#fn1" id="ref1">1</a></sup>, we 
    train on the task of modular addition ($p = 59$) and observe grokking. Across training, we do a PCA on the token embeddings and visualize their first 
    two principle components.<sup><a href="#fn2" id="ref2">2</a></sup> See the figure below:

    <br><br>
    <img src="assets/figures/ringplot.jpg" id="fullwidth">
    <figcaption>Figure 1 of <a href="https://arxiv.org/abs/2205.10343">our paper</a>, visualizing how embeddings change over training. The task here is modular addition ($p = 59$). Here 
        we show the embeddings simply projected onto the first two principle components. It appears that a distinct ring shape is learned 
        late in training.
    </figcaption>
    <br>

    We see something extremely cool when we do this: generalization is associated with a distinctive ring 
    structure in the embeddings. Furthermore, the embeddings are very precisely ordered along the ring, looping back to 0 after 58. 
    Here is a video showing how the embeddings change throughout training:
    
    <!-- generalizes a bit before the embeddings arrange
    themselves in a tight ring, but does seem to happen as the embeddings  -->
    
    <br><br>
    <video controls>Your browser does not support the &lt;video&gt; tag.
        <source src="assets/figures/modular-addition59-deep_pca12.mp4"/>
    </video>
    <figcaption>
        Visualization of first two principle components of learned embeddings throughout training. In this video, we do the PCA at each step. The
        sudden jumps are an artifact of this and not discontinuous changes in the embeddings themselves. 
    </figcaption>
    <br>

    Note that the model hits 100% validation accuracy at around step 3700. Right before this, there seems to be some quick movement where 
    the embeddings are repelled from the center into a loose ring. At the instant when the model fully generalizes, 
    there seems to be a global ordering of the embeddings but more locally they do 
    not seem to be ordered very precisely. The ring becomes more precise, and the embeddings become fully ordered, 
    if one continues training for a while. Instead of doing a PCA at each 
    step, it is useful to visualize the embeddings along the top two components <i>computed at the end of training.</i>:
    
    <br><br>
    <video controls>Your browser does not support the &lt;video&gt; tag.
        <source src="assets/figures/modular-addition59-deep_pca12_fixed.mp4"/>
    </video>
    <figcaption>
        Visualization of first two principle components of learned embeddings throughout training. In this video, we compute the PCA at the very
        end of training and use these components to visualize the embeddings throughout training. 
    </figcaption>
    <br>

    It appears that the two principle components at the end of training are those for which the randomly-initialized embeddings 
    were already globally somewhat ordered. From this video, it appears that generalization occurs when the embeddings 
    correctly order themselves additionally at the local level. If you track how "19" moves throughout training, for instance, you see that 
    at initialization it is positioned after (counterclockwise) 20, 22, arguably 25, and on top of 23. When the network has generalized, 19 
    is "correctly" positioned between 18 and 20. Not all the embeddings appear to be ordered correctly at the moment of full generalization (see 
    4, 10, 17, 33, 42, and 44), though if one continues to train for longer they appear to become fully ordered.
    <br><br>

    It is interesting how this circular, ordered layout resembles how we humans visualize modular addition as "clock math". 
    However, different orderings are sometimes learned with different seeds (e.g. consecutive embeddings might be spaced about 1/4th of
     the circle apart instead of 1/59th), so we should be careful about drawing too much from this resemblance. 
    <u>What we do draw from these experiments is that there appears to be a connection between generalization and the embeddings arranging
    themselves in some structured way.</u> We do not claim that this structure is fully captured by our 2-dimensional PCA plots (the 
    fact that the network first generalizes when a few embeddings are still "out of order" suggests that other dimensions are at play, or that the PCA 
    axes are not quite the most relevant projection), though we do find them very suggestive...
    <br><br>

    <h2>Our Theory (somewhat speculative)</h2>

    For any given feedforward neural network, the "early layers" and the "late layers" are 
    engaged in a kind of cooperative game. The "early layers" transform the network's input into some representation,
    and the "late layers" perform some computation on this representation to produce the network's output. The job of the "early layers"
    is to service the "late layers" with a usable representation. The job of the "late layers" is to compute the correct output from 
    this representation -- to not bungle the good work that the "early layers" have done in giving them a workable representation!
    <!-- the "early layers" have given them and to compute the correct output from the representation.  -->
    <br><br>
    
    For <i>algorithmic datasets</i>,
    we conjecture that there are special, structured representations that the "early layers" can learn which allow the "late layers" to
    internally implement an operation isomorphic to the "algorithmic" operation itself (modular addition, multiplication, etc.). 
    Thus on algorithmic datasets, the setting of grokking, the reason why the network 
    is able to generalize at all is because internally it is performing something akin to the true target operation. It is not just "fitting"
    the dataset, but rather its internal structure corresponds in some way to the process that generated the data. 
    <!-- computation isomorphic to the binary operation  -->
    <!-- that the whole network is tasked with learning.  -->
    <!-- But doing this requires really special representations of inputs to be learned by the "early layers". -->
    <br><br>

    There is a difficult coordination problem that the "early layers" and the "late layers" have to solve for this special 
    internal operation to be learned. In particular, if the "late layers" learn much faster than the "early layers", then they will 
    quickly fit bad, random, approximately static representations (given by the "early layers" at initialization), resulting in overfitting. 
    On the other hand, if the "early layers" learn much faster than the "late layers", then they will quickly find (weird) representations 
    which when thrown through the bad, random, approximately static "later layers" will produce the desired outputs. This will also result in 
    overfitting.
    
    <!-- If
    the "late layers" learn much slower than the "early layers", then the "early layers" will try to quickly find representations 

    
    which
    when inputed into the bad, approximately static "late layers" -->
    <br><br> 
    <!-- But if they coordinate well, then generalization can happen at about the same time the network "fits" the training data.  -->
    Generalization requires that the "early layers" and "late layers" coordinate well. 
    In the case of grokking, there is a coordination failure at first. The network learns unstructured representations and fits them. But it 
    can't do this perfectly (the training loss is not exactly zero), giving some training signal for the "early layers" and "late layers"
    to work with. We suggest that the "later layers" will be less complex, and able to achieve lower loss, if they learn to perform something 
    akin to the underlying binary operation. The reason is that, if some part of the network internally implements the binary operation, then the 
    downstream layers need only fit $p$ representations instead of $\mathcal{O}(p^2)$ points. Regularization schemes like weight decay, reported to be helpful
    in producing generalization in the original grokking paper, provide a training signal towards lower-complexity "late layers" which 
    perform the target operation internally.
<!--     
    "late layers" first fit random representations from the "early layers". But they can't do 
    this perfectly (the training loss is not exactly zero), giving some training signal for the "early layers" and "late layers"
    to work with. We suggest that the "later layers" will be less complex, and able to achieve lower loss, if they learn to perform something 
    akin to the underlying binary operation. The reason is that, if some part of the network internally implements the binary operation, then the rest 
    of the network only needs to fit $p$ representations instead of $\mathcal{O}(p^2)$ points. Regularization schemes like weight decay, reported to be helpful
    in producing generalization in the original grokking paper, provide an additional training signal towards lower-complexity "late layers" which 
    perform the operation.  -->
    <br><br>

    Theoretically, there are as many ways of arbitrarily dividing a network into "early layers" and "late layers" as there are 
    layers in the network. For the Transformer setup, we consider the learned embeddings to be the "early layers" and the 
    whole decoder to be the "late layers". 
    Based on the picture presented above, we should expect that the decoder learning rate, relative to the learning rate that the 
    embeddings are trained with, and the decoder regularization (weight decay here) should control grokking. Indeed, doing a grid search 
    over decoder learning rate and weight decay, we find a variety of learning behaviors, with grokking occurring within a certain 
    strip of learning rates. 

    <br><br>
    <img src="assets/figures/phasediagram_transformer.jpg">
    <figcaption>From Figure 6 of <a href="https://arxiv.org/abs/2205.10343">our paper</a>, a <i>phase diagram</i> of learning dynamics.
        <b>Memorization</b> means the network overfits but doesn't generalize (within $10^5$ steps), <b>confusion</b> means the network 
        neither overfit nor generalized, <b>comprehension</b> means that the network generalized within 1000 steps of fitting the training set,
        <b>grokking</b> means the network generalized at least 1000 steps after fitting the training set. See Table 1 of our paper for full definitions. 
    </figcaption>
    <br>

    <h2>A Toy Model and Effective Theory</h2>

    The basic picture we've laid out is that models (1) find a good representation of inputs (2) internally perform something akin to 
    the target binary operation with these representaitons and (3) fit the result to the desired output. It would be cool to mechanistically
    understand our models to check this, but we haven't done this yet. Instead, we developed a toy model capturing this basic structure and 
    studied its properties. 
    <br><br>

    We consider learning the binary operation of addition (not modulo anything -- it's not closed, so if inputs are from $0, \ldots, p-1$, 
    outputs are from $0, \ldots, 2p-2$). Our toy model takes as input the symbols $a, b$, 
    maps them to (trainable) embeddings $\bm{E}_a, \bm{E}_b \in \mathbb{R}^{d_{\text{in}}}$, adds these together, then maps the result to targets with an MLP decoder:
    $$ (a, b) \mapsto {\rm Dec}(\bm{E}_a + \bm{E}_b). $$
    
    This kind of toy model is nice since we see that generalization comes directly from learning structured representations. 
    In particular, if the model learns to place the embeddings
    $\bm{E}_0, \bm{E}_1, \bm{E}_2, \ldots, \bm{E}_{p-1}$ evenly spaced along a single line, i.e. $\bm{E}_k = \bm{v} + k \bm{w}$ for any 
    $\bm{v}, \bm{w} \in \mathbb{R}^{d_\text{in}}$, then if the model achieves zero error on training sample $((i, j), i+j)$, it will 
    generalize to any other sample $((m, n), m + n)$ for which $m + n = i + j$. This is because the input to the decoder will be 
    the same for these two samples. 
    <br><br>

    What do the learning dynamics of the embeddings $\bm{E}_{*}$ look like for this toy model? What determines whether the model 
    learns to arrange the $\bm{E}_{*}$ along a line? We develop an effective theory for the learning dynamics of embeddings. 
    The way that the $\bm{E}_{*}$ evolve over training, in practice, will depend on the decoder and the relative learning rate 
    between the $\bm{E}_{*}$ and ${\rm Dec}$. As discussed earlier, if the decoder learning rate is much faster than the learning 
    rate for the embeddings, the model will likely overfit to the embeddings at initialization. Unfortunately, in the effective 
    theory presented in this first paper, we have not incorporated any facts about the decoder into our effective theory. This 
    means that we can't yet analytically predict how learning hyperparameters determine grokking -- we can't 
    compute the phase diagram of learning behavior, shown above, analytically. However, our effective theory does seem to 
    predict the dependence of grokking on the training set size, question #2 discussed at the beginning. 
    <br><br>
    
    We model the learning dynamics of the $\bm{E}_{*}$ as evolving under an <i>effective loss</i> $\ell_\text{eff}$. This effective loss 
    basically measures how well/poorly the embeddings are arranged along a line. It is defined as:
    $$ \ell_\text{eff} = \frac{1}{Z_0} \sum_{(i, j, m, n) \in P_0(D)} |(\bm{E}_i + \bm{E}_j) - (\bm{E}_m + \bm{E}_n)| $$
    where $Z_0 = \sum_k |\bm{E}_k|^2 $, and $P_0(D)$ is defined as:
    $$ P_0(D) = \{ (i, j, m, n) : (i, j) \in D, (m, n) \in D,  i + j = m + n \} $$
    where $D$ is the training dataset. $P_0(D)$ consists of pairs of elements in the training dataset which have the same sum. 
    We see that if the $\bm{E}_{*}$ are evenly-spaced on a line, then $\bm{E}_i + \bm{E}_j = \bm{E}_m + \bm{E}_n$ for
    $(i, j, m, n) \in P_0(D)$, and thus $\ell_\text{eff} = 0$. The learning dynamics under this loss are simply:
    $$ \frac{d\bm{E}_i}{dt} = -\eta \frac{\partial \ell_\text{eff}}{\partial \bm{E}_i} $$



    <!-- simpler and achieve lower loss.  -->

    <!-- , and hopefully discover some simpler solution where they cooperate and perform the operation internally. Internally
    implementing the operation leads to simpler "later layers" since only $p$ representations need to be fit (representations after the binary 
    operation has been performed internally) instead of $\mathcal{O}(p^2)$.  -->
    
    <!-- solution is 
    simpler for the 
    
    This does not 
    have to happen all at once. 
    
    and eventually the "early layers" produce a representation which 
    can be  -->




    
    <br><br><br><br>
    Consider partition a feedforward network into two parts -- the "early layers" and the "late layers". At what 
    layer you draw this line is arbitrary, though there are likely more and less interesting ways 
    one can think of the early layers and the late layers as being engaged 
    
    of the parts of the network before 
    this layer and the parts of the network after this layer are engaged in a kind of cooperative game. 
    The first part 



    <!-- is that  -->

    <br><br><br><br>
    
    but it may still be suggestive.


    This has 
    a striking resemblance to how we visualize modular addition as "clock math". While this resemblance is perhaps suggestive, 
    one has to be a bit 
    careful in taking it too seriously, since we find that other orderings are sometimes learned with different seeds. These orderings
    are still structured, but have a different spacing (e.g. consecutive embeddings are spaced about 1/4th of the circle apart instead of 1/59th).


    
    <!-- However, if instead of 
    
    At this stage, the embeddings seem only globally ordered, and not
    locally. There does seem to be a quick and the super precise ring forms if one continues training for much longer. 
    -->


    <!---------------------------------
                FOOTNOTES
    ---------------------------------->
    <div id="bar"></div> 
    <h2>Footnotes</h2>
    
    <sup id="fn1">1. There are some slight differences, but these are incidental and were not chosen to influence results.<a href="#ref1" title="Jump back to footnote 1 in the text.">↩</a></sup>
    <br>
    <sup id="fn2">2. The original paper on grokking from Power et al. showed a visualization of embeddings with t-SNE,
        but we find the PCA projections to be more interesting. We also track how structure in the embeddings changes over training,
        whereas they just showed embeddings on the trained network.<a href="#ref2" title="Jump back to footnote 2 in the text.">↩</a></sup>
    
    
    <br><br><br><br><br>
    In this paper, they observed something surprising: neural
    networks which generalize <i>after</i> overfitting.

<br>
$$ \Gamma(s) = \int_0^\infty t^{s-1} e^{-t} dt $$
Vestibulum ac risus ut elit placerat congue quis et nisi. Mauris a tortor in turpis pellentesque varius eget faucibus neque. In dui orci, rutrum id tellus non, ultrices pellentesque tortor. Cras ornare, enim in iaculis auctor, ante leo efficitur ligula, sit amet convallis erat arcu vulputate velit. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Fusce sodales felis sapien, at lacinia sapien congue vel. Nulla facilisi. Morbi id cursus velit, at rhoncus metus. Phasellus pellentesque imperdiet magna, accumsan mollis risus auctor sit amet. Aliquam efficitur, est sit amet iaculis porta, orci purus finibus orci, fringilla pulvinar orci sapien ac sapien. Aenean sit amet neque tortor. Donec nisi arcu, suscipit sit amet neque at, iaculis sollicitudin nisl. Morbi at porta diam.
<br><br>
Etiam mollis, libero et mollis tempus, justo metus tincidunt urna, tincidunt commodo lectus neque in magna. Aenean in nunc ut ipsum bibendum commodo nec ullamcorper diam. Pellentesque nec enim metus. Fusce blandit, felis eget luctus pharetra, metus velit sagittis dui, vel tempus sapien leo at felis. Phasellus ullamcorper, ligula semper laoreet cursus, elit risus venenatis justo, sed tempor justo velit eu urna. Pellentesque feugiat nisl at rhoncus malesuada. Fusce vehicula diam tellus, eget elementum ante sodales et. Quisque tincidunt tellus posuere lectus condimentum euismod. Praesent sodales pellentesque vehicula. Sed fringilla gravida quam vitae consequat.
<br><br>
In sollicitudin, dolor et maximus accumsan, quam mauris dignissim eros, sed finibus lectus ipsum eget sem. Aliquam vel metus orci. Sed maximus finibus finibus. Vivamus quis nunc quis erat tristique condimentum vitae ac dui. Pellentesque elementum turpis id venenatis facilisis. In pellentesque magna lectus, ac suscipit massa sollicitudin feugiat. Nulla sed risus quis lorem elementum luctus a a orci. Nam condimentum eros ac mi mollis maximus. Curabitur sed semper turpis, et commodo ipsum. Proin ut erat dictum, blandit ex et, pulvinar quam.
<br><br>
Cras id fringilla arcu. Vestibulum placerat mi metus, non gravida nulla cursus ac. Fusce in ornare erat, ac ullamcorper erat. Aliquam erat volutpat. Morbi sed erat ut libero porttitor semper. Proin pretium rutrum magna, dignissim vestibulum velit dignissim ac. Aenean sed malesuada orci, vitae dapibus tortor. Cras sit amet sapien ligula. Pellentesque sed consectetur justo. Quisque ultrices nulla lorem, sit amet congue sapien euismod sed. Phasellus sed nisi rutrum, lacinia lacus nec, congue dui. Maecenas lacinia eget eros sed ornare. Praesent vitae volutpat arcu. In fermentum diam elementum commodo tincidunt.
<br><br>
Nam quis fermentum turpis. Ut ut turpis euismod, ullamcorper neque quis, eleifend mauris. Maecenas sem nisl, efficitur at vehicula a, maximus a orci. Aliquam posuere porttitor lorem, nec sodales tellus efficitur sit amet. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam sed magna a lectus varius fringilla. Morbi laoreet, tortor vitae accumsan sollicitudin, nulla enim cursus felis, auctor tincidunt nisi sem ac arcu. Phasellus euismod pharetra quam, vitae pellentesque diam sagittis vitae. Mauris et finibus nibh, id porta tortor. Nulla eget purus et nisl cursus efficitur. Donec aliquet facilisis eros, ac ultrices dolor feugiat sed. Etiam posuere scelerisque metus, nec lacinia libero maximus at. Sed vitae ex nec lectus imperdiet consequat ut auctor sapien.
<br><br>
Pellentesque euismod eget augue vitae eleifend. Donec condimentum augue nisl, et elementum orci vestibulum eget. Aenean vitae ante et augue tincidunt consequat. Sed malesuada quam felis, id rhoncus dui bibendum laoreet. Praesent pellentesque, ipsum nec pellentesque dictum, tellus ipsum faucibus odio, id venenatis sapien nisl eget elit. Fusce aliquam odio a velit laoreet aliquam. Curabitur condimentum lorem nec tellus rhoncus ornare.
    
</p> 

    
</div>

    
</body>
</html>
