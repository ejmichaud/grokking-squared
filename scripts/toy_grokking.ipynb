{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=-1, epochs=20000, lr_decoder=0.0002, lr_rep=0.001, dropout=0.05, weight_decay=1.0, rep_noise=False, tune=False, log=20, save_weights=True, save_ckpt=True, plot=0, perfect_rep=False, seed=1, p=59, m=59, split_ratio=0.8, latent_dim=256, decoder_width=128, exp_name='modular-addition59-deep', device='cuda:0', loss='cross_entropy', decoder_depth=2)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from phasegrok.models import Decoder\n",
    "# from phasegrok.config import args\n",
    "from phasegrok.data_gen import generate_data\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.functional import accuracy\n",
    "from phasegrok.utils import Logger, get_loss\n",
    "from phasegrok.utils.modDivide import modDivide\n",
    "import os\n",
    "import optuna \n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "\n",
    "filename = \"../runs/modular-addition59-deep/0427-1456/hparams.txt\"\n",
    "#aka runs/GoldenBoy\n",
    "with open(filename, \"r\") as f:\n",
    "    arg_dict = eval(f.read())\n",
    "    arg_dict['decoder_depth'] = 2\n",
    "args = Namespace(**arg_dict)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args is the configuration environment. The defaults are in config.py\n",
    "\n",
    "pairs, train_indices, test_indices, _ = generate_data(args.p, args.seed, args.split_ratio,\n",
    "                                                      ignore_symmetric=False,\n",
    "                                                      batch_size=args.batch_size)\n",
    "# modular division\n",
    "# nums = torch.arange(args.p) + 1\n",
    "# Y = torch.from_numpy(modDivide(nums, nums.view(-1, 1), args.m))\n",
    "# division\n",
    "# nums = torch.arange(1, args.p + 1)\n",
    "# Y = nums.div(nums.view(-1, 1))\n",
    "# addition\n",
    "nums = torch.arange(args.p)\n",
    "Y = nums + nums.view(-1, 1)\n",
    "Y = Y % args.m if args.m > 1 else Y\n",
    "Y = Y.long().to(args.device)\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "representation = torch.randn(args.p, args.latent_dim).to(\n",
    "    args.device).requires_grad_()\n",
    "out_classes = args.m if args.m > 1 else 2 * args.p - 1\n",
    "model = Decoder(input_dim=args.latent_dim, output_dim=out_classes,\n",
    "                w=args.decoder_width, depth=args.decoder_depth,\n",
    "                concat=True, dropout=args.dropout).to(args.device)\n",
    "\n",
    "\n",
    "param_groups = [{\"params\": (representation, ), \"lr\": args.lr_rep},\n",
    "                {\"params\": model.parameters(), \"lr\": args.lr_decoder,\n",
    "                \"weight_decay\": args.weight_decay}]\n",
    "\n",
    "optimizer = torch.optim.AdamW(param_groups)\n",
    "loss_func = get_loss(args.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, *_ in train_indices:\n",
    "    optimizer.zero_grad()\n",
    "    loss_train, acc_train = step(idx)\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for idx, *_ in test_indices:\n",
    "        loss_test, acc_test = step(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'decoder_depth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9873/1859595314.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# main()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_9873/1859595314.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mout_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     model = Decoder(input_dim=args.latent_dim, output_dim=out_classes,\n\u001b[0;32m---> 24\u001b[0;31m                     \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_depth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                     concat=True, dropout=args.dropout).to(args.device)\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Namespace' object has no attribute 'decoder_depth'"
     ]
    }
   ],
   "source": [
    "# args is the configuration environment. The defaults are in config.py\n",
    "\n",
    "pairs, train_indices, test_indices, _ = generate_data(args.p, args.seed, args.split_ratio,\n",
    "                                                      ignore_symmetric=False,\n",
    "                                                      batch_size=args.batch_size)\n",
    "# modular division\n",
    "# nums = torch.arange(args.p) + 1\n",
    "# Y = torch.from_numpy(modDivide(nums, nums.view(-1, 1), args.m))\n",
    "# division\n",
    "# nums = torch.arange(1, args.p + 1)\n",
    "# Y = nums.div(nums.view(-1, 1))\n",
    "# addition\n",
    "nums = torch.arange(args.p)\n",
    "Y = nums + nums.view(-1, 1)\n",
    "Y = Y % args.m if args.m > 1 else Y\n",
    "Y = Y.long().to(args.device)\n",
    "\n",
    "def main():\n",
    "    torch.manual_seed(args.seed)\n",
    "    representation = torch.randn(args.p, args.latent_dim).to(\n",
    "        args.device).requires_grad_()\n",
    "    out_classes = args.m if args.m > 1 else 2 * args.p - 1\n",
    "    model = Decoder(input_dim=args.latent_dim, output_dim=out_classes,\n",
    "                    w=args.decoder_width, depth=args.decoder_depth,\n",
    "                    concat=True, dropout=args.dropout).to(args.device)\n",
    "\n",
    "\n",
    "    param_groups = [{\"params\": (representation, ), \"lr\": args.lr_rep},\n",
    "                    {\"params\": model.parameters(), \"lr\": args.lr_decoder,\n",
    "                    \"weight_decay\": args.weight_decay}]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(param_groups)\n",
    "    loss_func = get_loss(args.loss)\n",
    "\n",
    "    print(args)\n",
    "\n",
    "\n",
    "    def step(idx):\n",
    "        x = representation[idx]\n",
    "        pred = model(x)\n",
    "        target = Y[idx[:, 0], idx[:, 1]]\n",
    "        if loss_func == torch.nn.functional.mse_loss:\n",
    "            # pred = pred.softmax(1)\n",
    "            loss = loss_func(pred, torch.nn.functional.one_hot(\n",
    "                target, out_classes).float())\n",
    "        else:\n",
    "            loss = loss_func(pred, target)\n",
    "        acc = accuracy(pred, target)\n",
    "        return loss, acc\n",
    "\n",
    "\n",
    "    pbar = tqdm(range(args.epochs))\n",
    "    logger = Logger(args, experiment=f\"{args.exp_name}\", timestamp=True, model=model, debug=True)\n",
    "        \n",
    "    for epoch in pbar:\n",
    "        model.train()\n",
    "        metrics = {}\n",
    "        for idx, *_ in train_indices:\n",
    "            optimizer.zero_grad()\n",
    "            loss_train, acc_train = step(idx)\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for idx, *_ in test_indices:\n",
    "                loss_test, acc_test = step(idx)\n",
    "\n",
    "        # logging\n",
    "        msg = f\"Loss {loss_train.item():.2e}|{loss_test.item():.2e} || \"\n",
    "        msg += f\"Acc {acc_train:.3f}|{acc_test:.3f}\"\n",
    "        pbar.set_description(msg)\n",
    "\n",
    "        # Logging metrics and embeddings\n",
    "        metrics = {\"loss/train\": loss_train.item(), \"loss/test\": loss_test.item(),\n",
    "                \"acc/train\": acc_train, \"acc/test\": acc_test}\n",
    "        logger.log(metrics, weights=representation.data, ckpt=model.state_dict())\n",
    "        # if epoch == 0:\n",
    "        #     if args.save_ckpt:\n",
    "        #         torch.save(model.cpu(), os.path.join(logger.log_path, \"model.pt\"))\n",
    "        #         model.to(args.device)\n",
    "\n",
    "        # Plotting embeddings\n",
    "        if args.plot > 0:\n",
    "            if epoch % args.plot == 0:\n",
    "                logger.plot_embedding(representation.detach(), metrics, epoch)\n",
    "            if epoch == args.epochs - 1:\n",
    "                logger.plot_embedding(representation.detach(), metrics, epoch)\n",
    "                logger.save_anim(\"bruv2\")\n",
    "\n",
    "    logger.close()\n",
    "    return loss_test, acc_test\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if args.tune:\n",
    "        def objective(trial):\n",
    "            args.__dict__[\"lr_rep\"] = trial.suggest_loguniform(\"lr_rep\", 1e-3, 1e-1)\n",
    "            args.__dict__[\"lr_decoder\"] = trial.suggest_loguniform(\n",
    "                \"lr_decoder\", 1e-3, 1e-2)\n",
    "            # args.__dict__[\"weight_decay\"] = trial.suggest_loguniform(\n",
    "            #     \"weight_decay\", 1e-5, 0.1)\n",
    "            # args.__dict__[\"dropout\"] = trial.suggest_loguniform(\"dropout\", 1e-3, 1e-1)\n",
    "            # args.__dict__[\"decoder_width\"] = trial.suggest_int(\"decoder_width\", 10, 100)\n",
    "            # args.__dict__[\"seed\"] = trial.suggest_int(\"seed\", 0, 1000)\n",
    "            test_loss, test_acc, *_ = main()\n",
    "            return test_acc\n",
    "\n",
    "        search_space = {'lr_decoder': np.exp(np.linspace(-4, -1, 10)),\n",
    "                        'lr_rep': np.exp(np.linspace(-4, -1, 10))}\n",
    "        if args.tune == -1:\n",
    "            study = optuna.create_study(sampler=optuna.samplers.GridSampler(search_space))\n",
    "            study.optimize(objective)\n",
    "        else:\n",
    "            study = optuna.create_study(direction=\"maximize\")\n",
    "            study.optimize(objective, n_trials=args.tune)\n",
    "        # joblib.dump(study, \"study.pkl\")\n",
    "        print(\"_________________________________________\" * 2)\n",
    "        # print(\"Done! Now training:\")\n",
    "        # print(study.best_params)\n",
    "        # for k, v in study.best_params.items():\n",
    "        #     args.__dict__[k] = v\n",
    "        # args.__dict__[\"epochs\"] = 100000\n",
    "        # args.__dict__[\"no-log\"] = False\n",
    "        # main()\n",
    "    else:\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d57e5d6bf9feb93d751da48b3322cce599fe4781d036e1a5603ac4d4b53d3fd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('main')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
