{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8077513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import scipy.linalg\n",
    "import scipy\n",
    "import copy\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd60431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (0,2,1)(2,1,0)->(2,0,1)\n",
    "\n",
    "op1 = np.array([0,2,1])\n",
    "op2 = np.array([2,1,0])\n",
    "op_out = op2[op1]\n",
    "\n",
    "path = \"./results/fig5d/\"\n",
    "isExist = os.path.exists(path)\n",
    "if not isExist:\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Python function to print permutations of a given list\n",
    "def permutation(lst):\n",
    " \n",
    "    # If lst is empty then there are no permutations\n",
    "    if len(lst) == 0:\n",
    "        return []\n",
    " \n",
    "    # If there is only one element in lst then, only\n",
    "    # one permutation is possible\n",
    "    if len(lst) == 1:\n",
    "        return [lst]\n",
    " \n",
    "    # Find the permutations for lst if there are\n",
    "    # more than 1 characters\n",
    " \n",
    "    l = [] # empty list that will store current permutation\n",
    " \n",
    "    # Iterate the input(lst) and calculate the permutation\n",
    "    for i in range(len(lst)):\n",
    "       m = lst[i]\n",
    " \n",
    "       # Extract lst[i] or m from the list.  remLst is\n",
    "       # remaining list\n",
    "       remLst = lst[:i] + lst[i+1:]\n",
    " \n",
    "       # Generating all permutations where m is first\n",
    "       # element\n",
    "       for p in permutation(remLst):\n",
    "           l.append([m] + p)\n",
    "    return l\n",
    " \n",
    "\n",
    "p = 3\n",
    "data = list(np.arange(p))\n",
    "perms = []\n",
    "perm2id = {}\n",
    "id_ = 0\n",
    "for perm in permutation(data):\n",
    "    perms.append(perm)\n",
    "    perm2id[\"{}\".format(perm)] = id_\n",
    "    id_ = id_ + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d37b590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "def train(params):\n",
    "\n",
    "    train_num = 24\n",
    "    epochs = int(params[2])\n",
    "    batch_size = train_num\n",
    "\n",
    "    init_scale = 0.1\n",
    "    eta1 = 1e-3\n",
    "    eta2 = params[0]\n",
    "    weight_decay = params[1]\n",
    "    wd2 = weight_decay\n",
    "    log = 100\n",
    "    w = 200\n",
    "    wd1 = 0.0\n",
    "    lamb = 1e-3\n",
    "    iters = []\n",
    "    latents = []\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    print(\"train_num={}\".format(train_num))\n",
    "    print(\"seed={}\".format(seed))\n",
    "\n",
    "\n",
    "\n",
    "    p = 3\n",
    "    data = list(np.arange(p))\n",
    "    perms = []\n",
    "    perm2id = {}\n",
    "    id_ = 0\n",
    "    for perm in permutation(data):\n",
    "        perms.append(perm)\n",
    "        perm2id[\"{}\".format(perm)] = id_\n",
    "        id_ = id_ + 1\n",
    "\n",
    "    num = math.factorial(p)\n",
    "\n",
    "\n",
    "    latent_dim = p**2\n",
    "    output_dim = 30\n",
    "\n",
    "\n",
    "    y_templates = np.random.normal(0,1,size=(num, output_dim))\n",
    "    y_templates = torch.tensor(y_templates, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "\n",
    "    D0_id = []\n",
    "    xx_id = []\n",
    "    yy_id = []\n",
    "    out_id = []\n",
    "    for i in range(num):\n",
    "        for j in range(num):\n",
    "            D0_id.append((i,j))\n",
    "            xx_id.append(i)\n",
    "            yy_id.append(j)\n",
    "\n",
    "            p1 = perms[i]\n",
    "            p2 = perms[j]\n",
    "            p_out = list(np.array(p2)[np.array(p1)])\n",
    "            id_ = perm2id[\"{}\".format(p_out)]\n",
    "            out_id.append(id_)\n",
    "\n",
    "    xx_id = np.array(xx_id)\n",
    "    yy_id = np.array(yy_id)\n",
    "    out_id = np.array(out_id)\n",
    "\n",
    "\n",
    "    # neural network part\n",
    "    all_num = num*num\n",
    "    train_id = np.random.choice(all_num,train_num, replace=False)\n",
    "    test_id = np.array(list(set(np.arange(all_num)) - set(train_id)))\n",
    "\n",
    "    inputs_id = np.transpose(np.array([xx_id,yy_id]))\n",
    "\n",
    "    labels_train = torch.tensor(y_templates[out_id[train_id]], dtype=torch.float, requires_grad=True)\n",
    "    out_id_train = out_id[train_id]\n",
    "\n",
    "    labels_test = torch.tensor(y_templates[out_id[test_id]], dtype=torch.float, requires_grad=True)\n",
    "    out_id_test = out_id[test_id]\n",
    "\n",
    "\n",
    "\n",
    "    # P0\n",
    "    P0 = []\n",
    "    P0_id = []\n",
    "\n",
    "    ii = 0\n",
    "    for i in range(all_num):\n",
    "        for j in range(i+1,all_num):\n",
    "            if out_id[i] == out_id[j]:\n",
    "                P0.append(frozenset({D0_id[i], D0_id[j]}))\n",
    "                P0_id.append(ii)\n",
    "                ii += 1\n",
    "\n",
    "    P0_num = len(P0_id)\n",
    "    print(P0_num)\n",
    "\n",
    "\n",
    "    class NET(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, w=200):\n",
    "            super(NET, self).__init__()\n",
    "            self.l1 = nn.Linear(input_dim, w)\n",
    "            self.l2 = nn.Linear(w, w)\n",
    "            self.l3 = nn.Linear(w, output_dim)\n",
    "\n",
    "        def forward(self, x):\n",
    "            f = torch.nn.Tanh()\n",
    "            #f = torch.nn.LeakyReLU(0.2)\n",
    "            self.x1 = f(self.l1(x))\n",
    "            self.x2 = f(self.l2(self.x1))\n",
    "            self.x3 = self.l3(self.x2)\n",
    "            return self.x3\n",
    "\n",
    "    class DEC(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, w=200):\n",
    "            super(DEC, self).__init__()\n",
    "            self.net = NET(input_dim, output_dim, w=200)\n",
    "\n",
    "        def forward(self, latent, x_id):\n",
    "            bs = x_id.shape[0]\n",
    "            self.add1 = latent[x_id[:,0]]\n",
    "            self.add2 = latent[x_id[:,1]]\n",
    "            self.add = torch.matmul(self.add1.reshape(bs,p,p), self.add2.reshape(bs,p,p)).reshape(bs,-1)\n",
    "            self.out = self.net(self.add.reshape(bs,-1))\n",
    "            return self.out\n",
    "\n",
    "\n",
    "    class AE(nn.Module):\n",
    "\n",
    "        def __init__(self, w=200, input_dim=1, output_dim=1):\n",
    "            super(AE, self).__init__()\n",
    "            self.enc = NET(input_dim, latent_dim, w=w)\n",
    "            self.dec = DEC(latent_dim, output_dim, w=w)\n",
    "\n",
    "        def forward(self, x, x_id):\n",
    "            self.latent = self.enc(x)\n",
    "            self.out = self.dec(self.latent,x_id)\n",
    "\n",
    "            return self.out\n",
    "\n",
    "    model = AE(input_dim=1, output_dim=output_dim, w=w)\n",
    "\n",
    "    ccs_epoch = []\n",
    "    rqi_epoch = []\n",
    "    train_epoch = []\n",
    "    test_epoch = []\n",
    "\n",
    "    print(\"eta2={}\".format(eta2))\n",
    "\n",
    "    latent = torch.nn.parameter.Parameter((torch.rand(num,latent_dim)-1/2)*init_scale)\n",
    "    optimizer1 = torch.optim.AdamW({latent}, lr=eta1, weight_decay = wd1)\n",
    "    optimizer2 = torch.optim.AdamW(model.dec.parameters(), lr=eta2, weight_decay = weight_decay)\n",
    "\n",
    "\n",
    "    reach_acc_test = False\n",
    "    reach_acc_train = False\n",
    "    reach_rqi = False\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "\n",
    "        choice = np.random.choice(np.arange(train_num), batch_size, replace=False)\n",
    "\n",
    "        outputs_train = model.dec(latent, inputs_id[train_id[choice]])\n",
    "        outputs_test = model.dec(latent, inputs_id[test_id])\n",
    "        loss_train = torch.mean((outputs_train-labels_train[choice])**2) + lamb*torch.mean(torch.abs(latent))\n",
    "        loss_train.backward()\n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "\n",
    "\n",
    "        # accuracy\n",
    "        outputs_train = model.dec(latent, inputs_id[train_id])\n",
    "        outputs_test = model.dec(latent, inputs_id[test_id])\n",
    "        pred_train_id = torch.argmin(torch.sum((outputs_train.unsqueeze(dim=1) - y_templates.unsqueeze(dim=0))**2, dim=2), dim=1)\n",
    "        pred_test_id = torch.argmin(torch.sum((outputs_test.unsqueeze(dim=1) - y_templates.unsqueeze(dim=0))**2, dim=2), dim=1)\n",
    "        #print(pred_train_id.detach().numpy())\n",
    "        acc_nn_train = np.mean(pred_train_id.detach().numpy() == out_id_train)\n",
    "        acc_nn_test = np.mean(pred_test_id.detach().numpy() == out_id_test)\n",
    "        train_epoch.append(acc_nn_train)\n",
    "        test_epoch.append(acc_nn_test)\n",
    "        acc_nn = (acc_nn_train*train_id.shape[0] + acc_nn_test*test_id.shape[0])/all_num\n",
    "\n",
    "        if not reach_acc_test:\n",
    "            if acc_nn_test >= 0.9:\n",
    "                reach_acc_test = True\n",
    "                iter_test = epoch\n",
    "\n",
    "        if not reach_acc_train:\n",
    "            if acc_nn_train >= 0.9:\n",
    "                reach_acc_train = True\n",
    "                iter_train = epoch\n",
    "\n",
    "        if epoch % log == 0:\n",
    "            print(\"epoch: %d  | loss: %.8f \"%(epoch, loss_train.detach().numpy()))\n",
    "\n",
    "        latent_scale = latent/torch.sqrt(torch.mean(latent**2))\n",
    "        latent_scale = latent_scale.reshape(num,p,p)\n",
    "        count = 0\n",
    "        PR_id = []\n",
    "        PR = []\n",
    "        for i in range(P0_num):\n",
    "            ids = list(P0[i])\n",
    "            a, b, c, d = ids[0][0], ids[0][1], ids[1][0], ids[1][1]\n",
    "            res = torch.matmul(latent_scale[a], latent_scale[b])-torch.matmul(latent_scale[c], latent_scale[d])\n",
    "            if torch.mean(res**2) < 0.5:\n",
    "                count += 1\n",
    "                PR_id.append(i)\n",
    "                PR.append(P0[i])\n",
    "\n",
    "\n",
    "        rqi = count/P0_num\n",
    "        if not reach_rqi:\n",
    "            if rqi > 0.95:\n",
    "                reach_rqi = True\n",
    "                iter_rqi = epoch\n",
    "\n",
    "        latents.append(latent_scale.detach().numpy())\n",
    "        rqi_epoch.append(rqi)\n",
    "\n",
    "    # Dbar(D)\n",
    "    Dbar_id = list(copy.deepcopy(train_id))\n",
    "\n",
    "    for i1 in test_id:\n",
    "        flag = 0\n",
    "        for j1 in train_id:\n",
    "            i, j = D0_id[i1]\n",
    "            m, n = D0_id[j1]\n",
    "            if {(i,j),(m,n)} in PR:\n",
    "                flag = 1\n",
    "                break\n",
    "\n",
    "        if flag == 1:\n",
    "            Dbar_id.append(i1)\n",
    "\n",
    "\n",
    "    pred_acc = len(Dbar_id)/all_num\n",
    "    print(\"pred_acc = {}/{}={}\".format(len(Dbar_id),all_num,pred_acc))\n",
    "    print(\"real_acc = {}\".format(acc_nn))\n",
    "\n",
    "\n",
    "    if not reach_acc_test:\n",
    "        iter_test = epoch\n",
    "\n",
    "    if not reach_acc_train:\n",
    "        iter_train = epoch\n",
    "\n",
    "    if not reach_rqi:\n",
    "        iter_rqi = epoch\n",
    "\n",
    "    rqi_epoch = np.array(rqi_epoch)\n",
    "    train_epoch = np.array(train_epoch)\n",
    "    test_epoch = np.array(test_epoch)\n",
    "    \n",
    "    np.savetxt(path+\"train_eta2_%.5f_wd_%.1f.txt\"%(eta2,wd2), [iter_train])\n",
    "    np.savetxt(path+\"test_eta2_%.5f_wd_%.1f.txt\"%(eta2,wd2), [iter_test])\n",
    "    #np.savetxt(path+\"rqi_eta2_%.5f_wd_%.1f.txt\"%(seed,eta2,wd2), [rqi])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25b6af97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_num=24train_num=24\n",
      "train_num=24\n",
      "seed=0\n",
      "seed=0train_num=24train_num=24\n",
      "train_num=24\n",
      "\n",
      "seed=0\n",
      "train_num=24\n",
      "seed=0train_num=24seed=0\n",
      "\n",
      "seed=0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_67862/2534176872.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_train = torch.tensor(y_templates[out_id[train_id]], dtype=torch.float, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_67862/2534176872.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_train = torch.tensor(y_templates[out_id[train_id]], dtype=torch.float, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "seed=0train_num=24"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_67862/2534176872.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_test = torch.tensor(y_templates[out_id[test_id]], dtype=torch.float, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_67862/2534176872.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_test = torch.tensor(y_templates[out_id[test_id]], dtype=torch.float, requires_grad=True)\n",
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_67862/2534176872.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_train = torch.tensor(y_templates[out_id[train_id]], dtype=torch.float, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed=0train_num=24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_67862/2534176872.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_train = torch.tensor(y_templates[out_id[train_id]], dtype=torch.float, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_num=2490"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_67862/2534176872.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_train = torch.tensor(y_templates[out_id[train_id]], dtype=torch.float, requires_grad=True)\n",
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_67862/2534176872.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_test = torch.tensor(y_templates[out_id[test_id]], dtype=torch.float, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "90\n",
      "seed=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_67862/2534176872.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_test = torch.tensor(y_templates[out_id[test_id]], dtype=torch.float, requires_grad=True)\n",
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_67862/2534176872.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_train = torch.tensor(y_templates[out_id[train_id]], dtype=torch.float, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_67862/2534176872.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_test = torch.tensor(y_templates[out_id[test_id]], dtype=torch.float, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "seed=090\n",
      "seed=0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_67862/2534176872.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_train = torch.tensor(y_templates[out_id[train_id]], dtype=torch.float, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_67862/2534176872.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_test = torch.tensor(y_templates[out_id[test_id]], dtype=torch.float, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_67862/2534176872.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_train = torch.tensor(y_templates[out_id[train_id]], dtype=torch.float, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_67862/2534176872.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_test = torch.tensor(y_templates[out_id[test_id]], dtype=torch.float, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta2=1e-0590eta2=0.0001"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_67862/2534176872.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_test = torch.tensor(y_templates[out_id[test_id]], dtype=torch.float, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_67862/2534176872.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_train = torch.tensor(y_templates[out_id[train_id]], dtype=torch.float, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "90eta2=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_67862/2534176872.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_train = torch.tensor(y_templates[out_id[train_id]], dtype=torch.float, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_67862/2534176872.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_train = torch.tensor(y_templates[out_id[train_id]], dtype=torch.float, requires_grad=True)\n",
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_67862/2534176872.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_test = torch.tensor(y_templates[out_id[test_id]], dtype=torch.float, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "eta2=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_67862/2534176872.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_test = torch.tensor(y_templates[out_id[test_id]], dtype=torch.float, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta2=4.641588833612782e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_67862/2534176872.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_test = torch.tensor(y_templates[out_id[test_id]], dtype=torch.float, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "\n",
      "eta2=0.0004641588833612777390\n",
      "90eta2=0.004641588833612777\n",
      "\n",
      "\n",
      "eta2=2.1544346900318823e-05\n",
      "\n",
      "eta2=0.00021544346900318823\n",
      "eta2=1e-05eta2=0.002154434690031882\n",
      "\n",
      "\n",
      "epoch: 0  | loss: 1.05029035 epoch: 0  | loss: 1.05029035 epoch: 0  | loss: 1.05029035 \n",
      "epoch: 0  | loss: 1.05029035 \n",
      "\n",
      "epoch: 0  | loss: 1.05029035 \n",
      "epoch: 0  | loss: 1.05029035 epoch: 0  | loss: 1.05029035 \n",
      "\n",
      "\n",
      "epoch: 0  | loss: 1.05029035 epoch: 0  | loss: 1.05029035 epoch: 0  | loss: 1.05029035 \n",
      "\n",
      "epoch: 0  | loss: 1.05029035 \n",
      "\n",
      "epoch: 100  | loss: 0.74692088 \n",
      "epoch: 100  | loss: 0.96591949 epoch: 100  | loss: 0.85278535 \n",
      "\n",
      "epoch: 100  | loss: 0.57466400 epoch: 100  | loss: 0.12537731 \n",
      "\n",
      "epoch: 100  | loss: 0.30104217 \n",
      "epoch: 100  | loss: 1.00518644 \n",
      "epoch: 100  | loss: 1.00530410 \n",
      "epoch: 100  | loss: 0.88148838 \n",
      "epoch: 100  | loss: 0.66534293 \n",
      "epoch: 100  | loss: 0.90744156 \n",
      "epoch: 200  | loss: 0.50035387 \n",
      "epoch: 200  | loss: 0.27478150 \n",
      "epoch: 200  | loss: 0.86019677 \n",
      "epoch: 200  | loss: 0.00157634 epoch: 200  | loss: 0.08346827 epoch: 200  | loss: 0.58619189 epoch: 200  | loss: 0.94267565 \n",
      "\n",
      "\n",
      "\n",
      "epoch: 200  | loss: 0.94153762 \n",
      "epoch: 200  | loss: 0.00621810 \n",
      "epoch: 200  | loss: 0.75049144 \n",
      "epoch: 200  | loss: 0.08692945 \n",
      "epoch: 300  | loss: 0.00049666 \n",
      "epoch: 300  | loss: 0.30330014 \n",
      "epoch: 300  | loss: 0.82107908 \n",
      "epoch: 300  | loss: 0.01498610 \n",
      "epoch: 300  | loss: 0.81761873 \n",
      "epoch: 300  | loss: 0.11961120 \n",
      "epoch: 300  | loss: 0.64915609 \n",
      "epoch: 300  | loss: 0.48849905 \n",
      "epoch: 300  | loss: 0.00293435 \n",
      "epoch: 300  | loss: 0.00362931 \n",
      "epoch: 300  | loss: 0.00036211 \n",
      "epoch: 400  | loss: 0.00025807 \n",
      "epoch: 400  | loss: 0.09765401 epoch: 400  | loss: 0.00097687 \n",
      "\n",
      "epoch: 400  | loss: 0.33148348 \n",
      "epoch: 400  | loss: 0.00457212 \n",
      "epoch: 400  | loss: 0.67582059 \n",
      "epoch: 400  | loss: 0.01971431 \n",
      "epoch: 400  | loss: 0.47270739 \n",
      "epoch: 400  | loss: 0.67042440 \n",
      "epoch: 400  | loss: 0.00017806 \n",
      "epoch: 400  | loss: 0.00129104 \n",
      "epoch: 500  | loss: 0.02795183 \n",
      "epoch: 500  | loss: 0.00213054 \n",
      "epoch: 500  | loss: 0.23061870 \n",
      "epoch: 500  | loss: 0.56637317 epoch: 500  | loss: 0.00019701 \n",
      "\n",
      "epoch: 500  | loss: 0.36545670 \n",
      "epoch: 500  | loss: 0.00767528 \n",
      "epoch: 500  | loss: 0.56006867 \n",
      "epoch: 500  | loss: 0.00048083 \n",
      "epoch: 500  | loss: 0.00063012 \n",
      "epoch: 500  | loss: 0.00068390 \n",
      "epoch: 600  | loss: 0.00118465 \n",
      "epoch: 600  | loss: 0.01307938 \n",
      "epoch: 600  | loss: 0.48275682 \n",
      "epoch: 600  | loss: 0.14118715 epoch: 600  | loss: 0.00017478 \n",
      "\n",
      "epoch: 600  | loss: 0.00376432 \n",
      "epoch: 600  | loss: 0.26963887 \n",
      "epoch: 600  | loss: 0.47494605 \n",
      "epoch: 600  | loss: 0.00010229 \n",
      "epoch: 600  | loss: 0.00046673 \n",
      "epoch: 600  | loss: 0.00033744 \n",
      "epoch: 700  | loss: 0.06563470 epoch: 700  | loss: 0.00075318 \n",
      "\n",
      "epoch: 700  | loss: 0.00016533 \n",
      "epoch: 700  | loss: 0.40611732 \n",
      "epoch: 700  | loss: 0.00661262 \n",
      "epoch: 700  | loss: 0.00217493 \n",
      "epoch: 700  | loss: 0.19169952 epoch: 700  | loss: 0.00028333 \n",
      "\n",
      "epoch: 700  | loss: 0.39635265 \n",
      "epoch: 700  | loss: 0.00009060 \n",
      "epoch: 700  | loss: 0.00034433 \n",
      "epoch: 800  | loss: 0.00053794 \n",
      "epoch: 800  | loss: 0.03259631 \n",
      "epoch: 800  | loss: 0.33710647 \n",
      "epoch: 800  | loss: 0.00354956 \n",
      "epoch: 800  | loss: 0.14539309 epoch: 800  | loss: 0.32604161 \n",
      "\n",
      "epoch: 800  | loss: 0.00025873 \n",
      "epoch: 800  | loss: 0.00142509 epoch: 800  | loss: 0.00068597 \n",
      "\n",
      "epoch: 800  | loss: 0.00016133 \n",
      "epoch: 800  | loss: 0.00028326 \n",
      "epoch: 900  | loss: 0.01896846 \n",
      "epoch: 900  | loss: 0.28058112 \n",
      "epoch: 900  | loss: 0.00008060 \n",
      "epoch: 900  | loss: 0.00207847 \n",
      "epoch: 900  | loss: 0.00024652 \n",
      "epoch: 900  | loss: 0.00042424 epoch: 900  | loss: 0.11517851 \n",
      "epoch: 900  | loss: 0.26917183 \n",
      "\n",
      "epoch: 900  | loss: 0.00102915 \n",
      "epoch: 900  | loss: 0.00016001 \n",
      "epoch: 900  | loss: 0.00024476 \n",
      "epoch: 1000  | loss: 0.01179473 \n",
      "epoch: 1000  | loss: 0.00036116 epoch: 1000  | loss: 0.00007789 \n",
      "\n",
      "epoch: 1000  | loss: 0.00135994 \n",
      "epoch: 1000  | loss: 0.23650160 epoch: 1000  | loss: 0.00016017 epoch: 1000  | loss: 0.08977443 \n",
      "\n",
      "epoch: 1000  | loss: 0.00024043 \n",
      "\n",
      "epoch: 1000  | loss: 0.22515471 \n",
      "epoch: 1000  | loss: 0.00080149 \n",
      "epoch: 1000  | loss: 0.00022084 \n",
      "epoch: 1100  | loss: 0.00032483 \n",
      "epoch: 1100  | loss: 0.20171343 \n",
      "epoch: 1100  | loss: 0.00761637 \n",
      "epoch: 1100  | loss: 0.00099355 \n",
      "epoch: 1100  | loss: 0.00186969 \n",
      "epoch: 1100  | loss: 0.00016120 \n",
      "epoch: 1100  | loss: 0.19048880 \n",
      "epoch: 1100  | loss: 0.00023777 \n",
      "epoch: 1100  | loss: 0.00066253 \n",
      "epoch: 1100  | loss: 0.06738789 \n",
      "epoch: 1100  | loss: 0.00021331 \n",
      "epoch: 1200  | loss: 0.00030331 epoch: 1200  | loss: 0.00511217 \n",
      "\n",
      "epoch: 1200  | loss: 0.17296880 \n",
      "epoch: 1200  | loss: 0.00079185 \n",
      "epoch: 1200  | loss: 0.00007496 \n",
      "epoch: 1200  | loss: 0.16194308 \n",
      "epoch: 1200  | loss: 0.04968763 \n",
      "epoch: 1200  | loss: 0.00016278 \n",
      "epoch: 1200  | loss: 0.00057435 \n",
      "epoch: 1200  | loss: 0.00019211 \n",
      "epoch: 1200  | loss: 0.00023713 \n",
      "epoch: 1300  | loss: 0.00358572 \n",
      "epoch: 1300  | loss: 0.14816251 \n",
      "epoch: 1300  | loss: 0.00029036 \n",
      "epoch: 1300  | loss: 0.00016469 epoch: 1300  | loss: 0.00007379 \n",
      "\n",
      "epoch: 1300  | loss: 0.00067062 \n",
      "epoch: 1300  | loss: 0.13743301 \n",
      "epoch: 1300  | loss: 0.03676851 \n",
      "epoch: 1300  | loss: 0.00018311 \n",
      "epoch: 1300  | loss: 0.00023766 \n",
      "epoch: 1300  | loss: 0.00051624 \n",
      "epoch: 1400  | loss: 0.00263680 \n",
      "epoch: 1400  | loss: 0.00028255 \n",
      "epoch: 1400  | loss: 0.00059165 epoch: 1400  | loss: 0.00007287 \n",
      "\n",
      "epoch: 1400  | loss: 0.12615559 \n",
      "epoch: 1400  | loss: 0.02751499 \n",
      "epoch: 1400  | loss: 0.00016677 \n",
      "epoch: 1400  | loss: 0.11572612 \n",
      "epoch: 1400  | loss: 0.00023890 \n",
      "epoch: 1400  | loss: 0.00040823 \n",
      "epoch: 1400  | loss: 0.00047735 \n",
      "epoch: 1500  | loss: 0.00027792 \n",
      "epoch: 1500  | loss: 0.00203264 \n",
      "epoch: 1500  | loss: 0.10632488 \n",
      "epoch: 1500  | loss: 0.02092436 \n",
      "epoch: 1500  | loss: 0.00016894 \n",
      "epoch: 1500  | loss: 0.00053692 \n",
      "epoch: 1500  | loss: 0.09623161 \n",
      "epoch: 1500  | loss: 0.00214301 \n",
      "epoch: 1500  | loss: 0.00045058 epoch: 1500  | loss: 0.00024059 \n",
      "\n",
      "epoch: 1500  | loss: 0.00017062 \n",
      "epoch: 1600  | loss: 0.00163727 \n",
      "epoch: 1600  | loss: 0.00027531 \n",
      "epoch: 1600  | loss: 0.08843280 \n",
      "epoch: 1600  | loss: 0.00017113 \n",
      "epoch: 1600  | loss: 0.07885337 \n",
      "epoch: 1600  | loss: 0.00049732 \n",
      "epoch: 1600  | loss: 0.01622446 \n",
      "epoch: 1600  | loss: 0.00007277 \n",
      "epoch: 1600  | loss: 0.00043195 \n",
      "epoch: 1600  | loss: 0.00016758 \n",
      "epoch: 1600  | loss: 0.00024256 \n",
      "epoch: 1700  | loss: 0.00137055 \n",
      "epoch: 1700  | loss: 0.00027404 \n",
      "epoch: 1700  | loss: 0.00017331 \n",
      "epoch: 1700  | loss: 0.01282587 \n",
      "epoch: 1700  | loss: 0.06381904 \n",
      "epoch: 1700  | loss: 0.07256948 \n",
      "epoch: 1700  | loss: 0.00046764 \n",
      "epoch: 1700  | loss: 0.00007206 \n",
      "epoch: 1700  | loss: 0.00041893 \n",
      "epoch: 1700  | loss: 0.00016310 \n",
      "epoch: 1700  | loss: 0.00024470 \n",
      "epoch: 1800  | loss: 0.00118481 \n",
      "epoch: 1800  | loss: 0.00017546 \n",
      "epoch: 1800  | loss: 0.00027365 \n",
      "epoch: 1800  | loss: 0.05130082 \n",
      "epoch: 1800  | loss: 0.01031685 \n",
      "epoch: 1800  | loss: 0.05897611 \n",
      "epoch: 1800  | loss: 0.00007157 \n",
      "epoch: 1800  | loss: 0.00044489 \n",
      "epoch: 1800  | loss: 0.00040988 \n",
      "epoch: 1800  | loss: 0.00024693 \n",
      "epoch: 1800  | loss: 0.00034801 \n",
      "epoch: 1900  | loss: 0.00105133 \n",
      "epoch: 1900  | loss: 0.00027387 \n",
      "epoch: 1900  | loss: 0.00017764 \n",
      "epoch: 1900  | loss: 0.00842564 \n",
      "epoch: 1900  | loss: 0.04116256 \n",
      "epoch: 1900  | loss: 0.04770862 \n",
      "epoch: 1900  | loss: 0.00007114 \n",
      "epoch: 1900  | loss: 0.00042716 \n",
      "epoch: 1900  | loss: 0.00015969 \n",
      "epoch: 1900  | loss: 0.00024922 \n",
      "epoch: 1900  | loss: 0.00040369 \n",
      "epoch: 2000  | loss: 0.00027452 epoch: 2000  | loss: 0.00095260 \n",
      "\n",
      "epoch: 2000  | loss: 0.00017960 \n",
      "epoch: 2000  | loss: 0.00697468 \n",
      "epoch: 2000  | loss: 0.03307294 \n",
      "epoch: 2000  | loss: 0.00007076 \n",
      "epoch: 2000  | loss: 0.03857020 \n",
      "epoch: 2000  | loss: 0.00039976 \n",
      "epoch: 2000  | loss: 0.00041346 \n",
      "epoch: 2000  | loss: 0.00025153 \n",
      "epoch: 2000  | loss: 0.00015878 \n",
      "epoch: 2100  | loss: 0.00087772 \n",
      "epoch: 2100  | loss: 0.00584621 epoch: 2100  | loss: 0.02667384 \n",
      "\n",
      "epoch: 2100  | loss: 0.00027546 \n",
      "epoch: 2100  | loss: 0.00018175 \n",
      "epoch: 2100  | loss: 0.00007329 \n",
      "epoch: 2100  | loss: 0.00039738 \n",
      "epoch: 2100  | loss: 0.00040277 \n",
      "epoch: 2100  | loss: 0.03125728 \n",
      "epoch: 2100  | loss: 0.00015915 \n",
      "epoch: 2100  | loss: 0.00025385 \n",
      "epoch: 2200  | loss: 0.00081971 \n",
      "epoch: 2200  | loss: 0.00018370 \n",
      "epoch: 2200  | loss: 0.02164438 epoch: 2200  | loss: 0.00027662 \n",
      "\n",
      "epoch: 2200  | loss: 0.00495966 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2200  | loss: 0.00039616 \n",
      "epoch: 2200  | loss: 0.00007085 \n",
      "epoch: 2200  | loss: 0.02546319 \n",
      "epoch: 2200  | loss: 0.00021393 \n",
      "epoch: 2200  | loss: 0.00039416 \n",
      "epoch: 2200  | loss: 0.00025617 \n",
      "epoch: 2300  | loss: 0.00027791 \n",
      "epoch: 2300  | loss: 0.01770681 \n",
      "epoch: 2300  | loss: 0.00077397 \n",
      "epoch: 2300  | loss: 0.00018605 \n",
      "epoch: 2300  | loss: 0.00425765 \n",
      "epoch: 2300  | loss: 0.00007055 \n",
      "epoch: 2300  | loss: 0.00039580 \n",
      "epoch: 2300  | loss: 0.00015272 \n",
      "epoch: 2300  | loss: 0.02090609 \n",
      "epoch: 2300  | loss: 0.00025847 \n",
      "epoch: 2300  | loss: 0.00038711 \n",
      "epoch: 2400  | loss: 0.00073740 \n",
      "epoch: 2400  | loss: 0.00369803 \n",
      "epoch: 2400  | loss: 0.00027931 \n",
      "epoch: 2400  | loss: 0.01462280 \n",
      "epoch: 2400  | loss: 0.00018778 \n",
      "epoch: 2400  | loss: 0.00007214 \n",
      "epoch: 2400  | loss: 0.00039611 \n",
      "epoch: 2400  | loss: 0.01733276 \n",
      "epoch: 2400  | loss: 0.00031495 \n",
      "epoch: 2400  | loss: 0.00038129 \n",
      "epoch: 2400  | loss: 0.00026076 \n",
      "epoch: 2500  | loss: 0.00070780 \n",
      "epoch: 2500  | loss: 0.00324894 \n",
      "epoch: 2500  | loss: 0.00028078 \n",
      "epoch: 2500  | loss: 0.01219441 \n",
      "epoch: 2500  | loss: 0.00018977 \n",
      "epoch: 2500  | loss: 0.00007101 \n",
      "epoch: 2500  | loss: 0.00039691 \n",
      "epoch: 2500  | loss: 0.01452310 \n",
      "epoch: 2500  | loss: 0.00017757 \n",
      "epoch: 2500  | loss: 0.00037644 \n",
      "epoch: 2500  | loss: 0.00026302 \n",
      "epoch: 2600  | loss: 0.00288617 \n",
      "epoch: 2600  | loss: 0.00068360 \n",
      "epoch: 2600  | loss: 0.00019193 \n",
      "epoch: 2600  | loss: 0.00030126 \n",
      "epoch: 2600  | loss: 0.01026133 \n",
      "epoch: 2600  | loss: 0.00028228 \n",
      "epoch: 2600  | loss: 0.00039808 \n",
      "epoch: 2600  | loss: 0.00037233 \n",
      "epoch: 2600  | loss: 0.01229638 \n",
      "epoch: 2600  | loss: 0.00034751 \n",
      "epoch: 2600  | loss: 0.00026526 \n",
      "epoch: 2700  | loss: 0.00259129 epoch: 2700  | loss: 0.00007168 \n",
      "epoch: 2700  | loss: 0.00019425 \n",
      "\n",
      "epoch: 2700  | loss: 0.00066364 \n",
      "epoch: 2700  | loss: 0.00870305 \n",
      "epoch: 2700  | loss: 0.00028380 \n",
      "epoch: 2700  | loss: 0.00039954 \n",
      "epoch: 2700  | loss: 0.01051030 \n",
      "epoch: 2700  | loss: 0.00036883 \n",
      "epoch: 2700  | loss: 0.00014860 \n",
      "epoch: 2700  | loss: 0.00026744 \n",
      "epoch: 2800  | loss: 0.00064705 \n",
      "epoch: 2800  | loss: 0.00743249 \n",
      "epoch: 2800  | loss: 0.00235018 \n",
      "epoch: 2800  | loss: 0.00019651 \n",
      "epoch: 2800  | loss: 0.00028533 \n",
      "epoch: 2800  | loss: 0.00040121 \n",
      "epoch: 2800  | loss: 0.00007140 \n",
      "epoch: 2800  | loss: 0.00014830 \n",
      "epoch: 2800  | loss: 0.00036579 \n",
      "epoch: 2800  | loss: 0.00905651 \n",
      "epoch: 2800  | loss: 0.00026959 \n",
      "epoch: 2900  | loss: 0.00215195 \n",
      "epoch: 2900  | loss: 0.00019891 \n",
      "epoch: 2900  | loss: 0.00638485 \n",
      "epoch: 2900  | loss: 0.00007115 \n",
      "epoch: 2900  | loss: 0.00063315 \n",
      "epoch: 2900  | loss: 0.00028684 \n",
      "epoch: 2900  | loss: 0.00040305 \n",
      "epoch: 2900  | loss: 0.00014761 \n",
      "epoch: 2900  | loss: 0.00785691 \n",
      "epoch: 2900  | loss: 0.00036314 \n",
      "epoch: 2900  | loss: 0.00027169 \n",
      "epoch: 3000  | loss: 0.00198814 \n",
      "epoch: 3000  | loss: 0.00062143 \n",
      "epoch: 3000  | loss: 0.00551484 epoch: 3000  | loss: 0.00020311 \n",
      "\n",
      "epoch: 3000  | loss: 0.00007604 \n",
      "epoch: 3000  | loss: 0.00040501 \n",
      "epoch: 3000  | loss: 0.00028834 \n",
      "epoch: 3000  | loss: 0.00021930 \n",
      "epoch: 3000  | loss: 0.00685570 \n",
      "epoch: 3000  | loss: 0.00036082 \n",
      "epoch: 3000  | loss: 0.00027378 \n",
      "epoch: 3100  | loss: 0.00185210 \n",
      "epoch: 3100  | loss: 0.00020453 \n",
      "epoch: 3100  | loss: 0.00478948 \n",
      "epoch: 3100  | loss: 0.00061148 \n",
      "epoch: 3100  | loss: 0.00040706 epoch: 3100  | loss: 0.00007132 \n",
      "\n",
      "epoch: 3100  | loss: 0.00028981 \n",
      "epoch: 3100  | loss: 0.00025934 \n",
      "epoch: 3100  | loss: 0.00601284 \n",
      "epoch: 3100  | loss: 0.00035876 \n",
      "epoch: 3100  | loss: 0.00027588 \n",
      "epoch: 3200  | loss: 0.00173859 \n",
      "epoch: 3200  | loss: 0.00022369 \n",
      "epoch: 3200  | loss: 0.00418417 \n",
      "epoch: 3200  | loss: 0.00060298 \n",
      "epoch: 3200  | loss: 0.00042093 \n",
      "epoch: 3200  | loss: 0.00040918 \n",
      "epoch: 3200  | loss: 0.00029125 \n",
      "epoch: 3200  | loss: 0.00017257 \n",
      "epoch: 3200  | loss: 0.00529916 \n",
      "epoch: 3200  | loss: 0.00035689 \n",
      "epoch: 3200  | loss: 0.00027797 \n",
      "epoch: 3300  | loss: 0.00164342 \n",
      "epoch: 3300  | loss: 0.00367917 epoch: 3300  | loss: 0.00022142 \n",
      "\n",
      "epoch: 3300  | loss: 0.00059567 \n",
      "epoch: 3300  | loss: 0.00041134 \n",
      "epoch: 3300  | loss: 0.00007121 \n",
      "epoch: 3300  | loss: 0.00469287 \n",
      "epoch: 3300  | loss: 0.00035518 \n",
      "epoch: 3300  | loss: 0.00014564 \n",
      "epoch: 3300  | loss: 0.00029265 \n",
      "epoch: 3300  | loss: 0.00028000 \n",
      "epoch: 3400  | loss: 0.00042388 \n",
      "epoch: 3400  | loss: 0.00156326 \n",
      "epoch: 3400  | loss: 0.00325864 \n",
      "epoch: 3400  | loss: 0.00041354 \n",
      "epoch: 3400  | loss: 0.00058934 \n",
      "epoch: 3400  | loss: 0.00007844 \n",
      "epoch: 3400  | loss: 0.00417717 \n",
      "epoch: 3400  | loss: 0.00035357 \n",
      "epoch: 3400  | loss: 0.00015248 \n",
      "epoch: 3400  | loss: 0.00029401 \n",
      "epoch: 3400  | loss: 0.00028295 \n",
      "epoch: 3500  | loss: 0.00021560 \n",
      "epoch: 3500  | loss: 0.00149543 \n",
      "epoch: 3500  | loss: 0.00058383 \n",
      "epoch: 3500  | loss: 0.00290929 \n",
      "epoch: 3500  | loss: 0.00041575 \n",
      "epoch: 3500  | loss: 0.00009013 \n",
      "epoch: 3500  | loss: 0.00029533 \n",
      "epoch: 3500  | loss: 0.00373873 \n",
      "epoch: 3500  | loss: 0.00035207 \n",
      "epoch: 3500  | loss: 0.00014484 \n",
      "epoch: 3500  | loss: 0.00028432 \n",
      "epoch: 3600  | loss: 0.00022029 \n",
      "epoch: 3600  | loss: 0.00143779 \n",
      "epoch: 3600  | loss: 0.00261979 \n",
      "epoch: 3600  | loss: 0.00057899 \n",
      "epoch: 3600  | loss: 0.00026145 \n",
      "epoch: 3600  | loss: 0.00041796 \n",
      "epoch: 3600  | loss: 0.00029659 \n",
      "epoch: 3600  | loss: 0.00035064 \n",
      "epoch: 3600  | loss: 0.00336624 \n",
      "epoch: 3600  | loss: 0.00014593 \n",
      "epoch: 3600  | loss: 0.00034771 \n",
      "epoch: 3700  | loss: 0.00022397 \n",
      "epoch: 3700  | loss: 0.00238039 \n",
      "epoch: 3700  | loss: 0.00138860 \n",
      "epoch: 3700  | loss: 0.00057476 \n",
      "epoch: 3700  | loss: 0.00142755 \n",
      "epoch: 3700  | loss: 0.00042016 \n",
      "epoch: 3700  | loss: 0.00029780 \n",
      "epoch: 3700  | loss: 0.00029091 \n",
      "epoch: 3700  | loss: 0.00305034 \n",
      "epoch: 3700  | loss: 0.00034927 \n",
      "epoch: 3700  | loss: 0.00028908 \n",
      "epoch: 3800  | loss: 0.00027580 \n",
      "epoch: 3800  | loss: 0.00134648 \n",
      "epoch: 3800  | loss: 0.00218275 \n",
      "epoch: 3800  | loss: 0.00057102 \n",
      "epoch: 3800  | loss: 0.00007354 \n",
      "epoch: 3800  | loss: 0.00042235 \n",
      "epoch: 3800  | loss: 0.00034795 \n",
      "epoch: 3800  | loss: 0.00278297 \n",
      "epoch: 3800  | loss: 0.00014368 \n",
      "epoch: 3800  | loss: 0.00029896 \n",
      "epoch: 3800  | loss: 0.00029694 \n",
      "epoch: 3900  | loss: 0.00131028 \n",
      "epoch: 3900  | loss: 0.00027190 \n",
      "epoch: 3900  | loss: 0.00201972 \n",
      "epoch: 3900  | loss: 0.00056768 \n",
      "epoch: 3900  | loss: 0.00042450 \n",
      "epoch: 3900  | loss: 0.00041914 \n",
      "epoch: 3900  | loss: 0.00030006 \n",
      "epoch: 3900  | loss: 0.00014338 \n",
      "epoch: 3900  | loss: 0.00255710 \n",
      "epoch: 3900  | loss: 0.00034667 \n",
      "epoch: 3900  | loss: 0.00043871 \n",
      "epoch: 4000  | loss: 0.00023660 \n",
      "epoch: 4000  | loss: 0.00127908 \n",
      "epoch: 4000  | loss: 0.00188527 \n",
      "epoch: 4000  | loss: 0.00056466 \n",
      "epoch: 4000  | loss: 0.00042661 \n",
      "epoch: 4000  | loss: 0.00007189 \n",
      "epoch: 4000  | loss: 0.00030110 \n",
      "epoch: 4000  | loss: 0.00034543 \n",
      "epoch: 4000  | loss: 0.00014292 \n",
      "epoch: 4000  | loss: 0.00236665 \n",
      "epoch: 4000  | loss: 0.00029584 \n",
      "epoch: 4100  | loss: 0.00023275 \n",
      "epoch: 4100  | loss: 0.00177433 \n",
      "epoch: 4100  | loss: 0.00056191 \n",
      "epoch: 4100  | loss: 0.00125212 \n",
      "epoch: 4100  | loss: 0.00042866 \n",
      "epoch: 4100  | loss: 0.00012800 \n",
      "epoch: 4100  | loss: 0.00030208 \n",
      "epoch: 4100  | loss: 0.00022736 \n",
      "epoch: 4100  | loss: 0.00034421 \n",
      "epoch: 4100  | loss: 0.00220638 \n",
      "epoch: 4100  | loss: 0.00044186 \n",
      "epoch: 4200  | loss: 0.00023382 \n",
      "epoch: 4200  | loss: 0.00122876 \n",
      "epoch: 4200  | loss: 0.00168266 \n",
      "epoch: 4200  | loss: 0.00055939 \n",
      "epoch: 4200  | loss: 0.00043066 \n",
      "epoch: 4200  | loss: 0.00007284 \n",
      "epoch: 4200  | loss: 0.00030299 \n",
      "epoch: 4200  | loss: 0.00207166 \n",
      "epoch: 4200  | loss: 0.00034300 \n",
      "epoch: 4200  | loss: 0.00014328 \n",
      "epoch: 4200  | loss: 0.00030109 \n",
      "epoch: 4300  | loss: 0.00036524 \n",
      "epoch: 4300  | loss: 0.00120849 \n",
      "epoch: 4300  | loss: 0.00055707 \n",
      "epoch: 4300  | loss: 0.00160677 \n",
      "epoch: 4300  | loss: 0.00043260 \n",
      "epoch: 4300  | loss: 0.00007490 \n",
      "epoch: 4300  | loss: 0.00023932 \n",
      "epoch: 4300  | loss: 0.00034180 \n",
      "epoch: 4300  | loss: 0.00030384 \n",
      "epoch: 4300  | loss: 0.00195866 \n",
      "epoch: 4300  | loss: 0.00030775 \n",
      "epoch: 4400  | loss: 0.00119086 \n",
      "epoch: 4400  | loss: 0.00024042 \n",
      "epoch: 4400  | loss: 0.00154376 \n",
      "epoch: 4400  | loss: 0.00055489 \n",
      "epoch: 4400  | loss: 0.00043445 \n",
      "epoch: 4400  | loss: 0.00017514 \n",
      "epoch: 4400  | loss: 0.00024587 \n",
      "epoch: 4400  | loss: 0.00034059 \n",
      "epoch: 4400  | loss: 0.00186377 \n",
      "epoch: 4400  | loss: 0.00030462 \n",
      "epoch: 4400  | loss: 0.00030692 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4500  | loss: 0.00117551 \n",
      "epoch: 4500  | loss: 0.00042260 \n",
      "epoch: 4500  | loss: 0.00149127 \n",
      "epoch: 4500  | loss: 0.00055285 \n",
      "epoch: 4500  | loss: 0.00043623 \n",
      "epoch: 4500  | loss: 0.00007188 \n",
      "epoch: 4500  | loss: 0.00024930 \n",
      "epoch: 4500  | loss: 0.00178406 \n",
      "epoch: 4500  | loss: 0.00030532 \n",
      "epoch: 4500  | loss: 0.00033937 \n",
      "epoch: 4500  | loss: 0.00054606 \n",
      "epoch: 4600  | loss: 0.00116211 \n",
      "epoch: 4600  | loss: 0.00041579 \n",
      "epoch: 4600  | loss: 0.00144738 \n",
      "epoch: 4600  | loss: 0.00055092 \n",
      "epoch: 4600  | loss: 0.00037425 \n",
      "epoch: 4600  | loss: 0.00043792 \n",
      "epoch: 4600  | loss: 0.00171710 \n",
      "epoch: 4600  | loss: 0.00033814 epoch: 4600  | loss: 0.00021962 \n",
      "\n",
      "epoch: 4600  | loss: 0.00030595 \n",
      "epoch: 4600  | loss: 0.00032077 \n",
      "epoch: 4700  | loss: 0.00115042 \n",
      "epoch: 4700  | loss: 0.00025976 \n",
      "epoch: 4700  | loss: 0.00141051 \n",
      "epoch: 4700  | loss: 0.00054911 \n",
      "epoch: 4700  | loss: 0.00007373 \n",
      "epoch: 4700  | loss: 0.00033689 \n",
      "epoch: 4700  | loss: 0.00043952 epoch: 4700  | loss: 0.00014427 \n",
      "\n",
      "epoch: 4700  | loss: 0.00166080 \n",
      "epoch: 4700  | loss: 0.00030664 \n",
      "epoch: 4700  | loss: 0.00032352 \n",
      "epoch: 4800  | loss: 0.00114019 \n",
      "epoch: 4800  | loss: 0.00137938 \n",
      "epoch: 4800  | loss: 0.00025492 \n",
      "epoch: 4800  | loss: 0.00054739 \n",
      "epoch: 4800  | loss: 0.00028238 \n",
      "epoch: 4800  | loss: 0.00044102 \n",
      "epoch: 4800  | loss: 0.00161341 \n",
      "epoch: 4800  | loss: 0.00030700 \n",
      "epoch: 4800  | loss: 0.00033561 \n",
      "epoch: 4800  | loss: 0.00014588 \n",
      "epoch: 4800  | loss: 0.00041584 \n",
      "epoch: 4900  | loss: 0.00113123 \n",
      "epoch: 4900  | loss: 0.00135296 \n",
      "epoch: 4900  | loss: 0.00048418 \n",
      "epoch: 4900  | loss: 0.00054571 \n",
      "epoch: 4900  | loss: 0.00007389 \n",
      "epoch: 4900  | loss: 0.00044242 \n",
      "epoch: 4900  | loss: 0.00033429 \n",
      "epoch: 4900  | loss: 0.00030740 \n",
      "epoch: 4900  | loss: 0.00157347 \n",
      "epoch: 4900  | loss: 0.00027092 \n",
      "epoch: 4900  | loss: 0.00032268 \n",
      "epoch: 5000  | loss: 0.00112336 \n",
      "epoch: 5000  | loss: 0.00046855 \n",
      "epoch: 5000  | loss: 0.00133041 \n",
      "epoch: 5000  | loss: 0.00054406 \n",
      "epoch: 5000  | loss: 0.00007356 \n",
      "epoch: 5000  | loss: 0.00033294 \n",
      "epoch: 5000  | loss: 0.00013911 \n",
      "epoch: 5000  | loss: 0.00030790 \n",
      "epoch: 5000  | loss: 0.00153973 \n",
      "epoch: 5000  | loss: 0.00044372 \n",
      "epoch: 5000  | loss: 0.00033039 \n",
      "epoch: 5100  | loss: 0.00111646 \n",
      "epoch: 5100  | loss: 0.00131106 \n",
      "epoch: 5100  | loss: 0.00030536 \n",
      "epoch: 5100  | loss: 0.00054243 \n",
      "epoch: 5100  | loss: 0.00007323 \n",
      "epoch: 5100  | loss: 0.00030844 \n",
      "epoch: 5100  | loss: 0.00151118 \n",
      "epoch: 5100  | loss: 0.00013840 \n",
      "epoch: 5100  | loss: 0.00033155 \n",
      "epoch: 5100  | loss: 0.00044490 \n",
      "epoch: 5100  | loss: 0.00033864 \n",
      "epoch: 5200  | loss: 0.00111037 \n",
      "epoch: 5200  | loss: 0.00029083 \n",
      "epoch: 5200  | loss: 0.00129434 \n",
      "epoch: 5200  | loss: 0.00054080 \n",
      "epoch: 5200  | loss: 0.00007288 \n",
      "epoch: 5200  | loss: 0.00148695 \n",
      "epoch: 5200  | loss: 0.00030889 \n",
      "epoch: 5200  | loss: 0.00033012 \n",
      "epoch: 5200  | loss: 0.00044597 \n",
      "epoch: 5200  | loss: 0.00018457 \n",
      "epoch: 5200  | loss: 0.00033613 \n",
      "epoch: 5300  | loss: 0.00110501 \n",
      "epoch: 5300  | loss: 0.00027390 \n",
      "epoch: 5300  | loss: 0.00127981 \n",
      "epoch: 5300  | loss: 0.00053915 \n",
      "epoch: 5300  | loss: 0.00007754 \n",
      "epoch: 5300  | loss: 0.00146634 \n",
      "epoch: 5300  | loss: 0.00030906 \n",
      "epoch: 5300  | loss: 0.00015573 epoch: 5300  | loss: 0.00032863 \n",
      "\n",
      "epoch: 5300  | loss: 0.00044692 \n",
      "epoch: 5300  | loss: 0.00033809 \n",
      "epoch: 5400  | loss: 0.00110025 \n",
      "epoch: 5400  | loss: 0.00126710 \n",
      "epoch: 5400  | loss: 0.00044297 \n",
      "epoch: 5400  | loss: 0.00007319 \n",
      "epoch: 5400  | loss: 0.00053748 \n",
      "epoch: 5400  | loss: 0.00030930 \n",
      "epoch: 5400  | loss: 0.00144874 \n",
      "epoch: 5400  | loss: 0.00016124 epoch: 5400  | loss: 0.00032709 \n",
      "\n",
      "epoch: 5400  | loss: 0.00044777 \n",
      "epoch: 5400  | loss: 0.00034811 \n",
      "epoch: 5500  | loss: 0.00125591 \n",
      "epoch: 5500  | loss: 0.00109603 \n",
      "epoch: 5500  | loss: 0.00031031 \n",
      "epoch: 5500  | loss: 0.00053578 \n",
      "epoch: 5500  | loss: 0.00009654 \n",
      "epoch: 5500  | loss: 0.00030946 epoch: 5500  | loss: 0.00032549 \n",
      "\n",
      "epoch: 5500  | loss: 0.00143366 \n",
      "epoch: 5500  | loss: 0.00014211 \n",
      "epoch: 5500  | loss: 0.00044849 \n",
      "epoch: 5500  | loss: 0.00034905 \n",
      "epoch: 5600  | loss: 0.00109226 \n",
      "epoch: 5600  | loss: 0.00124597 \n",
      "epoch: 5600  | loss: 0.00029555 \n",
      "epoch: 5600  | loss: 0.00053403 \n",
      "epoch: 5600  | loss: 0.00007451 \n",
      "epoch: 5600  | loss: 0.00030972 \n",
      "epoch: 5600  | loss: 0.00032382 \n",
      "epoch: 5600  | loss: 0.00142069 \n",
      "epoch: 5600  | loss: 0.00013979 \n",
      "epoch: 5600  | loss: 0.00044909 \n",
      "epoch: 5600  | loss: 0.00036617 \n",
      "epoch: 5700  | loss: 0.00108887 \n",
      "epoch: 5700  | loss: 0.00123709 \n",
      "epoch: 5700  | loss: 0.00032499 \n",
      "epoch: 5700  | loss: 0.00007330 \n",
      "epoch: 5700  | loss: 0.00053227 \n",
      "epoch: 5700  | loss: 0.00030949 \n",
      "epoch: 5700  | loss: 0.00032208 \n",
      "epoch: 5700  | loss: 0.00140949 \n",
      "epoch: 5700  | loss: 0.00017119 \n",
      "epoch: 5700  | loss: 0.00044956 \n",
      "epoch: 5700  | loss: 0.00035700 \n",
      "epoch: 5800  | loss: 0.00122909 \n",
      "epoch: 5800  | loss: 0.00108581 \n",
      "epoch: 5800  | loss: 0.00017481 \n",
      "epoch: 5800  | loss: 0.00037807 \n",
      "epoch: 5800  | loss: 0.00053047 \n",
      "epoch: 5800  | loss: 0.00030939 \n",
      "epoch: 5800  | loss: 0.00032031 \n",
      "epoch: 5800  | loss: 0.00139978 \n",
      "epoch: 5800  | loss: 0.00014277 epoch: 5800  | loss: 0.00044990 \n",
      "\n",
      "epoch: 5800  | loss: 0.00037501 \n",
      "epoch: 5900  | loss: 0.00122181 \n",
      "epoch: 5900  | loss: 0.00108302 \n",
      "epoch: 5900  | loss: 0.00031254 \n",
      "epoch: 5900  | loss: 0.00052863 \n",
      "epoch: 5900  | loss: 0.00007394 \n",
      "epoch: 5900  | loss: 0.00030920 \n",
      "epoch: 5900  | loss: 0.00031853 \n",
      "epoch: 5900  | loss: 0.00139131 \n",
      "epoch: 5900  | loss: 0.00045010 epoch: 5900  | loss: 0.00013423 \n",
      "\n",
      "epoch: 5900  | loss: 0.00036126 \n",
      "epoch: 6000  | loss: 0.00121513 epoch: 6000  | loss: 0.00108046 \n",
      "\n",
      "epoch: 6000  | loss: 0.00030355 \n",
      "epoch: 6000  | loss: 0.00007354 \n",
      "epoch: 6000  | loss: 0.00052671 \n",
      "epoch: 6000  | loss: 0.00030980 \n",
      "epoch: 6000  | loss: 0.00031669 \n",
      "epoch: 6000  | loss: 0.00138389 \n",
      "epoch: 6000  | loss: 0.00013445 \n",
      "epoch: 6000  | loss: 0.00045015 \n",
      "epoch: 6000  | loss: 0.00036610 \n",
      "epoch: 6100  | loss: 0.00107808 \n",
      "epoch: 6100  | loss: 0.00120894 \n",
      "epoch: 6100  | loss: 0.00030848 \n",
      "epoch: 6100  | loss: 0.00052472 \n",
      "epoch: 6100  | loss: 0.00007609 \n",
      "epoch: 6100  | loss: 0.00030855 \n",
      "epoch: 6100  | loss: 0.00137733 \n",
      "epoch: 6100  | loss: 0.00031478 \n",
      "epoch: 6100  | loss: 0.00045005 epoch: 6100  | loss: 0.00021829 \n",
      "\n",
      "epoch: 6100  | loss: 0.00037050 \n",
      "epoch: 6200  | loss: 0.00107583 \n",
      "epoch: 6200  | loss: 0.00120315 \n",
      "epoch: 6200  | loss: 0.00008902 \n",
      "epoch: 6200  | loss: 0.00052266 \n",
      "epoch: 6200  | loss: 0.00030947 \n",
      "epoch: 6200  | loss: 0.00030811 \n",
      "epoch: 6200  | loss: 0.00137151 \n",
      "epoch: 6200  | loss: 0.00031279 \n",
      "epoch: 6200  | loss: 0.00018660 \n",
      "epoch: 6200  | loss: 0.00044980 \n",
      "epoch: 6200  | loss: 0.00038546 \n",
      "epoch: 6300  | loss: 0.00107369 \n",
      "epoch: 6300  | loss: 0.00119767 \n",
      "epoch: 6300  | loss: 0.00031989 \n",
      "epoch: 6300  | loss: 0.00052050 \n",
      "epoch: 6300  | loss: 0.00007645 \n",
      "epoch: 6300  | loss: 0.00136628 epoch: 6300  | loss: 0.00031071 \n",
      "\n",
      "epoch: 6300  | loss: 0.00013203 \n",
      "epoch: 6300  | loss: 0.00030753 \n",
      "epoch: 6300  | loss: 0.00042024 \n",
      "epoch: 6300  | loss: 0.00044938 \n",
      "epoch: 6400  | loss: 0.00107163 \n",
      "epoch: 6400  | loss: 0.00119245 \n",
      "epoch: 6400  | loss: 0.00051825 epoch: 6400  | loss: 0.00008509 \n",
      "\n",
      "epoch: 6400  | loss: 0.00031864 \n",
      "epoch: 6400  | loss: 0.00030854 epoch: 6400  | loss: 0.00136156 \n",
      "\n",
      "epoch: 6400  | loss: 0.00030688 \n",
      "epoch: 6400  | loss: 0.00013295 \n",
      "epoch: 6400  | loss: 0.00038368 \n",
      "epoch: 6400  | loss: 0.00044879 \n",
      "epoch: 6500  | loss: 0.00106960 \n",
      "epoch: 6500  | loss: 0.00118741 \n",
      "epoch: 6500  | loss: 0.00051590 \n",
      "epoch: 6500  | loss: 0.00011296 \n",
      "epoch: 6500  | loss: 0.00036012 \n",
      "epoch: 6500  | loss: 0.00030626 \n",
      "epoch: 6500  | loss: 0.00135723 \n",
      "epoch: 6500  | loss: 0.00030614 \n",
      "epoch: 6500  | loss: 0.00013175 \n",
      "epoch: 6500  | loss: 0.00044803 \n",
      "epoch: 6500  | loss: 0.00043978 \n",
      "epoch: 6600  | loss: 0.00118251 \n",
      "epoch: 6600  | loss: 0.00106760 \n",
      "epoch: 6600  | loss: 0.00008599 \n",
      "epoch: 6600  | loss: 0.00051344 \n",
      "epoch: 6600  | loss: 0.00041495 \n",
      "epoch: 6600  | loss: 0.00135323 \n",
      "epoch: 6600  | loss: 0.00030388 \n",
      "epoch: 6600  | loss: 0.00030533 \n",
      "epoch: 6600  | loss: 0.00015894 \n",
      "epoch: 6600  | loss: 0.00040176 \n",
      "epoch: 6600  | loss: 0.00044708 \n",
      "epoch: 6700  | loss: 0.00106558 \n",
      "epoch: 6700  | loss: 0.00051087 \n",
      "epoch: 6700  | loss: 0.00008025 \n",
      "epoch: 6700  | loss: 0.00117770 \n",
      "epoch: 6700  | loss: 0.00033187 \n",
      "epoch: 6700  | loss: 0.00030447 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6700  | loss: 0.00134948 \n",
      "epoch: 6700  | loss: 0.00030139 \n",
      "epoch: 6700  | loss: 0.00017465 \n",
      "epoch: 6700  | loss: 0.00040358 \n",
      "epoch: 6700  | loss: 0.00044594 \n",
      "epoch: 6800  | loss: 0.00106353 \n",
      "epoch: 6800  | loss: 0.00117295 \n",
      "epoch: 6800  | loss: 0.00018786 \n",
      "epoch: 6800  | loss: 0.00050817 \n",
      "epoch: 6800  | loss: 0.00033621 \n",
      "epoch: 6800  | loss: 0.00134592 \n",
      "epoch: 6800  | loss: 0.00030335 \n",
      "epoch: 6800  | loss: 0.00029878 \n",
      "epoch: 6800  | loss: 0.00020665 \n",
      "epoch: 6800  | loss: 0.00042078 \n",
      "epoch: 6800  | loss: 0.00044460 \n",
      "epoch: 6900  | loss: 0.00106143 \n",
      "epoch: 6900  | loss: 0.00116823 \n",
      "epoch: 6900  | loss: 0.00016938 \n",
      "epoch: 6900  | loss: 0.00050535 \n",
      "epoch: 6900  | loss: 0.00036771 \n",
      "epoch: 6900  | loss: 0.00134251 \n",
      "epoch: 6900  | loss: 0.00030224 \n",
      "epoch: 6900  | loss: 0.00029603 \n",
      "epoch: 6900  | loss: 0.00012879 \n",
      "epoch: 6900  | loss: 0.00040864 \n",
      "epoch: 6900  | loss: 0.00044306 \n",
      "epoch: 7000  | loss: 0.00116350 \n",
      "epoch: 7000  | loss: 0.00105926 \n",
      "epoch: 7000  | loss: 0.00034396 \n",
      "epoch: 7000  | loss: 0.00009583 \n",
      "epoch: 7000  | loss: 0.00050239 \n",
      "epoch: 7000  | loss: 0.00133919 \n",
      "epoch: 7000  | loss: 0.00030105 \n",
      "epoch: 7000  | loss: 0.00014533 \n",
      "epoch: 7000  | loss: 0.00029314 \n",
      "epoch: 7000  | loss: 0.00041423 \n",
      "epoch: 7000  | loss: 0.00044129 \n",
      "epoch: 7100  | loss: 0.00115874 \n",
      "epoch: 7100  | loss: 0.00105699 \n",
      "epoch: 7100  | loss: 0.00010477 \n",
      "epoch: 7100  | loss: 0.00035250 epoch: 7100  | loss: 0.00049939 \n",
      "\n",
      "epoch: 7100  | loss: 0.00133591 \n",
      "epoch: 7100  | loss: 0.00029976 \n",
      "epoch: 7100  | loss: 0.00042039 \n",
      "epoch: 7100  | loss: 0.00013299 \n",
      "epoch: 7100  | loss: 0.00029011 \n",
      "epoch: 7100  | loss: 0.00043931 \n",
      "epoch: 7200  | loss: 0.00115395 \n",
      "epoch: 7200  | loss: 0.00105462 \n",
      "epoch: 7200  | loss: 0.00049642 \n",
      "epoch: 7200  | loss: 0.00008327 \n",
      "epoch: 7200  | loss: 0.00035807 \n",
      "epoch: 7200  | loss: 0.00133266 \n",
      "epoch: 7200  | loss: 0.00029841 \n",
      "epoch: 7200  | loss: 0.00042677 \n",
      "epoch: 7200  | loss: 0.00028692 \n",
      "epoch: 7200  | loss: 0.00012836 \n",
      "epoch: 7200  | loss: 0.00043706 \n",
      "epoch: 7300  | loss: 0.00105214 \n",
      "epoch: 7300  | loss: 0.00114920 \n",
      "epoch: 7300  | loss: 0.00049333 \n",
      "epoch: 7300  | loss: 0.00037641 \n",
      "epoch: 7300  | loss: 0.00132938 \n",
      "epoch: 7300  | loss: 0.00009988 \n",
      "epoch: 7300  | loss: 0.00029696 \n",
      "epoch: 7300  | loss: 0.00042936 \n",
      "epoch: 7300  | loss: 0.00021743 \n",
      "epoch: 7300  | loss: 0.00028357 \n",
      "epoch: 7300  | loss: 0.00043458 \n",
      "epoch: 7400  | loss: 0.00104951 \n",
      "epoch: 7400  | loss: 0.00114439 \n",
      "epoch: 7400  | loss: 0.00049010 \n",
      "epoch: 7400  | loss: 0.00043567 \n",
      "epoch: 7400  | loss: 0.00132606 \n",
      "epoch: 7400  | loss: 0.00012180 \n",
      "epoch: 7400  | loss: 0.00029695 \n",
      "epoch: 7400  | loss: 0.00043507 \n",
      "epoch: 7400  | loss: 0.00028014 \n",
      "epoch: 7400  | loss: 0.00012665 \n",
      "epoch: 7400  | loss: 0.00043184 \n",
      "epoch: 7500  | loss: 0.00104674 \n",
      "epoch: 7500  | loss: 0.00113950 \n",
      "epoch: 7500  | loss: 0.00048674 \n",
      "epoch: 7500  | loss: 0.00055726 \n",
      "epoch: 7500  | loss: 0.00132266 \n",
      "epoch: 7500  | loss: 0.00011926 \n",
      "epoch: 7500  | loss: 0.00029392 \n",
      "epoch: 7500  | loss: 0.00050180 \n",
      "epoch: 7500  | loss: 0.00012603 \n",
      "epoch: 7500  | loss: 0.00027655 \n",
      "epoch: 7500  | loss: 0.00042883 \n",
      "epoch: 7600  | loss: 0.00113453 \n",
      "epoch: 7600  | loss: 0.00104382 \n",
      "epoch: 7600  | loss: 0.00037107 \n",
      "epoch: 7600  | loss: 0.00048328 \n",
      "epoch: 7600  | loss: 0.00131917 \n",
      "epoch: 7600  | loss: 0.00009225 \n",
      "epoch: 7600  | loss: 0.00029667 \n",
      "epoch: 7600  | loss: 0.00044899 \n",
      "epoch: 7600  | loss: 0.00012427 \n",
      "epoch: 7600  | loss: 0.00027280 \n",
      "epoch: 7600  | loss: 0.00042560 \n",
      "epoch: 7700  | loss: 0.00112944 \n",
      "epoch: 7700  | loss: 0.00104075 \n",
      "epoch: 7700  | loss: 0.00047965 \n",
      "epoch: 7700  | loss: 0.00038418 \n",
      "epoch: 7700  | loss: 0.00007609 \n",
      "epoch: 7700  | loss: 0.00131558 \n",
      "epoch: 7700  | loss: 0.00029234 \n",
      "epoch: 7700  | loss: 0.00045329 \n",
      "epoch: 7700  | loss: 0.00012354 \n",
      "epoch: 7700  | loss: 0.00026887 \n",
      "epoch: 7700  | loss: 0.00042239 \n",
      "epoch: 7800  | loss: 0.00112424 \n",
      "epoch: 7800  | loss: 0.00103749 \n",
      "epoch: 7800  | loss: 0.00057070 \n",
      "epoch: 7800  | loss: 0.00047585 \n",
      "epoch: 7800  | loss: 0.00131187 \n",
      "epoch: 7800  | loss: 0.00021413 \n",
      "epoch: 7800  | loss: 0.00029401 \n",
      "epoch: 7800  | loss: 0.00071848 \n",
      "epoch: 7800  | loss: 0.00012476 \n",
      "epoch: 7800  | loss: 0.00026480 \n",
      "epoch: 7800  | loss: 0.00041902 \n",
      "epoch: 7900  | loss: 0.00111892 \n",
      "epoch: 7900  | loss: 0.00103405 \n",
      "epoch: 7900  | loss: 0.00037983 \n",
      "epoch: 7900  | loss: 0.00047185 \n",
      "epoch: 7900  | loss: 0.00130802 \n",
      "epoch: 7900  | loss: 0.00011749 \n",
      "epoch: 7900  | loss: 0.00053203 \n",
      "epoch: 7900  | loss: 0.00028771 \n",
      "epoch: 7900  | loss: 0.00012304 \n",
      "epoch: 7900  | loss: 0.00026059 \n",
      "epoch: 7900  | loss: 0.00041537 \n",
      "epoch: 8000  | loss: 0.00103042 \n",
      "epoch: 8000  | loss: 0.00111347 \n",
      "epoch: 8000  | loss: 0.00038552 \n",
      "epoch: 8000  | loss: 0.00046770 \n",
      "epoch: 8000  | loss: 0.00009321 \n",
      "epoch: 8000  | loss: 0.00130401 \n",
      "epoch: 8000  | loss: 0.00028602 \n",
      "epoch: 8000  | loss: 0.00046596 \n",
      "epoch: 8000  | loss: 0.00025628 \n",
      "epoch: 8000  | loss: 0.00019624 \n",
      "epoch: 8000  | loss: 0.00041145 \n",
      "epoch: 8100  | loss: 0.00102658 \n",
      "epoch: 8100  | loss: 0.00110788 \n",
      "epoch: 8100  | loss: 0.00049512 epoch: 8100  | loss: 0.00046338 \n",
      "\n",
      "epoch: 8100  | loss: 0.00017914 epoch: 8100  | loss: 0.00129983 \n",
      "\n",
      "epoch: 8100  | loss: 0.00028430 \n",
      "epoch: 8100  | loss: 0.00047143 \n",
      "epoch: 8100  | loss: 0.00025176 \n",
      "epoch: 8100  | loss: 0.00014566 \n",
      "epoch: 8100  | loss: 0.00040739 \n",
      "epoch: 8200  | loss: 0.00110218 \n",
      "epoch: 8200  | loss: 0.00102253 \n",
      "epoch: 8200  | loss: 0.00045884 \n",
      "epoch: 8200  | loss: 0.00039868 \n",
      "epoch: 8200  | loss: 0.00129546 \n",
      "epoch: 8200  | loss: 0.00007864 \n",
      "epoch: 8200  | loss: 0.00024703 \n",
      "epoch: 8200  | loss: 0.00056702 \n",
      "epoch: 8200  | loss: 0.00028249 \n",
      "epoch: 8200  | loss: 0.00012161 \n",
      "epoch: 8200  | loss: 0.00040358 \n",
      "epoch: 8300  | loss: 0.00109638 \n",
      "epoch: 8300  | loss: 0.00101826 \n",
      "epoch: 8300  | loss: 0.00045408 \n",
      "epoch: 8300  | loss: 0.00046701 \n",
      "epoch: 8300  | loss: 0.00129090 \n",
      "epoch: 8300  | loss: 0.00009760 \n",
      "epoch: 8300  | loss: 0.00028078 \n",
      "epoch: 8300  | loss: 0.00024200 \n",
      "epoch: 8300  | loss: 0.00068983 \n",
      "epoch: 8300  | loss: 0.00012023 \n",
      "epoch: 8300  | loss: 0.00039958 \n",
      "epoch: 8400  | loss: 0.00109044 \n",
      "epoch: 8400  | loss: 0.00101377 \n",
      "epoch: 8400  | loss: 0.00044906 \n",
      "epoch: 8400  | loss: 0.00053387 \n",
      "epoch: 8400  | loss: 0.00128614 \n",
      "epoch: 8400  | loss: 0.00012187 \n",
      "epoch: 8400  | loss: 0.00023658 \n",
      "epoch: 8400  | loss: 0.00048723 \n",
      "epoch: 8400  | loss: 0.00012713 \n",
      "epoch: 8400  | loss: 0.00027899 \n",
      "epoch: 8400  | loss: 0.00039521 \n",
      "epoch: 8500  | loss: 0.00108435 epoch: 8500  | loss: 0.00100905 \n",
      "\n",
      "epoch: 8500  | loss: 0.00044384 \n",
      "epoch: 8500  | loss: 0.00040362 \n",
      "epoch: 8500  | loss: 0.00007523 \n",
      "epoch: 8500  | loss: 0.00128117 \n",
      "epoch: 8500  | loss: 0.00023079 \n",
      "epoch: 8500  | loss: 0.00049145 \n",
      "epoch: 8500  | loss: 0.00027720 \n",
      "epoch: 8500  | loss: 0.00011809 \n",
      "epoch: 8500  | loss: 0.00039064 \n",
      "epoch: 8600  | loss: 0.00107811 \n",
      "epoch: 8600  | loss: 0.00100411 \n",
      "epoch: 8600  | loss: 0.00040807 \n",
      "epoch: 8600  | loss: 0.00043838 \n",
      "epoch: 8600  | loss: 0.00010300 \n",
      "epoch: 8600  | loss: 0.00022613 \n",
      "epoch: 8600  | loss: 0.00127598 \n",
      "epoch: 8600  | loss: 0.00063769 \n",
      "epoch: 8600  | loss: 0.00011775 \n",
      "epoch: 8600  | loss: 0.00027560 \n",
      "epoch: 8600  | loss: 0.00038568 \n",
      "epoch: 8700  | loss: 0.00099892 \n",
      "epoch: 8700  | loss: 0.00107173 \n",
      "epoch: 8700  | loss: 0.00079215 \n",
      "epoch: 8700  | loss: 0.00015832 \n",
      "epoch: 8700  | loss: 0.00043263 \n",
      "epoch: 8700  | loss: 0.00127057 \n",
      "epoch: 8700  | loss: 0.00022428 \n",
      "epoch: 8700  | loss: 0.00055958 \n",
      "epoch: 8700  | loss: 0.00011801 \n",
      "epoch: 8700  | loss: 0.00027660 \n",
      "epoch: 8700  | loss: 0.00038054 \n",
      "epoch: 8800  | loss: 0.00106520 \n",
      "epoch: 8800  | loss: 0.00099350 \n",
      "epoch: 8800  | loss: 0.00041519 \n",
      "epoch: 8800  | loss: 0.00042655 \n",
      "epoch: 8800  | loss: 0.00007846 \n",
      "epoch: 8800  | loss: 0.00126494 \n",
      "epoch: 8800  | loss: 0.00022219 \n",
      "epoch: 8800  | loss: 0.00050864 \n",
      "epoch: 8800  | loss: 0.00016098 \n",
      "epoch: 8800  | loss: 0.00027182 \n",
      "epoch: 8800  | loss: 0.00037534 \n",
      "epoch: 8900  | loss: 0.00105851 \n",
      "epoch: 8900  | loss: 0.00098783 \n",
      "epoch: 8900  | loss: 0.00060076 \n",
      "epoch: 8900  | loss: 0.00042010 \n",
      "epoch: 8900  | loss: 0.00010105 \n",
      "epoch: 8900  | loss: 0.00125907 \n",
      "epoch: 8900  | loss: 0.00021996 \n",
      "epoch: 8900  | loss: 0.00051234 \n",
      "epoch: 8900  | loss: 0.00011820 \n",
      "epoch: 8900  | loss: 0.00027021 \n",
      "epoch: 8900  | loss: 0.00036981 \n",
      "epoch: 9000  | loss: 0.00098192 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9000  | loss: 0.00105168 \n",
      "epoch: 9000  | loss: 0.00042323 \n",
      "epoch: 9000  | loss: 0.00041343 \n",
      "epoch: 9000  | loss: 0.00011183 \n",
      "epoch: 9000  | loss: 0.00125298 \n",
      "epoch: 9000  | loss: 0.00021794 \n",
      "epoch: 9000  | loss: 0.00051944 \n",
      "epoch: 9000  | loss: 0.00026836 \n",
      "epoch: 9000  | loss: 0.00019180 \n",
      "epoch: 9000  | loss: 0.00036448 \n",
      "epoch: 9100  | loss: 0.00097583 \n",
      "epoch: 9100  | loss: 0.00104471 \n",
      "epoch: 9100  | loss: 0.00047638 \n",
      "epoch: 9100  | loss: 0.00040659 \n",
      "epoch: 9100  | loss: 0.00012622 \n",
      "epoch: 9100  | loss: 0.00124665 \n",
      "epoch: 9100  | loss: 0.00071008 \n",
      "epoch: 9100  | loss: 0.00021533 \n",
      "epoch: 9100  | loss: 0.00011706 \n",
      "epoch: 9100  | loss: 0.00026701 \n",
      "epoch: 9100  | loss: 0.00035870 \n",
      "epoch: 9200  | loss: 0.00096964 \n",
      "epoch: 9200  | loss: 0.00103759 \n",
      "epoch: 9200  | loss: 0.00008138 \n",
      "epoch: 9200  | loss: 0.00134838 \n",
      "epoch: 9200  | loss: 0.00039970 \n",
      "epoch: 9200  | loss: 0.00124010 \n",
      "epoch: 9200  | loss: 0.00055608 \n",
      "epoch: 9200  | loss: 0.00021289 \n",
      "epoch: 9200  | loss: 0.00015174 \n",
      "epoch: 9200  | loss: 0.00026441 \n",
      "epoch: 9200  | loss: 0.00035277 \n",
      "epoch: 9300  | loss: 0.00096321 \n",
      "epoch: 9300  | loss: 0.00103034 \n",
      "epoch: 9300  | loss: 0.00072265 \n",
      "epoch: 9300  | loss: 0.00039364 \n",
      "epoch: 9300  | loss: 0.00007695 \n",
      "epoch: 9300  | loss: 0.00123331 \n",
      "epoch: 9300  | loss: 0.00021058 \n",
      "epoch: 9300  | loss: 0.00053266 \n",
      "epoch: 9300  | loss: 0.00026373 \n",
      "epoch: 9300  | loss: 0.00011743 \n",
      "epoch: 9300  | loss: 0.00034737 \n",
      "epoch: 9400  | loss: 0.00095655 \n",
      "epoch: 9400  | loss: 0.00102295 \n",
      "epoch: 9400  | loss: 0.00068527 \n",
      "epoch: 9400  | loss: 0.00038879 \n",
      "epoch: 9400  | loss: 0.00013211 \n",
      "epoch: 9400  | loss: 0.00122629 \n",
      "epoch: 9400  | loss: 0.00053693 \n",
      "epoch: 9400  | loss: 0.00020828 \n",
      "epoch: 9400  | loss: 0.00011523 \n",
      "epoch: 9400  | loss: 0.00026103 \n",
      "epoch: 9400  | loss: 0.00034063 \n",
      "epoch: 9500  | loss: 0.00094965 \n",
      "epoch: 9500  | loss: 0.00101546 \n",
      "epoch: 9500  | loss: 0.00038538 \n",
      "epoch: 9500  | loss: 0.00007178 \n",
      "epoch: 9500  | loss: 0.00050118 \n",
      "epoch: 9500  | loss: 0.00121905 \n",
      "epoch: 9500  | loss: 0.00054219 \n",
      "epoch: 9500  | loss: 0.00020596 \n",
      "epoch: 9500  | loss: 0.00026091 \n",
      "epoch: 9500  | loss: 0.00013941 \n",
      "epoch: 9500  | loss: 0.00033423 \n",
      "epoch: 9600  | loss: 0.00094251 \n",
      "epoch: 9600  | loss: 0.00100800 \n",
      "epoch: 9600  | loss: 0.00007163 \n",
      "epoch: 9600  | loss: 0.00038189 \n",
      "epoch: 9600  | loss: 0.00121158 \n",
      "epoch: 9600  | loss: 0.00044559 \n",
      "epoch: 9600  | loss: 0.00059598 \n",
      "epoch: 9600  | loss: 0.00020405 \n",
      "epoch: 9600  | loss: 0.00025845 \n",
      "epoch: 9600  | loss: 0.00011459 \n",
      "epoch: 9600  | loss: 0.00032768 \n",
      "epoch: 9700  | loss: 0.00100045 \n",
      "epoch: 9700  | loss: 0.00093514 \n",
      "epoch: 9700  | loss: 0.00007175 \n",
      "epoch: 9700  | loss: 0.00120389 epoch: 9700  | loss: 0.00044902 \n",
      "\n",
      "epoch: 9700  | loss: 0.00037826 \n",
      "epoch: 9700  | loss: 0.00055149 \n",
      "epoch: 9700  | loss: 0.00020134 \n",
      "epoch: 9700  | loss: 0.00025599 \n",
      "epoch: 9700  | loss: 0.00011156 \n",
      "epoch: 9700  | loss: 0.00032110 \n",
      "epoch: 9800  | loss: 0.00092753 \n",
      "epoch: 9800  | loss: 0.00099279 \n",
      "epoch: 9800  | loss: 0.00007906 \n",
      "epoch: 9800  | loss: 0.00045327 epoch: 9800  | loss: 0.00119599 \n",
      "\n",
      "epoch: 9800  | loss: 0.00037449 \n",
      "epoch: 9800  | loss: 0.00056231 \n",
      "epoch: 9800  | loss: 0.00019936 \n",
      "epoch: 9800  | loss: 0.00025453 \n",
      "epoch: 9800  | loss: 0.00023170 \n",
      "epoch: 9800  | loss: 0.00031502 \n",
      "epoch: 9900  | loss: 0.00091969 \n",
      "epoch: 9900  | loss: 0.00098503 \n",
      "epoch: 9900  | loss: 0.00010599 \n",
      "epoch: 9900  | loss: 0.00118789 \n",
      "epoch: 9900  | loss: 0.00051604 \n",
      "epoch: 9900  | loss: 0.00037060 \n",
      "epoch: 9900  | loss: 0.00019672 \n",
      "epoch: 9900  | loss: 0.00056113 \n",
      "epoch: 9900  | loss: 0.00025354 \n",
      "epoch: 9900  | loss: 0.00015075 \n",
      "epoch: 9900  | loss: 0.00030978 \n",
      "pred_acc = 24/36=0.6666666666666666\n",
      "real_acc = 0.8611111111111112\n",
      "train_num=24\n",
      "seed=0\n",
      "90\n",
      "eta2=4.641588833612782e-05\n",
      "epoch: 0  | loss: 1.05029035 \n",
      "pred_acc = 24/36=0.6666666666666666\n",
      "real_acc = 0.8333333333333334\n",
      "train_num=24\n",
      "seed=0\n",
      "pred_acc = 36/36=1.0\n",
      "real_acc = 1.0\n",
      "90\n",
      "eta2=2.1544346900318823e-05\n",
      "epoch: 0  | loss: 1.05029035 \n",
      "pred_acc = 24/36=0.6666666666666666\n",
      "real_acc = 0.8333333333333334\n",
      "train_num=24pred_acc = 36/36=1.0\n",
      "real_acc = 1.0\n",
      "\n",
      "seed=0\n",
      "train_num=2490\n",
      "eta2=1e-05\n",
      "\n",
      "epoch: 0  | loss: 1.05029035 \n",
      "seed=0\n",
      "90train_num=24\n",
      "pred_acc = 36/36=1.0\n",
      "real_acc = 1.0\n",
      "eta2=2.1544346900318823e-05\n",
      "\n",
      "seed=0epoch: 0  | loss: 1.05029035 \n",
      "\n",
      "train_num=2490\n",
      "\n",
      "seed=0eta2=0.01\n",
      "\n",
      "90\n",
      "eta2=0.0001epoch: 0  | loss: 1.05029035 \n",
      "\n",
      "epoch: 0  | loss: 1.05029035 \n",
      "pred_acc = 36/36=1.0\n",
      "real_acc = 1.0\n",
      "train_num=24\n",
      "seed=0\n",
      "90\n",
      "eta2=0.004641588833612777\n",
      "epoch: 0  | loss: 1.05029035 \n",
      "pred_acc = 36/36=1.0\n",
      "real_acc = 1.0\n",
      "train_num=24pred_acc = 36/36=1.0\n",
      "real_acc = 1.0\n",
      "\n",
      "train_num=24seed=0\n",
      "\n",
      "seed=090\n",
      "\n",
      "90eta2=0.00021544346900318823\n",
      "\n",
      "eta2=0.001\n",
      "epoch: 0  | loss: 1.05029035 \n",
      "epoch: 0  | loss: 1.05029035 \n",
      "pred_acc = 36/36=1.0\n",
      "real_acc = 1.0\n",
      "train_num=24\n",
      "seed=0\n",
      "90\n",
      "eta2=0.002154434690031882\n",
      "epoch: 0  | loss: 1.05029035 \n",
      "pred_acc = 36/36=1.0\n",
      "real_acc = 1.0\n",
      "train_num=24\n",
      "seed=0\n",
      "90\n",
      "eta2=0.00046415888336127773\n",
      "epoch: 0  | loss: 1.05029035 \n",
      "epoch: 100  | loss: 0.90805340 \n",
      "epoch: 100  | loss: 0.96540755 \n",
      "epoch: 100  | loss: 0.96617413 \n",
      "epoch: 100  | loss: 1.00522554 \n",
      "epoch: 100  | loss: 0.16347845 \n",
      "epoch: 100  | loss: 0.88191420 \n",
      "epoch: 100  | loss: 0.38946620 \n",
      "epoch: 100  | loss: 0.84773833 \n",
      "epoch: 100  | loss: 0.68112957 \n",
      "epoch: 100  | loss: 0.46970916 \n",
      "epoch: 100  | loss: 0.75128496 \n",
      "epoch: 200  | loss: 0.75350040 \n",
      "epoch: 200  | loss: 0.85773820 \n",
      "epoch: 200  | loss: 0.94191843 \n",
      "epoch: 200  | loss: 0.86140478 \n",
      "epoch: 200  | loss: 0.00299695 \n",
      "epoch: 200  | loss: 0.59306955 \n",
      "epoch: 200  | loss: 0.05488572 \n",
      "epoch: 200  | loss: 0.46375445 \n",
      "epoch: 200  | loss: 0.11576868 \n",
      "epoch: 200  | loss: 0.01664399 \n",
      "epoch: 200  | loss: 0.31358227 \n",
      "epoch: 300  | loss: 0.49345344 \n",
      "epoch: 300  | loss: 0.64306587 \n",
      "epoch: 300  | loss: 0.81877488 \n",
      "epoch: 300  | loss: 0.00059950 \n",
      "epoch: 300  | loss: 0.65210813 \n",
      "epoch: 300  | loss: 0.31144509 \n",
      "epoch: 300  | loss: 0.00453574 \n",
      "epoch: 300  | loss: 0.00079943 \n",
      "epoch: 300  | loss: 0.08580935 \n",
      "epoch: 300  | loss: 0.00145740 \n",
      "epoch: 300  | loss: 0.01891893 \n",
      "epoch: 400  | loss: 0.33622530 \n",
      "epoch: 400  | loss: 0.46632984 \n",
      "epoch: 400  | loss: 0.67222339 \n",
      "epoch: 400  | loss: 0.10786655 \n",
      "epoch: 400  | loss: 0.00033551 \n",
      "epoch: 400  | loss: 0.47580069 \n",
      "epoch: 400  | loss: 0.00040397 \n",
      "epoch: 400  | loss: 0.00149797 \n",
      "epoch: 400  | loss: 0.01620576 \n",
      "epoch: 400  | loss: 0.00060821 \n",
      "epoch: 400  | loss: 0.00532764 \n",
      "epoch: 500  | loss: 0.23580194 \n",
      "epoch: 500  | loss: 0.35770521 \n",
      "epoch: 500  | loss: 0.56217086 \n",
      "epoch: 500  | loss: 0.03020410 \n",
      "epoch: 500  | loss: 0.00024702 \n",
      "epoch: 500  | loss: 0.36902064 \n",
      "epoch: 500  | loss: 0.00624364 \n",
      "epoch: 500  | loss: 0.00030177 \n",
      "epoch: 500  | loss: 0.00072568 \n",
      "epoch: 500  | loss: 0.00081588 \n",
      "epoch: 500  | loss: 0.00248606 \n",
      "epoch: 600  | loss: 0.14785622 \n",
      "epoch: 600  | loss: 0.25977391 \n",
      "epoch: 600  | loss: 0.01423023 \n",
      "epoch: 600  | loss: 0.47755525 \n",
      "epoch: 600  | loss: 0.00020781 \n",
      "epoch: 600  | loss: 0.27417585 \n",
      "epoch: 600  | loss: 0.00045377 \n",
      "epoch: 600  | loss: 0.00296968 \n",
      "epoch: 600  | loss: 0.00025806 \n",
      "epoch: 600  | loss: 0.00025929 \n",
      "epoch: 600  | loss: 0.00137535 \n",
      "epoch: 700  | loss: 0.07039270 \n",
      "epoch: 700  | loss: 0.18305069 \n",
      "epoch: 700  | loss: 0.00733944 epoch: 700  | loss: 0.39961389 \n",
      "\n",
      "epoch: 700  | loss: 0.19606540 \n",
      "epoch: 700  | loss: 0.00018813 \n",
      "epoch: 700  | loss: 0.00033677 \n",
      "epoch: 700  | loss: 0.00023638 \n",
      "epoch: 700  | loss: 0.00165197 \n",
      "epoch: 700  | loss: 0.00026871 \n",
      "epoch: 700  | loss: 0.00086709 \n",
      "epoch: 800  | loss: 0.03474638 \n",
      "epoch: 800  | loss: 0.13882487 \n",
      "epoch: 800  | loss: 0.00401385 \n",
      "epoch: 800  | loss: 0.32972401 \n",
      "epoch: 800  | loss: 0.00017767 \n",
      "epoch: 800  | loss: 0.14889599 \n",
      "epoch: 800  | loss: 0.00105175 \n",
      "epoch: 800  | loss: 0.00027967 \n",
      "epoch: 800  | loss: 0.00022518 \n",
      "epoch: 800  | loss: 0.00037455 \n",
      "epoch: 800  | loss: 0.00061664 \n",
      "epoch: 900  | loss: 0.02019582 \n",
      "epoch: 900  | loss: 0.10902913 \n",
      "epoch: 900  | loss: 0.27295551 \n",
      "epoch: 900  | loss: 0.00237290 \n",
      "epoch: 900  | loss: 0.11844176 \n",
      "epoch: 900  | loss: 0.00017208 \n",
      "epoch: 900  | loss: 0.00074888 \n",
      "epoch: 900  | loss: 0.00024979 \n",
      "epoch: 900  | loss: 0.00021959 \n",
      "epoch: 900  | loss: 0.00015424 \n",
      "epoch: 900  | loss: 0.00048639 \n",
      "epoch: 1000  | loss: 0.01263361 \n",
      "epoch: 1000  | loss: 0.08332404 \n",
      "epoch: 1000  | loss: 0.22890845 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1000  | loss: 0.00154989 \n",
      "epoch: 1000  | loss: 0.00016927 \n",
      "epoch: 1000  | loss: 0.09318703 \n",
      "epoch: 1000  | loss: 0.00058168 \n",
      "epoch: 1000  | loss: 0.00023365 \n",
      "epoch: 1000  | loss: 0.00021715 \n",
      "epoch: 1000  | loss: 0.00014584 \n",
      "epoch: 1000  | loss: 0.00041566 \n",
      "epoch: 1100  | loss: 0.00820973 \n",
      "epoch: 1100  | loss: 0.06124630 \n",
      "epoch: 1100  | loss: 0.19418694 \n",
      "epoch: 1100  | loss: 0.00112337 \n",
      "epoch: 1100  | loss: 0.07071715 \n",
      "epoch: 1100  | loss: 0.00016814 \n",
      "epoch: 1100  | loss: 0.00048224 \n",
      "epoch: 1100  | loss: 0.00022496 \n",
      "epoch: 1100  | loss: 0.00021651 \n",
      "epoch: 1100  | loss: 0.00049468 \n",
      "epoch: 1100  | loss: 0.00037595 \n",
      "epoch: 1200  | loss: 0.00553282 \n",
      "epoch: 1200  | loss: 0.04453982 \n",
      "epoch: 1200  | loss: 0.16555278 \n",
      "epoch: 1200  | loss: 0.00088834 \n",
      "epoch: 1200  | loss: 0.05255646 \n",
      "epoch: 1200  | loss: 0.00025978 \n",
      "epoch: 1200  | loss: 0.00041951 \n",
      "epoch: 1200  | loss: 0.00021697 \n",
      "epoch: 1200  | loss: 0.00022052 \n",
      "epoch: 1200  | loss: 0.00012634 \n",
      "epoch: 1200  | loss: 0.00035317 \n",
      "epoch: 1300  | loss: 0.00388522 \n",
      "epoch: 1300  | loss: 0.03262661 \n",
      "epoch: 1300  | loss: 0.00074850 \n",
      "epoch: 1300  | loss: 0.14093275 \n",
      "epoch: 1300  | loss: 0.03911823 \n",
      "epoch: 1300  | loss: 0.00037805 \n",
      "epoch: 1300  | loss: 0.00016884 \n",
      "epoch: 1300  | loss: 0.00021861 \n",
      "epoch: 1300  | loss: 0.00021810 \n",
      "epoch: 1300  | loss: 0.00012581 \n",
      "epoch: 1300  | loss: 0.00034004 \n",
      "epoch: 1400  | loss: 0.00285260 \n",
      "epoch: 1400  | loss: 0.02418543 \n",
      "epoch: 1400  | loss: 0.11912323 \n",
      "epoch: 1400  | loss: 0.00065878 \n",
      "epoch: 1400  | loss: 0.02942587 epoch: 1400  | loss: 0.00025957 \n",
      "\n",
      "epoch: 1400  | loss: 0.00034952 \n",
      "epoch: 1400  | loss: 0.00021823 \n",
      "epoch: 1400  | loss: 0.00021972 \n",
      "epoch: 1400  | loss: 0.00142417 \n",
      "epoch: 1400  | loss: 0.00033261 \n",
      "epoch: 1500  | loss: 0.00219108 \n",
      "epoch: 1500  | loss: 0.01821932 \n",
      "epoch: 1500  | loss: 0.09951150 \n",
      "epoch: 1500  | loss: 0.00059744 \n",
      "epoch: 1500  | loss: 0.02247948 \n",
      "epoch: 1500  | loss: 0.00017148 \n",
      "epoch: 1500  | loss: 0.00032914 \n",
      "epoch: 1500  | loss: 0.00021881 \n",
      "epoch: 1500  | loss: 0.00022166 \n",
      "epoch: 1500  | loss: 0.00011633 \n",
      "epoch: 1500  | loss: 0.00032865 \n",
      "epoch: 1600  | loss: 0.00175648 \n",
      "epoch: 1600  | loss: 0.01397704 \n",
      "epoch: 1600  | loss: 0.01750529 \n",
      "epoch: 1600  | loss: 0.00055346 \n",
      "epoch: 1600  | loss: 0.08194675 \n",
      "epoch: 1600  | loss: 0.00018232 \n",
      "epoch: 1600  | loss: 0.00031408 \n",
      "epoch: 1600  | loss: 0.00011509 \n",
      "epoch: 1600  | loss: 0.00022006 \n",
      "epoch: 1600  | loss: 0.00022383 \n",
      "epoch: 1600  | loss: 0.00032687 \n",
      "epoch: 1700  | loss: 0.00146291 \n",
      "epoch: 1700  | loss: 0.01091350 \n",
      "epoch: 1700  | loss: 0.00052083 \n",
      "epoch: 1700  | loss: 0.01390264 \n",
      "epoch: 1700  | loss: 0.06661619 \n",
      "epoch: 1700  | loss: 0.00030260 \n",
      "epoch: 1700  | loss: 0.00098399 \n",
      "epoch: 1700  | loss: 0.00022167 \n",
      "epoch: 1700  | loss: 0.00015621 \n",
      "epoch: 1700  | loss: 0.00022611 \n",
      "epoch: 1700  | loss: 0.00032649 \n",
      "epoch: 1800  | loss: 0.00125871 \n",
      "epoch: 1800  | loss: 0.00866288 \n",
      "epoch: 1800  | loss: 0.01124013 \n",
      "epoch: 1800  | loss: 0.00049603 \n",
      "epoch: 1800  | loss: 0.05372839 \n",
      "epoch: 1800  | loss: 0.00017747 \n",
      "epoch: 1800  | loss: 0.00029361 \n",
      "epoch: 1800  | loss: 0.00022351 \n",
      "epoch: 1800  | loss: 0.00012195 \n",
      "epoch: 1800  | loss: 0.00022846 \n",
      "epoch: 1800  | loss: 0.00032704 \n",
      "epoch: 1900  | loss: 0.00111247 \n",
      "epoch: 1900  | loss: 0.00698428 \n",
      "epoch: 1900  | loss: 0.00922842 \n",
      "epoch: 1900  | loss: 0.00047687 \n",
      "epoch: 1900  | loss: 0.04321375 \n",
      "epoch: 1900  | loss: 0.00028642 \n",
      "epoch: 1900  | loss: 0.00018004 \n",
      "epoch: 1900  | loss: 0.00022547 \n",
      "epoch: 1900  | loss: 0.00011306 \n",
      "epoch: 1900  | loss: 0.00023420 \n",
      "epoch: 1900  | loss: 0.00032819 \n",
      "epoch: 2000  | loss: 0.00100482 \n",
      "epoch: 2000  | loss: 0.00571666 \n",
      "epoch: 2000  | loss: 0.00767816 \n",
      "epoch: 2000  | loss: 0.03478151 \n",
      "epoch: 2000  | loss: 0.00046198 \n",
      "epoch: 2000  | loss: 0.00028054 \n",
      "epoch: 2000  | loss: 0.00018874 \n",
      "epoch: 2000  | loss: 0.00022749 \n",
      "epoch: 2000  | loss: 0.00011058 \n",
      "epoch: 2000  | loss: 0.00023271 \n",
      "epoch: 2000  | loss: 0.00032976 \n",
      "epoch: 2100  | loss: 0.00092362 \n",
      "epoch: 2100  | loss: 0.00474969 \n",
      "epoch: 2100  | loss: 0.00646474 \n",
      "epoch: 2100  | loss: 0.02808829 \n",
      "epoch: 2100  | loss: 0.00045059 \n",
      "epoch: 2100  | loss: 0.00029177 \n",
      "epoch: 2100  | loss: 0.00027567 \n",
      "epoch: 2100  | loss: 0.00022954 \n",
      "epoch: 2100  | loss: 0.00056628 \n",
      "epoch: 2100  | loss: 0.00011344 \n",
      "epoch: 2100  | loss: 0.00033159 \n",
      "epoch: 2200  | loss: 0.00086108 \n",
      "epoch: 2200  | loss: 0.00400560 \n",
      "epoch: 2200  | loss: 0.00550399 \n",
      "epoch: 2200  | loss: 0.00044161 \n",
      "epoch: 2200  | loss: 0.00055385 \n",
      "epoch: 2200  | loss: 0.02281624 \n",
      "epoch: 2200  | loss: 0.00027158 \n",
      "epoch: 2200  | loss: 0.00023158 \n",
      "epoch: 2200  | loss: 0.00010947 \n",
      "epoch: 2200  | loss: 0.00023772 \n",
      "epoch: 2200  | loss: 0.00033361 \n",
      "epoch: 2300  | loss: 0.00342818 epoch: 2300  | loss: 0.00081206 \n",
      "\n",
      "epoch: 2300  | loss: 0.00473683 \n",
      "epoch: 2300  | loss: 0.01868510 epoch: 2300  | loss: 0.00043476 \n",
      "\n",
      "epoch: 2300  | loss: 0.00020690 \n",
      "epoch: 2300  | loss: 0.00026810 \n",
      "epoch: 2300  | loss: 0.00024163 epoch: 2300  | loss: 0.00023361 \n",
      "\n",
      "epoch: 2300  | loss: 0.00012695 \n",
      "epoch: 2300  | loss: 0.00033574 \n",
      "epoch: 2400  | loss: 0.00077308 \n",
      "epoch: 2400  | loss: 0.00297622 \n",
      "epoch: 2400  | loss: 0.00412018 \n",
      "epoch: 2400  | loss: 0.01545052 \n",
      "epoch: 2400  | loss: 0.00205789 \n",
      "epoch: 2400  | loss: 0.00026509 epoch: 2400  | loss: 0.00042937 \n",
      "\n",
      "epoch: 2400  | loss: 0.00017492 \n",
      "epoch: 2400  | loss: 0.00023559 \n",
      "epoch: 2400  | loss: 0.00024357 \n",
      "epoch: 2400  | loss: 0.00033794 \n",
      "epoch: 2500  | loss: 0.00074173 \n",
      "epoch: 2500  | loss: 0.00261929 \n",
      "epoch: 2500  | loss: 0.00362129 \n",
      "epoch: 2500  | loss: 0.01290650 \n",
      "epoch: 2500  | loss: 0.00042510 \n",
      "epoch: 2500  | loss: 0.00026248 \n",
      "epoch: 2500  | loss: 0.00123676 \n",
      "epoch: 2500  | loss: 0.00011223 \n",
      "epoch: 2500  | loss: 0.00023756 \n",
      "epoch: 2500  | loss: 0.00024747 \n",
      "epoch: 2500  | loss: 0.00034017 \n",
      "epoch: 2600  | loss: 0.00071626 \n",
      "epoch: 2600  | loss: 0.00233495 \n",
      "epoch: 2600  | loss: 0.00321534 \n",
      "epoch: 2600  | loss: 0.01088531 \n",
      "epoch: 2600  | loss: 0.00027941 \n",
      "epoch: 2600  | loss: 0.00042172 \n",
      "epoch: 2600  | loss: 0.00026019 \n",
      "epoch: 2600  | loss: 0.00010894 \n",
      "epoch: 2600  | loss: 0.00023944 \n",
      "epoch: 2600  | loss: 0.00025030 \n",
      "epoch: 2600  | loss: 0.00034241 \n",
      "epoch: 2700  | loss: 0.00069540 \n",
      "epoch: 2700  | loss: 0.00210661 \n",
      "epoch: 2700  | loss: 0.00288316 \n",
      "epoch: 2700  | loss: 0.00925890 \n",
      "epoch: 2700  | loss: 0.00020516 epoch: 2700  | loss: 0.00041906 \n",
      "\n",
      "epoch: 2700  | loss: 0.00025816 \n",
      "epoch: 2700  | loss: 0.00078079 \n",
      "epoch: 2700  | loss: 0.00024128 \n",
      "epoch: 2700  | loss: 0.00067438 \n",
      "epoch: 2700  | loss: 0.00034465 \n",
      "epoch: 2800  | loss: 0.00067819 \n",
      "epoch: 2800  | loss: 0.00192187 \n",
      "epoch: 2800  | loss: 0.00260990 \n",
      "epoch: 2800  | loss: 0.00041699 \n",
      "epoch: 2800  | loss: 0.00025634 \n",
      "epoch: 2800  | loss: 0.00793280 \n",
      "epoch: 2800  | loss: 0.00024216 \n",
      "epoch: 2800  | loss: 0.00011316 \n",
      "epoch: 2800  | loss: 0.00024307 \n",
      "epoch: 2800  | loss: 0.00030898 \n",
      "epoch: 2800  | loss: 0.00034687 \n",
      "epoch: 2900  | loss: 0.00177139 \n",
      "epoch: 2900  | loss: 0.00066392 \n",
      "epoch: 2900  | loss: 0.00238398 \n",
      "epoch: 2900  | loss: 0.00041539 \n",
      "epoch: 2900  | loss: 0.00683819 \n",
      "epoch: 2900  | loss: 0.00025469 \n",
      "epoch: 2900  | loss: 0.00037147 \n",
      "epoch: 2900  | loss: 0.00010769 \n",
      "epoch: 2900  | loss: 0.00024638 \n",
      "epoch: 2900  | loss: 0.00026981 \n",
      "epoch: 2900  | loss: 0.00034906 \n",
      "epoch: 3000  | loss: 0.00065207 \n",
      "epoch: 3000  | loss: 0.00164800 \n",
      "epoch: 3000  | loss: 0.00219635 \n",
      "epoch: 3000  | loss: 0.00025319 \n",
      "epoch: 3000  | loss: 0.00592669 \n",
      "epoch: 3000  | loss: 0.00041418 \n",
      "epoch: 3000  | loss: 0.00052107 \n",
      "epoch: 3000  | loss: 0.00040741 \n",
      "epoch: 3000  | loss: 0.00024901 \n",
      "epoch: 3000  | loss: 0.00028545 \n",
      "epoch: 3000  | loss: 0.00035121 \n",
      "epoch: 3100  | loss: 0.00064215 \n",
      "epoch: 3100  | loss: 0.00154618 \n",
      "epoch: 3100  | loss: 0.00203985 \n",
      "epoch: 3100  | loss: 0.00516335 \n",
      "epoch: 3100  | loss: 0.00041328 \n",
      "epoch: 3100  | loss: 0.00025181 \n",
      "epoch: 3100  | loss: 0.00202846 \n",
      "epoch: 3100  | loss: 0.00010735 \n",
      "epoch: 3100  | loss: 0.00024808 \n",
      "epoch: 3100  | loss: 0.00065282 \n",
      "epoch: 3100  | loss: 0.00035332 \n",
      "epoch: 3200  | loss: 0.00063381 \n",
      "epoch: 3200  | loss: 0.00146159 \n",
      "epoch: 3200  | loss: 0.00190877 \n",
      "epoch: 3200  | loss: 0.00452227 \n",
      "epoch: 3200  | loss: 0.00041264 \n",
      "epoch: 3200  | loss: 0.00025052 \n",
      "epoch: 3200  | loss: 0.00023538 \n",
      "epoch: 3200  | loss: 0.00011130 \n",
      "epoch: 3200  | loss: 0.00024963 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3200  | loss: 0.00035537 \n",
      "epoch: 3200  | loss: 0.00029950 \n",
      "epoch: 3300  | loss: 0.00062677 \n",
      "epoch: 3300  | loss: 0.00139086 \n",
      "epoch: 3300  | loss: 0.00179855 \n",
      "epoch: 3300  | loss: 0.00041221 \n",
      "epoch: 3300  | loss: 0.00398357 \n",
      "epoch: 3300  | loss: 0.00023471 \n",
      "epoch: 3300  | loss: 0.00024931 \n",
      "epoch: 3300  | loss: 0.00010953 \n",
      "epoch: 3300  | loss: 0.00025111 \n",
      "epoch: 3300  | loss: 0.00027598 \n",
      "epoch: 3300  | loss: 0.00035738 \n",
      "epoch: 3400  | loss: 0.00062081 \n",
      "epoch: 3400  | loss: 0.00133132 \n",
      "epoch: 3400  | loss: 0.00353134 \n",
      "epoch: 3400  | loss: 0.00041195 \n",
      "epoch: 3400  | loss: 0.00170549 \n",
      "epoch: 3400  | loss: 0.00024158 \n",
      "epoch: 3400  | loss: 0.00024817 \n",
      "epoch: 3400  | loss: 0.00010886 \n",
      "epoch: 3400  | loss: 0.00025254 \n",
      "epoch: 3400  | loss: 0.00035933 \n",
      "epoch: 3400  | loss: 0.00028874 \n",
      "epoch: 3500  | loss: 0.00061575 \n",
      "epoch: 3500  | loss: 0.00128089 \n",
      "epoch: 3500  | loss: 0.00162663 \n",
      "epoch: 3500  | loss: 0.00041183 \n",
      "epoch: 3500  | loss: 0.00315252 \n",
      "epoch: 3500  | loss: 0.00024707 \n",
      "epoch: 3500  | loss: 0.00024078 \n",
      "epoch: 3500  | loss: 0.00042692 \n",
      "epoch: 3500  | loss: 0.00025393 \n",
      "epoch: 3500  | loss: 0.00028509 \n",
      "epoch: 3500  | loss: 0.00036122 \n",
      "epoch: 3600  | loss: 0.00061144 \n",
      "epoch: 3600  | loss: 0.00123791 \n",
      "epoch: 3600  | loss: 0.00283600 \n",
      "epoch: 3600  | loss: 0.00155955 \n",
      "epoch: 3600  | loss: 0.00041181 \n",
      "epoch: 3600  | loss: 0.00024601 \n",
      "epoch: 3600  | loss: 0.00024395 \n",
      "epoch: 3600  | loss: 0.00013035 \n",
      "epoch: 3600  | loss: 0.00025523 \n",
      "epoch: 3600  | loss: 0.00036305 \n",
      "epoch: 3600  | loss: 0.00028899 \n",
      "epoch: 3700  | loss: 0.00060776 \n",
      "epoch: 3700  | loss: 0.00120108 \n",
      "epoch: 3700  | loss: 0.00150230 \n",
      "epoch: 3700  | loss: 0.00257207 \n",
      "epoch: 3700  | loss: 0.00041191 \n",
      "epoch: 3700  | loss: 0.00024498 \n",
      "epoch: 3700  | loss: 0.00024738 \n",
      "epoch: 3700  | loss: 0.00011873 \n",
      "epoch: 3700  | loss: 0.00025670 \n",
      "epoch: 3700  | loss: 0.00036482 \n",
      "epoch: 3700  | loss: 0.00034185 \n",
      "epoch: 3800  | loss: 0.00060461 \n",
      "epoch: 3800  | loss: 0.00116935 \n",
      "epoch: 3800  | loss: 0.00145327 \n",
      "epoch: 3800  | loss: 0.00235243 \n",
      "epoch: 3800  | loss: 0.00024397 \n",
      "epoch: 3800  | loss: 0.00041209 \n",
      "epoch: 3800  | loss: 0.00025418 \n",
      "epoch: 3800  | loss: 0.00010729 \n",
      "epoch: 3800  | loss: 0.00025766 \n",
      "epoch: 3800  | loss: 0.00036652 \n",
      "epoch: 3800  | loss: 0.00084720 \n",
      "epoch: 3900  | loss: 0.00060201 \n",
      "epoch: 3900  | loss: 0.00114188 \n",
      "epoch: 3900  | loss: 0.00141117 \n",
      "epoch: 3900  | loss: 0.00216994 \n",
      "epoch: 3900  | loss: 0.00041234 \n",
      "epoch: 3900  | loss: 0.00024297 \n",
      "epoch: 3900  | loss: 0.00029581 \n",
      "epoch: 3900  | loss: 0.00025882 \n",
      "epoch: 3900  | loss: 0.00011113 \n",
      "epoch: 3900  | loss: 0.00030986 \n",
      "epoch: 3900  | loss: 0.00036816 \n",
      "epoch: 4000  | loss: 0.00111798 epoch: 4000  | loss: 0.00059982 \n",
      "\n",
      "epoch: 4000  | loss: 0.00041263 \n",
      "epoch: 4000  | loss: 0.00201850 epoch: 4000  | loss: 0.00137492 \n",
      "\n",
      "epoch: 4000  | loss: 0.00024198 \n",
      "epoch: 4000  | loss: 0.00026545 \n",
      "epoch: 4000  | loss: 0.00010620 \n",
      "epoch: 4000  | loss: 0.00026008 \n",
      "epoch: 4000  | loss: 0.00036973 \n",
      "epoch: 4000  | loss: 0.00039302 \n",
      "epoch: 4100  | loss: 0.00109711 \n",
      "epoch: 4100  | loss: 0.00059794 \n",
      "epoch: 4100  | loss: 0.00189287 \n",
      "epoch: 4100  | loss: 0.00134365 \n",
      "epoch: 4100  | loss: 0.00041294 \n",
      "epoch: 4100  | loss: 0.00096561 \n",
      "epoch: 4100  | loss: 0.00024100 \n",
      "epoch: 4100  | loss: 0.00010623 \n",
      "epoch: 4100  | loss: 0.00026106 \n",
      "epoch: 4100  | loss: 0.00037122 \n",
      "epoch: 4100  | loss: 0.00034015 \n",
      "epoch: 4200  | loss: 0.00107879 \n",
      "epoch: 4200  | loss: 0.00059632 \n",
      "epoch: 4200  | loss: 0.00131662 \n",
      "epoch: 4200  | loss: 0.00178865 \n",
      "epoch: 4200  | loss: 0.00041327 \n",
      "epoch: 4200  | loss: 0.00024000 \n",
      "epoch: 4200  | loss: 0.00027169 \n",
      "epoch: 4200  | loss: 0.00010811 \n",
      "epoch: 4200  | loss: 0.00026225 \n",
      "epoch: 4200  | loss: 0.00051300 epoch: 4200  | loss: 0.00037265 \n",
      "\n",
      "epoch: 4300  | loss: 0.00106264 \n",
      "epoch: 4300  | loss: 0.00059492 \n",
      "epoch: 4300  | loss: 0.00129323 \n",
      "epoch: 4300  | loss: 0.00041360 \n",
      "epoch: 4300  | loss: 0.00023900 \n",
      "epoch: 4300  | loss: 0.00170212 \n",
      "epoch: 4300  | loss: 0.00027163 \n",
      "epoch: 4300  | loss: 0.00010625 \n",
      "epoch: 4300  | loss: 0.00026451 \n",
      "epoch: 4300  | loss: 0.00051071 \n",
      "epoch: 4300  | loss: 0.00037400 \n",
      "epoch: 4400  | loss: 0.00104835 \n",
      "epoch: 4400  | loss: 0.00059369 \n",
      "epoch: 4400  | loss: 0.00127297 \n",
      "epoch: 4400  | loss: 0.00041392 \n",
      "epoch: 4400  | loss: 0.00023799 \n",
      "epoch: 4400  | loss: 0.00163019 \n",
      "epoch: 4400  | loss: 0.00027438 \n",
      "epoch: 4400  | loss: 0.00010570 \n",
      "epoch: 4400  | loss: 0.00033015 \n",
      "epoch: 4400  | loss: 0.00026414 \n",
      "epoch: 4400  | loss: 0.00037527 \n",
      "epoch: 4500  | loss: 0.00103564 \n",
      "epoch: 4500  | loss: 0.00059262 \n",
      "epoch: 4500  | loss: 0.00157029 \n",
      "epoch: 4500  | loss: 0.00125541 \n",
      "epoch: 4500  | loss: 0.00023696 \n",
      "epoch: 4500  | loss: 0.00041422 \n",
      "epoch: 4500  | loss: 0.00042530 \n",
      "epoch: 4500  | loss: 0.00011468 \n",
      "epoch: 4500  | loss: 0.00033591 \n",
      "epoch: 4500  | loss: 0.00026541 \n",
      "epoch: 4500  | loss: 0.00037646 \n",
      "epoch: 4600  | loss: 0.00102429 \n",
      "epoch: 4600  | loss: 0.00059166 \n",
      "epoch: 4600  | loss: 0.00152027 \n",
      "epoch: 4600  | loss: 0.00124019 \n",
      "epoch: 4600  | loss: 0.00023591 \n",
      "epoch: 4600  | loss: 0.00041451 \n",
      "epoch: 4600  | loss: 0.00042720 \n",
      "epoch: 4600  | loss: 0.00012024 \n",
      "epoch: 4600  | loss: 0.00026598 \n",
      "epoch: 4600  | loss: 0.00078723 \n",
      "epoch: 4600  | loss: 0.00037757 \n",
      "epoch: 4700  | loss: 0.00101411 \n",
      "epoch: 4700  | loss: 0.00059079 \n",
      "epoch: 4700  | loss: 0.00122700 \n",
      "epoch: 4700  | loss: 0.00147837 \n",
      "epoch: 4700  | loss: 0.00041476 \n",
      "epoch: 4700  | loss: 0.00031469 \n",
      "epoch: 4700  | loss: 0.00023484 \n",
      "epoch: 4700  | loss: 0.00010508 \n",
      "epoch: 4700  | loss: 0.00027030 \n",
      "epoch: 4700  | loss: 0.00050313 \n",
      "epoch: 4700  | loss: 0.00037860 \n",
      "epoch: 4800  | loss: 0.00100493 \n",
      "epoch: 4800  | loss: 0.00059000 \n",
      "epoch: 4800  | loss: 0.00144316 \n",
      "epoch: 4800  | loss: 0.00041497 \n",
      "epoch: 4800  | loss: 0.00121557 \n",
      "epoch: 4800  | loss: 0.00186599 \n",
      "epoch: 4800  | loss: 0.00023374 \n",
      "epoch: 4800  | loss: 0.00010823 \n",
      "epoch: 4800  | loss: 0.00036245 \n",
      "epoch: 4800  | loss: 0.00026767 \n",
      "epoch: 4800  | loss: 0.00037954 \n",
      "epoch: 4900  | loss: 0.00099662 \n",
      "epoch: 4900  | loss: 0.00058926 \n",
      "epoch: 4900  | loss: 0.00141345 \n",
      "epoch: 4900  | loss: 0.00120567 \n",
      "epoch: 4900  | loss: 0.00041514 \n",
      "epoch: 4900  | loss: 0.00033649 \n",
      "epoch: 4900  | loss: 0.00023266 \n",
      "epoch: 4900  | loss: 0.00011653 \n",
      "epoch: 4900  | loss: 0.00026848 \n",
      "epoch: 4900  | loss: 0.00071222 \n",
      "epoch: 4900  | loss: 0.00038039 epoch: 5000  | loss: 0.00098906 \n",
      "\n",
      "epoch: 5000  | loss: 0.00058856 \n",
      "epoch: 5000  | loss: 0.00119710 \n",
      "epoch: 5000  | loss: 0.00138826 \n",
      "epoch: 5000  | loss: 0.00041527 \n",
      "epoch: 5000  | loss: 0.00047299 \n",
      "epoch: 5000  | loss: 0.00023146 \n",
      "epoch: 5000  | loss: 0.00010499 \n",
      "epoch: 5000  | loss: 0.00026947 \n",
      "epoch: 5000  | loss: 0.00085939 \n",
      "epoch: 5100  | loss: 0.00098215 \n",
      "epoch: 5000  | loss: 0.00038117 \n",
      "epoch: 5100  | loss: 0.00058787 \n",
      "epoch: 5100  | loss: 0.00118970 \n",
      "epoch: 5100  | loss: 0.00136680 \n",
      "epoch: 5100  | loss: 0.00041534 \n",
      "epoch: 5100  | loss: 0.00200038 \n",
      "epoch: 5100  | loss: 0.00023046 \n",
      "epoch: 5100  | loss: 0.00012576 \n",
      "epoch: 5100  | loss: 0.00027002 \n",
      "epoch: 5100  | loss: 0.00036971 \n",
      "epoch: 5200  | loss: 0.00097580 \n",
      "epoch: 5100  | loss: 0.00038181 \n",
      "epoch: 5200  | loss: 0.00058720 \n",
      "epoch: 5200  | loss: 0.00134843 \n",
      "epoch: 5200  | loss: 0.00041535 \n",
      "epoch: 5200  | loss: 0.00118331 \n",
      "epoch: 5200  | loss: 0.00041816 \n",
      "epoch: 5200  | loss: 0.00022903 \n",
      "epoch: 5200  | loss: 0.00021211 epoch: 5200  | loss: 0.00037805 epoch: 5200  | loss: 0.00027079 epoch: 5300  | loss: 0.00096993 epoch: 5200  | loss: 0.00038238 epoch: 5300  | loss: 0.00058651 epoch: 5300  | loss: 0.00117779 epoch: 5300  | loss: 0.00133262 epoch: 5300  | loss: 0.00041530 epoch: 5300  | loss: 0.00031724 epoch: 5300  | loss: 0.00022765 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch: 5300  | loss: 0.00094384 epoch: 5400  | loss: 0.00096447 epoch: 5400  | loss: 0.00117304 \n",
      "epoch: 5400  | loss: 0.00058580 \n",
      "\n",
      "\n",
      "epoch: 5300  | loss: 0.00027231 epoch: 5300  | loss: 0.00041309 \n",
      "\n",
      "epoch: 5400  | loss: 0.00041517 \n",
      "epoch: 5400  | loss: 0.00035285 \n",
      "epoch: 5400  | loss: 0.00131893 \n",
      "epoch: 5400  | loss: 0.00022703 \n",
      "epoch: 5300  | loss: 0.00038284 \n",
      "epoch: 5500  | loss: 0.00050977 \n",
      "epoch: 5500  | loss: 0.00095937 \n",
      "epoch: 5400  | loss: 0.00027226 epoch: 5500  | loss: 0.00130700 \n",
      "\n",
      "epoch: 5500  | loss: 0.00041497 \n",
      "epoch: 5500  | loss: 0.00058506 \n",
      "epoch: 5400  | loss: 0.00038324 \n",
      "epoch: 5500  | loss: 0.00116894 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5400  | loss: 0.00039886 \n",
      "epoch: 5400  | loss: 0.00020852 \n",
      "epoch: 5500  | loss: 0.00022491 \n",
      "epoch: 5600  | loss: 0.00095456 \n",
      "epoch: 5600  | loss: 0.00058427 \n",
      "epoch: 5500  | loss: 0.00010660 epoch: 5600  | loss: 0.00041469 \n",
      "\n",
      "epoch: 5600  | loss: 0.00192826 \n",
      "epoch: 5500  | loss: 0.00027301 \n",
      "epoch: 5600  | loss: 0.00129654 \n",
      "epoch: 5500  | loss: 0.00046001 \n",
      "epoch: 5500  | loss: 0.00038389 epoch: 5600  | loss: 0.00116541 \n",
      "\n",
      "epoch: 5600  | loss: 0.00022346 \n",
      "epoch: 5700  | loss: 0.00034319 \n",
      "epoch: 5700  | loss: 0.00041431 epoch: 5700  | loss: 0.00094999 \n",
      "\n",
      "epoch: 5600  | loss: 0.00010385 \n",
      "epoch: 5700  | loss: 0.00116237 \n",
      "epoch: 5600  | loss: 0.00038433 \n",
      "epoch: 5700  | loss: 0.00128731 \n",
      "epoch: 5600  | loss: 0.00238282 \n",
      "epoch: 5700  | loss: 0.00022202 \n",
      "epoch: 5600  | loss: 0.00027399 \n",
      "epoch: 5700  | loss: 0.00058345 \n",
      "epoch: 5800  | loss: 0.00184165 \n",
      "epoch: 5800  | loss: 0.00094563 epoch: 5700  | loss: 0.00027517 \n",
      "\n",
      "epoch: 5800  | loss: 0.00115974 \n",
      "epoch: 5800  | loss: 0.00058260 \n",
      "epoch: 5800  | loss: 0.00127910 \n",
      "epoch: 5700  | loss: 0.00011110 \n",
      "epoch: 5700  | loss: 0.00041216 epoch: 5700  | loss: 0.00038461 \n",
      "\n",
      "epoch: 5800  | loss: 0.00041385 \n",
      "epoch: 5800  | loss: 0.00022043 \n",
      "epoch: 5900  | loss: 0.00112321 \n",
      "epoch: 5900  | loss: 0.00094144 \n",
      "epoch: 5800  | loss: 0.00027521 \n",
      "epoch: 5900  | loss: 0.00115746 \n",
      "epoch: 5900  | loss: 0.00041327 \n",
      "epoch: 5800  | loss: 0.00042043 \n",
      "epoch: 5900  | loss: 0.00058170 \n",
      "epoch: 5800  | loss: 0.00038492 \n",
      "epoch: 5900  | loss: 0.00127174 \n",
      "epoch: 5800  | loss: 0.00010362 \n",
      "epoch: 5900  | loss: 0.00021880 \n",
      "epoch: 6000  | loss: 0.00044677 \n",
      "epoch: 6000  | loss: 0.00093737 epoch: 5900  | loss: 0.00027584 \n",
      "\n",
      "epoch: 6000  | loss: 0.00041259 \n",
      "epoch: 6000  | loss: 0.00115547 \n",
      "epoch: 5900  | loss: 0.00042854 \n",
      "epoch: 6000  | loss: 0.00126508 \n",
      "epoch: 5900  | loss: 0.00038544 \n",
      "epoch: 6000  | loss: 0.00022346 epoch: 6000  | loss: 0.00058073 \n",
      "\n",
      "epoch: 5900  | loss: 0.00010666 \n",
      "epoch: 6100  | loss: 0.00115372 \n",
      "epoch: 6000  | loss: 0.00027659 \n",
      "epoch: 6100  | loss: 0.00033754 \n",
      "epoch: 6100  | loss: 0.00093340 \n",
      "epoch: 6100  | loss: 0.00041180 \n",
      "epoch: 6000  | loss: 0.00043604 \n",
      "epoch: 6100  | loss: 0.00125901 epoch: 6100  | loss: 0.00057969 \n",
      "epoch: 6100  | loss: 0.00021561 \n",
      "\n",
      "epoch: 6000  | loss: 0.00038530 \n",
      "epoch: 6000  | loss: 0.00010613 \n",
      "epoch: 6200  | loss: 0.00115217 \n",
      "epoch: 6100  | loss: 0.00027917 \n",
      "epoch: 6200  | loss: 0.00092949 \n",
      "epoch: 6200  | loss: 0.00033875 \n",
      "epoch: 6200  | loss: 0.00125341 \n",
      "epoch: 6100  | loss: 0.00044304 \n",
      "epoch: 6200  | loss: 0.00057855 \n",
      "epoch: 6200  | loss: 0.00021438 \n",
      "epoch: 6100  | loss: 0.00038554 \n",
      "epoch: 6200  | loss: 0.00041094 \n",
      "epoch: 6100  | loss: 0.00010281 \n",
      "epoch: 6300  | loss: 0.00115077 \n",
      "epoch: 6200  | loss: 0.00027939 \n",
      "epoch: 6300  | loss: 0.00034115 \n",
      "epoch: 6300  | loss: 0.00092562 \n",
      "epoch: 6300  | loss: 0.00124819 \n",
      "epoch: 6300  | loss: 0.00021588 \n",
      "epoch: 6200  | loss: 0.00044961 \n",
      "epoch: 6200  | loss: 0.00038538 epoch: 6300  | loss: 0.00040995 \n",
      "\n",
      "epoch: 6300  | loss: 0.00057732 \n",
      "epoch: 6200  | loss: 0.00010543 \n",
      "epoch: 6400  | loss: 0.00114948 \n",
      "epoch: 6400  | loss: 0.00034384 \n",
      "epoch: 6400  | loss: 0.00092181 \n",
      "epoch: 6300  | loss: 0.00027859 \n",
      "epoch: 6400  | loss: 0.00040882 \n",
      "epoch: 6400  | loss: 0.00124328 \n",
      "epoch: 6400  | loss: 0.00021286 \n",
      "epoch: 6300  | loss: 0.00045749 \n",
      "epoch: 6300  | loss: 0.00038534 epoch: 6300  | loss: 0.00023328 \n",
      "\n",
      "epoch: 6400  | loss: 0.00057599 \n",
      "epoch: 6500  | loss: 0.00114826 \n",
      "epoch: 6500  | loss: 0.00034665 \n",
      "epoch: 6500  | loss: 0.00091803 \n",
      "epoch: 6400  | loss: 0.00028013 \n",
      "epoch: 6500  | loss: 0.00040755 \n",
      "epoch: 6500  | loss: 0.00020766 \n",
      "epoch: 6500  | loss: 0.00123860 \n",
      "epoch: 6400  | loss: 0.00038522 \n",
      "epoch: 6400  | loss: 0.00014596 \n",
      "epoch: 6500  | loss: 0.00057454 \n",
      "epoch: 6400  | loss: 0.00056626 \n",
      "epoch: 6600  | loss: 0.00035238 \n",
      "epoch: 6600  | loss: 0.00020552 \n",
      "epoch: 6600  | loss: 0.00114710 \n",
      "epoch: 6500  | loss: 0.00028278 \n",
      "epoch: 6600  | loss: 0.00091425 \n",
      "epoch: 6600  | loss: 0.00040617 epoch: 6600  | loss: 0.00057297 \n",
      "\n",
      "epoch: 6500  | loss: 0.00038497 \n",
      "epoch: 6500  | loss: 0.00047210 \n",
      "epoch: 6500  | loss: 0.00010277 \n",
      "epoch: 6600  | loss: 0.00123411 \n",
      "epoch: 6700  | loss: 0.00035203 \n",
      "epoch: 6700  | loss: 0.00114594 \n",
      "epoch: 6600  | loss: 0.00028088 \n",
      "epoch: 6700  | loss: 0.00020327 \n",
      "epoch: 6700  | loss: 0.00091043 \n",
      "epoch: 6700  | loss: 0.00040476 \n",
      "epoch: 6700  | loss: 0.00122974 \n",
      "epoch: 6700  | loss: 0.00057127 \n",
      "epoch: 6600  | loss: 0.00103504 \n",
      "epoch: 6600  | loss: 0.00038469 \n",
      "epoch: 6600  | loss: 0.00011939 \n",
      "epoch: 6800  | loss: 0.00020091 epoch: 6800  | loss: 0.00090657 \n",
      "epoch: 6800  | loss: 0.00035496 \n",
      "\n",
      "epoch: 6700  | loss: 0.00028302 \n",
      "epoch: 6800  | loss: 0.00114478 \n",
      "epoch: 6800  | loss: 0.00040326 \n",
      "epoch: 6800  | loss: 0.00056943 \n",
      "epoch: 6800  | loss: 0.00122545 \n",
      "epoch: 6700  | loss: 0.00058247 \n",
      "epoch: 6700  | loss: 0.00017019 \n",
      "epoch: 6700  | loss: 0.00038434 \n",
      "epoch: 6900  | loss: 0.00035987 \n",
      "epoch: 6900  | loss: 0.00114357 \n",
      "epoch: 6900  | loss: 0.00090264 \n",
      "epoch: 6800  | loss: 0.00028382 \n",
      "epoch: 6900  | loss: 0.00019841 \n",
      "epoch: 6800  | loss: 0.00049383 \n",
      "epoch: 6900  | loss: 0.00056745 \n",
      "epoch: 6800  | loss: 0.00038397 \n",
      "epoch: 6900  | loss: 0.00122122 \n",
      "epoch: 6800  | loss: 0.00017531 \n",
      "epoch: 6900  | loss: 0.00040162 \n",
      "epoch: 7000  | loss: 0.00036296 \n",
      "epoch: 6900  | loss: 0.00028304 \n",
      "epoch: 7000  | loss: 0.00114231 \n",
      "epoch: 7000  | loss: 0.00089863 \n",
      "epoch: 7000  | loss: 0.00019556 \n",
      "epoch: 7000  | loss: 0.00056533 \n",
      "epoch: 7000  | loss: 0.00121699 \n",
      "epoch: 6900  | loss: 0.00079773 \n",
      "epoch: 6900  | loss: 0.00038344 \n",
      "epoch: 7000  | loss: 0.00039984 \n",
      "epoch: 6900  | loss: 0.00032328 \n",
      "epoch: 7100  | loss: 0.00036450 \n",
      "epoch: 7100  | loss: 0.00114097 \n",
      "epoch: 7100  | loss: 0.00089453 \n",
      "epoch: 7000  | loss: 0.00028389 \n",
      "epoch: 7100  | loss: 0.00056310 epoch: 7100  | loss: 0.00121276 \n",
      "\n",
      "epoch: 7000  | loss: 0.00050375 epoch: 7100  | loss: 0.00019267 \n",
      "\n",
      "epoch: 7000  | loss: 0.00038290 \n",
      "epoch: 7100  | loss: 0.00039791 \n",
      "epoch: 7000  | loss: 0.00029480 \n",
      "epoch: 7200  | loss: 0.00036850 \n",
      "epoch: 7200  | loss: 0.00113953 \n",
      "epoch: 7200  | loss: 0.00089034 \n",
      "epoch: 7100  | loss: 0.00028496 \n",
      "epoch: 7200  | loss: 0.00120848 \n",
      "epoch: 7100  | loss: 0.00051670 \n",
      "epoch: 7200  | loss: 0.00019267 \n",
      "epoch: 7200  | loss: 0.00056090 \n",
      "epoch: 7100  | loss: 0.00038230 \n",
      "epoch: 7200  | loss: 0.00039582 \n",
      "epoch: 7100  | loss: 0.00010032 \n",
      "epoch: 7300  | loss: 0.00037389 \n",
      "epoch: 7200  | loss: 0.00028644 \n",
      "epoch: 7300  | loss: 0.00088604 \n",
      "epoch: 7300  | loss: 0.00113799 \n",
      "epoch: 7300  | loss: 0.00120414 \n",
      "epoch: 7300  | loss: 0.00055875 \n",
      "epoch: 7200  | loss: 0.00053075 \n",
      "epoch: 7200  | loss: 0.00038166 \n",
      "epoch: 7300  | loss: 0.00018986 \n",
      "epoch: 7300  | loss: 0.00039355 \n",
      "epoch: 7200  | loss: 0.00020079 \n",
      "epoch: 7400  | loss: 0.00037554 \n",
      "epoch: 7400  | loss: 0.00113631 \n",
      "epoch: 7300  | loss: 0.00028629 epoch: 7400  | loss: 0.00088162 \n",
      "\n",
      "epoch: 7400  | loss: 0.00119972 \n",
      "epoch: 7400  | loss: 0.00055652 epoch: 7300  | loss: 0.00038093 epoch: 7300  | loss: 0.00052682 \n",
      "\n",
      "\n",
      "epoch: 7400  | loss: 0.00018282 \n",
      "epoch: 7400  | loss: 0.00039110 \n",
      "epoch: 7300  | loss: 0.00009945 \n",
      "epoch: 7400  | loss: 0.00028965 \n",
      "epoch: 7500  | loss: 0.00113447 \n",
      "epoch: 7500  | loss: 0.00087708 \n",
      "epoch: 7500  | loss: 0.00119520 \n",
      "epoch: 7500  | loss: 0.00050062 \n",
      "epoch: 7400  | loss: 0.00056312 \n",
      "epoch: 7500  | loss: 0.00055416 \n",
      "epoch: 7400  | loss: 0.00038023 \n",
      "epoch: 7500  | loss: 0.00038855 \n",
      "epoch: 7500  | loss: 0.00017913 \n",
      "epoch: 7400  | loss: 0.00010328 \n",
      "epoch: 7600  | loss: 0.00087241 epoch: 7600  | loss: 0.00113248 \n",
      "epoch: 7600  | loss: 0.00042510 \n",
      "\n",
      "epoch: 7500  | loss: 0.00028833 \n",
      "epoch: 7600  | loss: 0.00119057 \n",
      "epoch: 7500  | loss: 0.00053819 \n",
      "epoch: 7600  | loss: 0.00055165 \n",
      "epoch: 7500  | loss: 0.00037946 \n",
      "epoch: 7600  | loss: 0.00038596 epoch: 7600  | loss: 0.00017548 \n",
      "\n",
      "epoch: 7500  | loss: 0.00010427 \n",
      "epoch: 7600  | loss: 0.00029071 \n",
      "epoch: 7700  | loss: 0.00113030 \n",
      "epoch: 7700  | loss: 0.00039022 \n",
      "epoch: 7700  | loss: 0.00118582 \n",
      "epoch: 7600  | loss: 0.00081127 \n",
      "epoch: 7700  | loss: 0.00054896 \n",
      "epoch: 7700  | loss: 0.00086761 \n",
      "epoch: 7600  | loss: 0.00037894 \n",
      "epoch: 7700  | loss: 0.00038317 \n",
      "epoch: 7600  | loss: 0.00012614 \n",
      "epoch: 7700  | loss: 0.00017169 \n",
      "epoch: 7800  | loss: 0.00112794 \n",
      "epoch: 7800  | loss: 0.00042331 \n",
      "epoch: 7700  | loss: 0.00103922 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7800  | loss: 0.00118094 epoch: 7800  | loss: 0.00086268 \n",
      "\n",
      "epoch: 7800  | loss: 0.00054608 \n",
      "epoch: 7700  | loss: 0.00037776 \n",
      "epoch: 7700  | loss: 0.00028947 \n",
      "epoch: 7800  | loss: 0.00038018 \n",
      "epoch: 7700  | loss: 0.00011275 \n",
      "epoch: 7800  | loss: 0.00016777 \n",
      "epoch: 7900  | loss: 0.00045144 \n",
      "epoch: 7900  | loss: 0.00085759 \n",
      "epoch: 7900  | loss: 0.00112537 \n",
      "epoch: 7900  | loss: 0.00117591 \n",
      "epoch: 7800  | loss: 0.00056457 \n",
      "epoch: 7800  | loss: 0.00037694 \n",
      "epoch: 7900  | loss: 0.00037710 \n",
      "epoch: 7900  | loss: 0.00054302 \n",
      "epoch: 7800  | loss: 0.00029049 \n",
      "epoch: 7900  | loss: 0.00016447 \n",
      "epoch: 7800  | loss: 0.00010810 \n",
      "epoch: 8000  | loss: 0.00117077 \n",
      "epoch: 8000  | loss: 0.00085237 \n",
      "epoch: 8000  | loss: 0.00112259 \n",
      "epoch: 8000  | loss: 0.00063264 epoch: 7900  | loss: 0.00057316 \n",
      "\n",
      "epoch: 8000  | loss: 0.00053977 \n",
      "epoch: 7900  | loss: 0.00037622 \n",
      "epoch: 8000  | loss: 0.00037390 \n",
      "epoch: 7900  | loss: 0.00029221 \n",
      "epoch: 8000  | loss: 0.00016317 \n",
      "epoch: 7900  | loss: 0.00009761 \n",
      "epoch: 8100  | loss: 0.00116548 \n",
      "epoch: 8100  | loss: 0.00084699 \n",
      "epoch: 8100  | loss: 0.00041387 epoch: 8100  | loss: 0.00111960 \n",
      "\n",
      "epoch: 8100  | loss: 0.00053629 \n",
      "epoch: 8100  | loss: 0.00037054 \n",
      "epoch: 8000  | loss: 0.00058129 \n",
      "epoch: 8000  | loss: 0.00029522 \n",
      "epoch: 8000  | loss: 0.00037552 \n",
      "epoch: 8100  | loss: 0.00016177 \n",
      "epoch: 8000  | loss: 0.00009823 \n",
      "epoch: 8200  | loss: 0.00123256 \n",
      "epoch: 8200  | loss: 0.00116004 \n",
      "epoch: 8200  | loss: 0.00084146 \n",
      "epoch: 8200  | loss: 0.00111639 \n",
      "epoch: 8200  | loss: 0.00053256 \n",
      "epoch: 8100  | loss: 0.00058892 \n",
      "epoch: 8100  | loss: 0.00037486 \n",
      "epoch: 8100  | loss: 0.00030212 \n",
      "epoch: 8200  | loss: 0.00036699 \n",
      "epoch: 8200  | loss: 0.00016115 epoch: 8100  | loss: 0.00009837 \n",
      "\n",
      "epoch: 8300  | loss: 0.00115444 \n",
      "epoch: 8300  | loss: 0.00083578 \n",
      "epoch: 8300  | loss: 0.00052857 \n",
      "epoch: 8200  | loss: 0.00072253 \n",
      "epoch: 8200  | loss: 0.00037427 \n",
      "epoch: 8300  | loss: 0.00111295 \n",
      "epoch: 8300  | loss: 0.00071235 \n",
      "epoch: 8300  | loss: 0.00036322 \n",
      "epoch: 8200  | loss: 0.00030962 \n",
      "epoch: 8200  | loss: 0.00009682 epoch: 8300  | loss: 0.00017287 \n",
      "\n",
      "epoch: 8400  | loss: 0.00114867 \n",
      "epoch: 8400  | loss: 0.00082994 \n",
      "epoch: 8400  | loss: 0.00048118 \n",
      "epoch: 8300  | loss: 0.00060339 epoch: 8400  | loss: 0.00052432 \n",
      "\n",
      "epoch: 8400  | loss: 0.00110925 \n",
      "epoch: 8300  | loss: 0.00037445 \n",
      "epoch: 8300  | loss: 0.00009674 \n",
      "epoch: 8400  | loss: 0.00035921 \n",
      "epoch: 8300  | loss: 0.00029735 \n",
      "epoch: 8400  | loss: 0.00017203 \n",
      "epoch: 8500  | loss: 0.00082394 \n",
      "epoch: 8400  | loss: 0.00061072 \n",
      "epoch: 8500  | loss: 0.00114273 \n",
      "epoch: 8500  | loss: 0.00110536 \n",
      "epoch: 8500  | loss: 0.00195381 \n",
      "epoch: 8500  | loss: 0.00051979 \n",
      "epoch: 8500  | loss: 0.00035493 epoch: 8400  | loss: 0.00037438 epoch: 8400  | loss: 0.00029587 \n",
      "\n",
      "\n",
      "epoch: 8500  | loss: 0.00015751 \n",
      "epoch: 8400  | loss: 0.00010269 \n",
      "epoch: 8600  | loss: 0.00081778 \n",
      "epoch: 8600  | loss: 0.00113677 \n",
      "epoch: 8500  | loss: 0.00062444 \n",
      "epoch: 8600  | loss: 0.00110137 epoch: 8600  | loss: 0.00044954 \n",
      "\n",
      "epoch: 8500  | loss: 0.00037319 \n",
      "epoch: 8500  | loss: 0.00029694 \n",
      "epoch: 8600  | loss: 0.00015808 \n",
      "epoch: 8600  | loss: 0.00035035 epoch: 8600  | loss: 0.00051496 \n",
      "\n",
      "epoch: 8500  | loss: 0.00058225 \n",
      "epoch: 8700  | loss: 0.00081150 \n",
      "epoch: 8700  | loss: 0.00113074 \n",
      "epoch: 8600  | loss: 0.00227883 \n",
      "epoch: 8700  | loss: 0.00109712 \n",
      "epoch: 8700  | loss: 0.00084458 \n",
      "epoch: 8600  | loss: 0.00037192 epoch: 8600  | loss: 0.00029784 \n"
     ]
    }
   ],
   "source": [
    "dec_lrs = 10**np.linspace(-5,-2,num=10)\n",
    "wds = [0,1,2,3,5,10,20,30,50,100,200]\n",
    "\n",
    "\n",
    "epochs = [1e4]\n",
    "\n",
    "xx, yy, zz = np.meshgrid(dec_lrs, wds, epochs)\n",
    "params = list(np.transpose(np.array([xx.reshape(-1,), yy.reshape(-1,), zz.reshape(-1,)])))\n",
    "\n",
    "from multiprocess import Pool\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with Pool(11) as p:\n",
    "        print(p.map(train, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30bc4d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dec_lrs = 10**np.linspace(-5,-2,num=10)\n",
    "wds = np.array([0,1,2,3,5,10,20,30,50,100,200])\n",
    "path = \"./results/fig5d/\"\n",
    "\n",
    "train_iterss = []\n",
    "test_iterss = []\n",
    "for j in range(dec_lrs.shape[0]):\n",
    "    train_iters = []\n",
    "    test_iters = []\n",
    "    for k in range(wds.shape[0]):\n",
    "        eta2 = dec_lrs[j]\n",
    "        wd2 = wds[k]\n",
    "        train_iter = np.loadtxt(path+\"train_eta2_%.5f_wd_%.1f.txt\"%(eta2,wd2))\n",
    "        test_iter = np.loadtxt(path+\"test_eta2_%.5f_wd_%.1f.txt\"%(eta2,wd2))\n",
    "        #rqi = np.loadtxt(path+\"rqi_eta2_%.5f_wd_%.1f.txt\"%(seed,eta2,wd2))\n",
    "        train_iters.append(train_iter)\n",
    "        test_iters.append(test_iter)\n",
    "    train_iterss.append(train_iters)\n",
    "    test_iterss.append(test_iters)\n",
    "    \n",
    "train_iterss = np.array(train_iterss)\n",
    "test_iterss = np.array(test_iterss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a559140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fad0b085a90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "shp = train_iterss.shape\n",
    "\n",
    "palette = np.array([[160, 218, 57],   # green, compression\n",
    "                    [253, 231, 37],   # yellow, grokking\n",
    "                    [70, 50, 126],   # blue, memorization\n",
    "                    [30,30,30]])  # black, confusion\n",
    "\n",
    "palette = palette\n",
    "\n",
    "id_mat = np.eye(4,)\n",
    "\n",
    "steps = 1e4\n",
    "threshold = 1e3\n",
    "I = np.transpose((train_iterss < (steps -1))*(test_iterss < (steps-1))*((test_iterss-train_iterss)<threshold))*0 +np.transpose((train_iterss < (steps-1))*(test_iterss < (steps-1))*((test_iterss-train_iterss)>threshold))*1+np.transpose((train_iterss < (steps-1))*(test_iterss == (steps-1)))*2+np.transpose((train_iterss == (steps-1))*(test_iterss == (steps-1)))*3\n",
    "\n",
    "RGB = id_mat[I]\n",
    "\n",
    "import torch\n",
    "scale = 100\n",
    "RGBt = np.transpose(RGB, (2,0,1))\n",
    "RGBt = torch.nn.functional.interpolate(torch.tensor(np.array([RGBt]).astype(np.float64)), scale_factor=scale, mode=\"bicubic\")\n",
    "RGBt = np.transpose(RGBt[0], (1,2,0)).detach().numpy()\n",
    "\n",
    "I = np.argmin(np.mean((RGBt.reshape(shp[0]*shp[1]*scale**2,4)[np.newaxis,:]-id_mat[:,np.newaxis,:])**2, axis=2), axis=0).reshape(shp[1]*scale, shp[0]*scale)\n",
    "RGB = palette[I]\n",
    "\n",
    "plt.xlabel(\"decoder learning rate\",fontsize=30)\n",
    "plt.ylabel(\"weight decay\",fontsize=30, labelpad=-0)\n",
    "plt.yticks([0+int(scale/2),int(5.5*scale),int(10.5*scale)],[wds[0], wds[5], wds[10]],fontsize=25)\n",
    "plt.xticks([0+int(scale/2),int(3.5*scale),int(6.5*scale),int(9.5*scale)], [\"1e-5\", \"1e-4\", \"1e-3\", \"1e-2\"],fontsize=25)\n",
    "\n",
    "\n",
    "plt.imshow(RGB[:,::-1], interpolation=\"nearest\", alpha=1.0, aspect=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0f333e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
