{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bc2c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import scipy.linalg\n",
    "import scipy\n",
    "import copy\n",
    "\n",
    "# addition toy\n",
    "def train_add(p=10,\n",
    "          reprs_dim=1,\n",
    "          output_dim=30,\n",
    "          train_num=45,\n",
    "          seed=58,\n",
    "          steps=5000,\n",
    "          eff_steps = 5000,\n",
    "          batch_size=45,\n",
    "          init_scale_reprs=1.0,\n",
    "          init_scale_nn=1.0,\n",
    "          label_scale=1.0,\n",
    "          eta_reprs=1e-3,\n",
    "          eta_dec=1e-4,\n",
    "          log_freq=1000,\n",
    "          width=200,\n",
    "          weight_decay_reprs=0.0,\n",
    "          weight_decay_dec=0.0,\n",
    "          threshold_train_acc=0.9,\n",
    "          threshold_test_acc=0.9,\n",
    "          threshold_rqi=0.95,\n",
    "          threshold_P=0.01,\n",
    "          loss_type=\"MSE\"):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if batch_size > train_num:\n",
    "        batch_size = train_num\n",
    "        print(\"batch size larger than the training set. We have set batch size=training size.\")\n",
    "        \n",
    "    if loss_type == \"CE\":\n",
    "        output_dim = 2*p - 1\n",
    "        print(\"Using cross entropy, setting output_dim=2p-1={}\".format(output_dim))\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Using device:', device)\n",
    "    \n",
    "    print(\"train_num={}\".format(train_num))\n",
    "    print(\"seed={}\".format(seed))\n",
    "    print(\"steps={}\".format(steps))\n",
    "    \n",
    "    reprss = []\n",
    "    reprss_scale = []\n",
    "    losses_nn = []\n",
    "\n",
    "\n",
    "    y_templates = np.random.normal(0,1,size=(2*p-1, output_dim))*label_scale\n",
    "    y_templates = torch.tensor(y_templates, dtype=torch.float, requires_grad=True).to(device)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #-----------------------------Dataset------------------------------#\n",
    "    # the full dataset contains p(p+1)/2 samples. Each sample has input (a, b).\n",
    "    all_num = int(p*(p+1)/2) # tfor addition (abelian group), we deem a+b and b+a the same sample\n",
    "    D0_id = []\n",
    "    xx_id = []\n",
    "    yy_id = []\n",
    "    for i in range(p):\n",
    "        for j in range(i,p):\n",
    "            D0_id.append((i,j))\n",
    "            xx_id.append(i)\n",
    "            yy_id.append(j)\n",
    "    xx_id = np.array(xx_id)\n",
    "    yy_id = np.array(yy_id)\n",
    "\n",
    "    # P0 includes all parallelograms (a,b,c,d) in the full dataset\n",
    "    # A parallelogram means (a,b), (c,d) are training samples and a+b=c+d\n",
    "    P0 = []\n",
    "    P0_id = []\n",
    "    ii = 0\n",
    "    for i in range(all_num):\n",
    "        for j in range(i+1,all_num):\n",
    "            if np.sum(D0_id[i]) == np.sum(D0_id[j]):\n",
    "                P0.append(frozenset({D0_id[i], D0_id[j]}))\n",
    "                P0_id.append(ii)\n",
    "                ii += 1\n",
    "    P0_num = len(P0_id)\n",
    "\n",
    "    # A is equivalent to P0, but converts parallelograms (geometry) \n",
    "    # to coefficients of linear equations (algebra), ready for further analysis.\n",
    "    # For example, p=4, P0 contains a parallelogram (0,3,1,2)\n",
    "    # This translates to a row in A [1, -1, -1, 1], meaning E0 - E1 - E2 + E3 = 0\n",
    "    A = []\n",
    "    eq_id = 0\n",
    "    for i1 in range(P0_num):\n",
    "        i,j = list(P0[i1])[0]\n",
    "        m,n = list(P0[i1])[1]\n",
    "        if i+j==m+n:\n",
    "            x = np.zeros(p,)\n",
    "            x[i] = x[i] + 1; \n",
    "            x[j] = x[j] + 1; \n",
    "            x[m] = x[m] - 1;\n",
    "            x[n] = x[n] - 1;\n",
    "            A.append(x)\n",
    "            eq_id = eq_id + 1\n",
    "    A = np.array(A).astype(int)\n",
    "    \n",
    "    # draw a subset from the full set as training set D\n",
    "    train_id = np.random.choice(all_num,train_num, replace=False)\n",
    "    test_id = np.array(list(set(np.arange(all_num)) - set(train_id)))\n",
    "    inputs_id = np.transpose(np.array([xx_id,yy_id]))\n",
    "    out_id = (xx_id + yy_id)\n",
    "\n",
    "    print(\"----------------------------------------\")\n",
    "    print(\"Task 1: Analyzing the dataset before training...\")\n",
    "    # P0(D) is P_0(D) in paper: it includes all the parallelograms in training set D\n",
    "    P0D_id = []\n",
    "    ii = 0\n",
    "    for i in range(all_num):\n",
    "        for j in range(i+1,all_num):\n",
    "            if np.sum(D0_id[i]) == np.sum(D0_id[j]):\n",
    "                if i in train_id and j in train_id:\n",
    "                    P0D_id.append(ii)\n",
    "                ii += 1\n",
    "    P0D = []\n",
    "    for i in P0D_id:\n",
    "        P0D.append(P0[i])\n",
    "    # P0D_c includes the parallelograms not in P0(D), but in P0. 'c' means complement.\n",
    "    P0D_c_id = set(P0_id) - set(P0D_id)\n",
    "\n",
    "\n",
    "    # PD is P(D) in paper. PD includes P0D and all parallelograms induced from P0D.\n",
    "    # How does induction work?\n",
    "    # Example: (0,3,1,2) being a parallelogram and (2,5,3,4) being a parallelogram\n",
    "    # induce that (0,5,1,4) is also a parallelogram.\n",
    "    # This geometric argument is translated to linear dependence algebraically,\n",
    "    # i.e., {E0+E3=E1+E2, E2+E5=E3+E4} -> {E0+E5=E1+E4}.\n",
    "    PD_id = []\n",
    "    mat = A[P0D_id]\n",
    "    eigs = np.linalg.eigh(np.matmul(np.transpose(mat),mat))[0]\n",
    "    null_dim = np.sum(eigs < 1e-8)\n",
    "    lambda3 = eigs[2]\n",
    "\n",
    "    # a parallelogram can be induced from P0(D) if it is linearly dependent on P0(D).\n",
    "    # linear dependence <=> the rank (of mat) does not change after adding the parallelogram.\n",
    "    # linear independence <=> the rank (of mat) increases by one after adding the parallelogram.\n",
    "    for i in P0D_c_id:\n",
    "        P0D_id_aug = copy.deepcopy(P0D_id)\n",
    "        P0D_id_aug.append(i)\n",
    "        mat_aug = A[P0D_id_aug]\n",
    "        P0D_aug = []\n",
    "        for j in P0D_id_aug:\n",
    "            P0D_aug.append(P0[j])\n",
    "        null_dim_aug = np.sum(np.linalg.eigh(np.matmul(np.transpose(mat_aug),mat_aug))[0] < 1e-8)\n",
    "        if null_dim_aug == null_dim:\n",
    "            PD_id.append(i)\n",
    "\n",
    "    PD_id = PD_id + P0D_id\n",
    "\n",
    "    PD = []\n",
    "    for i in PD_id:\n",
    "        PD.append(P0[i])\n",
    "        \n",
    "    # Dbar(D) contains all the examples that can be got correctly (ideally) given training data\n",
    "    # One may ask: given a test sample i+j, how can the neural network know its answer if never seen it?\n",
    "    # The answer is via a good representation, i.e., parallelograms.\n",
    "    # If there exists a training sample m+n such that i+j=m+n, and (i,j,m,n) is a parallelogram (i.e., Ei+Ej=Em+En)\n",
    "    # then Dec(Ei+Ej) = Dec(Em+En) = Y_{m+n} = Y_{i+j}, i.e., the nueral network can get i+j correct.\n",
    "    Dbar_id = list(copy.deepcopy(train_id))\n",
    "\n",
    "    for i1 in test_id:\n",
    "        flag = 0\n",
    "        for j1 in train_id:\n",
    "            i, j = D0_id[i1]\n",
    "            m, n = D0_id[j1]\n",
    "            if {(i,j),(m,n)} in PD:\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 1:\n",
    "            Dbar_id.append(i1)\n",
    "\n",
    "    # Given training data, without training,\n",
    "    # we are able to determine the ideal accuracy, denoted as \\overline{\\rm Acc} in the paper.\n",
    "    # Empirical accuracy (by training nn, denoted as {\\rm Acc}),\n",
    "    # is upper bounded by the ideal accuracy (except for luck).\n",
    "    acc_ideal = len(Dbar_id)/all_num # the full dataset\n",
    "    acc_ideal_test = (len(Dbar_id)-len(train_id))/len(test_id) # test set\n",
    "    print(\"acc_ideal_test = {}/{}={}\".format(len(Dbar_id)-len(train_id),len(test_id),acc_ideal_test))\n",
    "    print(\"the degree of freedom (except translation/scaling) for the reprsentation is {}\".format(null_dim-2))\n",
    "    print(\"dof=0 means the linear repr is the unique repr, while dof>0 means existence of other reprs\")\n",
    "\n",
    "    # embedding input and output digits to random vectors at initialization, which are trainable.\n",
    "    reprs = torch.nn.parameter.Parameter((torch.rand(p,reprs_dim)-1/2)*init_scale_reprs)\n",
    "    reprs.to(device)\n",
    "    \n",
    "    labels_train = y_templates[out_id[train_id]].detach().clone().requires_grad_(True)\n",
    "    in_id_train = inputs_id[train_id]\n",
    "    out_id_train = out_id[train_id]\n",
    "    \n",
    "    labels_test = y_templates[out_id[test_id]].detach().clone().requires_grad_(True)\n",
    "    in_id_test = inputs_id[test_id]\n",
    "    out_id_test = out_id[test_id]\n",
    "\n",
    "\n",
    "    #--------------------------Neural Network---------------------------#\n",
    "    class NET(nn.Module):\n",
    "        # 2 hidden layer MLP\n",
    "        def __init__(self, input_dim, output_dim, w=width):\n",
    "            super(NET, self).__init__()\n",
    "            self.l1 = nn.Linear(input_dim, w)\n",
    "            self.l2 = nn.Linear(w, w)\n",
    "            self.l3 = nn.Linear(w, output_dim)\n",
    "\n",
    "        def forward(self, x):\n",
    "            f = torch.nn.Tanh()\n",
    "            #f = torch.nn.LeakyReLU(0.2)\n",
    "            self.x1 = f(self.l1(x))\n",
    "            self.x2 = f(self.l2(self.x1))\n",
    "            self.x3 = self.l3(self.x2)\n",
    "            return self.x3\n",
    "\n",
    "    class DEC(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, w=width):\n",
    "            super(DEC, self).__init__()\n",
    "            self.net = NET(reprs_dim, output_dim, w=width)\n",
    "\n",
    "        def forward(self, reprs, x_id):\n",
    "            self.add1 = reprs[x_id[:,0]]\n",
    "            self.add2 = reprs[x_id[:,1]]\n",
    "            # hard code addition\n",
    "            self.add = self.add1 + self.add2\n",
    "            self.out = self.net(self.add)\n",
    "            return self.out\n",
    "\n",
    "    # initialize the decoder. init_scale_nn is the initialization scale.\n",
    "    model = DEC(input_dim=reprs_dim, output_dim=output_dim, w=width)\n",
    "    model.to(device)\n",
    "    for p_ in model.net.parameters():\n",
    "        p_.data = p_.data * init_scale_nn\n",
    "\n",
    "    # collect statistics\n",
    "    losses_train = []\n",
    "    losses_test = []\n",
    "    accs_train = []\n",
    "    accs_test = []\n",
    "    rqis = []\n",
    "    reprss = []\n",
    "    reprss_scale = []\n",
    "\n",
    "    # use different optimizers for representations and the decoder.\n",
    "    optimizer1 = torch.optim.AdamW({reprs}, lr=eta_reprs, weight_decay = weight_decay_reprs)\n",
    "    optimizer2 = torch.optim.AdamW(model.parameters(), lr=eta_dec, weight_decay = weight_decay_dec, betas=(0.9,0.999))\n",
    "\n",
    "    # indicate whether metrics (training/test accuracy, RQI) rise above certain thresholds\n",
    "    reach_acc_train = False\n",
    "    reach_acc_test = False\n",
    "    reach_rqi = False\n",
    "    \n",
    "    print(\"----------------------------------------\")\n",
    "    print(\"Task 2: Training with neural network...\")\n",
    "\n",
    "    for step in range(steps):  # loop over the dataset multiple times\n",
    "\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "\n",
    "        # random batch\n",
    "        choice = np.random.choice(np.arange(train_num), batch_size, replace=False)\n",
    "        \n",
    "        # caluate loss for train/test\n",
    "        outputs_train = model(reprs, in_id_train[choice])\n",
    "        outputs_test = model(reprs, in_id_test)\n",
    "        if loss_type == \"MSE\":\n",
    "            loss_train = torch.mean((outputs_train-labels_train[choice])**2)\n",
    "            loss_test = torch.mean((outputs_test-labels_test)**2)\n",
    "        else:\n",
    "            loss_train = nn.CrossEntropyLoss()(outputs_train, torch.tensor(out_id_train[choice], dtype=torch.long))\n",
    "            loss_test = nn.CrossEntropyLoss()(outputs_test, torch.tensor(out_id_test, dtype=torch.long))\n",
    "            \n",
    "        losses_train.append(loss_train.detach().numpy())\n",
    "        losses_test.append(loss_test.detach().numpy())\n",
    "        \n",
    "        # update weights\n",
    "        loss_train.backward()\n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "\n",
    "        # calculate accuracy for train/test\n",
    "        outputs_train = model(reprs, in_id_train)\n",
    "        if loss_type == \"MSE\":\n",
    "            pred_train_id = torch.argmin(torch.sum((outputs_train.unsqueeze(dim=1) - y_templates.unsqueeze(dim=0))**2, dim=2), dim=1)\n",
    "            pred_test_id = torch.argmin(torch.sum((outputs_test.unsqueeze(dim=1) - y_templates.unsqueeze(dim=0))**2, dim=2), dim=1)\n",
    "        else:\n",
    "            pred_train_id = torch.argmax(outputs_train, dim=1)\n",
    "            pred_test_id = torch.argmax(outputs_test, dim=1)\n",
    "        acc_nn_train = np.mean(pred_train_id.detach().numpy() == out_id_train)\n",
    "        accs_train.append(acc_nn_train)\n",
    "        \n",
    "        outputs_test = model(reprs, in_id_test)\n",
    "        acc_nn_test = np.mean(pred_test_id.detach().numpy() == out_id_test)\n",
    "        accs_test.append(acc_nn_test)\n",
    "\n",
    "        if not reach_acc_train:\n",
    "            if acc_nn_train >= threshold_train_acc:\n",
    "                reach_acc_train = True\n",
    "                iter_train = step\n",
    "                \n",
    "        if not reach_acc_test:\n",
    "            if acc_nn_test >= threshold_test_acc:\n",
    "                reach_acc_test = True\n",
    "                iter_test = step\n",
    "\n",
    "        if step % log_freq == 0:\n",
    "            print(\"step: %d  | loss: %.8f \"%(step, loss_train.detach().numpy()))\n",
    "\n",
    "        # normalized representations (zero mean, unit variance) \n",
    "        reprs_scale = (reprs-torch.mean(reprs,dim=0).unsqueeze(dim=0))/torch.std((reprs-torch.mean(reprs,dim=0).unsqueeze(dim=0)),dim=0,unbiased=True).unsqueeze(dim=0)\n",
    "        \n",
    "        #num_P_ideal: the number of all possible parallelogram\n",
    "        #num_P_real: the number of parallelogram actually appearing in the representation after training\n",
    "        num_P_ideal = 0\n",
    "        num_P_real = 0\n",
    "        for i in range(p):\n",
    "            for j in range(i+1,p):\n",
    "                for m in range(j,p):\n",
    "                    for n in range(m+1,p):\n",
    "                        if (i+n-j-m) == 0:\n",
    "                            num_P_ideal += 1\n",
    "                            dist = reprs_scale[i] + reprs_scale[n] - reprs_scale[m] - reprs_scale[j]\n",
    "                            num_P_real = num_P_real + (torch.mean(dist**2)<threshold_P)\n",
    "                            \n",
    "        # define RQI as ratio of the number of real vs ideal (all) parallelograms\n",
    "        rqi = num_P_real/num_P_ideal\n",
    "        if not reach_rqi:\n",
    "            if rqi > threshold_rqi:\n",
    "                reach_rqi = True\n",
    "                iter_rqi = step\n",
    "                \n",
    "        # Given training set D and representation R after training, out theory can predict the test accuracy\n",
    "        PR = []\n",
    "        PR_id = []\n",
    "        count = 0\n",
    "        for ii in range(P0_num):\n",
    "            i, j = list(P0[ii])[0]\n",
    "            m, n = list(P0[ii])[1]\n",
    "            dist = reprs_scale[i] + reprs_scale[j] - reprs_scale[m] - reprs_scale[n]\n",
    "            if (torch.mean(dist**2)<threshold_P):\n",
    "                PR_id.append(ii)\n",
    "                PR.append(P0[ii])\n",
    "                \n",
    "        # Dbar(D,P). Note this is different from Dbar(D).\n",
    "        Dbar_P_id = list(copy.deepcopy(train_id))\n",
    "\n",
    "        for i1 in test_id:\n",
    "            flag = 0\n",
    "            for j1 in train_id:\n",
    "                i, j = D0_id[i1]\n",
    "                m, n = D0_id[j1]\n",
    "                if {(i,j),(m,n)} in PR:\n",
    "                    flag = 1\n",
    "                    break\n",
    "            if flag == 1:\n",
    "                Dbar_P_id.append(i1)\n",
    "                \n",
    "        acc_pred = len(Dbar_P_id)/all_num\n",
    "        acc_pred_test = (len(Dbar_P_id)-len(train_id))/len(test_id)\n",
    "        \n",
    "\n",
    "        rqis.append(rqi)\n",
    "        reprss.append(copy.deepcopy(reprs.detach().numpy()))\n",
    "        reprss_scale.append(copy.deepcopy(reprs_scale.detach().numpy()))\n",
    "\n",
    "\n",
    "    if not reach_acc_train:\n",
    "        iter_train = step\n",
    "        \n",
    "    if not reach_acc_test:\n",
    "        iter_test = step\n",
    "\n",
    "    if not reach_rqi:\n",
    "        iter_rqi = step\n",
    "\n",
    "    rqis = np.array(rqis)\n",
    "    losses_train = np.array(losses_train)\n",
    "    losses_test = np.array(losses_test)\n",
    "    accs_train = np.array(accs_train)\n",
    "    accs_test = np.array(accs_test)\n",
    "    reprss = np.array(reprss)\n",
    "    reprss_scale = np.array(reprss_scale)\n",
    "    \n",
    "    print(\"final train acc=%.4f, test acc=%.4f, RQI=%.4f\"%(acc_nn_train, acc_nn_test, rqi))\n",
    "    print(\"Steps to reach thresholds: train acc={}, test acc={}, RQI={}\".format(iter_train, iter_test, iter_rqi))\n",
    "    \n",
    "    \n",
    "    #---------------------effective theory-------------------#\n",
    "    print(\"----------------------------------------\")\n",
    "    print(\"Task 3: Training with effective loss...\")\n",
    "    E = reprss_scale[0,:,0]\n",
    "    Z0 = np.sum(E**2)\n",
    "    l0 = np.sum(np.sum(A*E[np.newaxis,:], axis=1)**2)\n",
    "    temp = np.sum(np.sum(A*E[np.newaxis,:], axis=1)[:,np.newaxis]*A, axis=0)\n",
    "    dE = 2*l0/Z0**2*E - 2/Z0*temp\n",
    "    Es_eff = []\n",
    "    losses_eff = []\n",
    "\n",
    "    step = eta_reprs\n",
    "    n_step = eff_steps\n",
    "\n",
    "    for i in range(n_step):\n",
    "        Es_eff.append(copy.deepcopy(E))\n",
    "        l0 = np.sum(np.sum(A*E[np.newaxis,:], axis=1)**2)\n",
    "        losses_eff.append(l0)\n",
    "        temp = np.sum(np.sum(A*E[np.newaxis,:], axis=1)[:,np.newaxis]*A, axis=0)\n",
    "        dE = 2*l0/Z0**2*E - 2/Z0*temp\n",
    "        E = E + step*dE\n",
    "        if i % log_freq == 0:\n",
    "            print(\"step: %d  | loss: %.8f \"%(i, l0))\n",
    "    Es_eff = np.array(Es_eff)    \n",
    "    losses_eff = np.array(losses_eff)\n",
    "    print(\"saving trajectories...\")\n",
    "    \n",
    "    # collect return results in a dictionary\n",
    "    # task 1 data\n",
    "    dic = {}\n",
    "    dic[\"all_num\"] = all_num\n",
    "    dic[\"train_num\"] = train_num\n",
    "    dic[\"test_num\"] = all_num - train_num\n",
    "    dic[\"train_ratio\"] = train_num/all_num\n",
    "    dic[\"test_ratio\"] = 1 - train_num/all_num\n",
    "    dic[\"ideal_test_acc\"] = acc_ideal_test\n",
    "    dic[\"ideal_acc\"] = acc_ideal\n",
    "    dic[\"pred_test_acc\"] = acc_pred_test\n",
    "    dic[\"pred_acc\"] = ((all_num - train_num)*acc_pred_test + train_num*1.0)/all_num\n",
    "    dic[\"dof\"] = null_dim\n",
    "    \n",
    "    # task 2 data\n",
    "    dic[\"loss_train\"] = losses_train\n",
    "    dic[\"loss_test\"] = losses_test\n",
    "    dic[\"acc_train\"] = accs_train\n",
    "    dic[\"acc_test\"] = accs_test\n",
    "    dic[\"acc\"] = ((all_num - train_num)*accs_test + train_num*accs_train)/all_num\n",
    "    dic[\"repr_nn\"] = reprss\n",
    "    dic[\"repr_normalized_nn\"] = reprss_scale\n",
    "    dic[\"rqi\"] = rqis\n",
    "    dic[\"iter_train\"] = iter_train\n",
    "    dic[\"iter_test\"] = iter_test\n",
    "    dic[\"iter_rqi\"] = iter_rqi\n",
    "    \n",
    "    # task 3 data\n",
    "    dic[\"repr_eff\"] = Es_eff\n",
    "    dic[\"loss_eff\"] = losses_eff\n",
    "    \n",
    "    # configuration parameters\n",
    "    dic[\"p\"] = p\n",
    "    dic[\"repr_dim\"] = reprs_dim\n",
    "    dic[\"seed\"] = seed\n",
    "    dic[\"steps\"] = steps\n",
    "    dic[\"eff_steps\"] = eff_steps\n",
    "    dic[\"batch_size\"] = batch_size\n",
    "    dic[\"init_scale_reprs\"] = init_scale_reprs\n",
    "    dic[\"init_scale_nn\"] = init_scale_nn\n",
    "    dic[\"label_scale\"] = label_scale\n",
    "    dic[\"eta_repr\"] = eta_reprs\n",
    "    dic[\"eta_dec\"] = eta_dec\n",
    "    dic[\"width\"] = width\n",
    "    dic[\"weight_decay_repr\"] = weight_decay_reprs\n",
    "    dic[\"weight_decay_dec\"] = weight_decay_dec\n",
    "    dic[\"threshold_train_acc\"] = threshold_train_acc\n",
    "    dic[\"threshold_test_acc\"] = threshold_test_acc\n",
    "    dic[\"threshold_rqi\"] = threshold_rqi\n",
    "    dic[\"threshold_P\"] = threshold_P\n",
    "    dic[\"loss_type\"] = loss_type\n",
    "    return dic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b9eb57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
