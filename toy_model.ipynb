{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b76035e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_ideal_test=10/10=1.0\n",
      "epoch: 0  | loss: 1.01850033 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_83400/1988763294.py:171: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_train = torch.tensor(y_templates[out_id[train_id]], dtype=torch.float, requires_grad=True)\n",
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_83400/1988763294.py:176: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_test = torch.tensor(y_templates[out_id[test_id]], dtype=torch.float, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import scipy.linalg\n",
    "import scipy\n",
    "import copy\n",
    "\n",
    "\n",
    "# This jupyter notebook has four parts. Only Part I and Part III are related to neural networks.\n",
    "# Part II and Part IV are just serving to support the argument \"representation => generalization\".\n",
    "# For a self-contained simplified version involving only Part I and III, please use toy_model_nn.ipynb\n",
    "\n",
    "#################### Part I: Hyperparameters ####################\n",
    "eta1 = 1e-4 # encoder learning rate (gradient)/ representation learning rate (natural gradient)\n",
    "eta2 = 1e-3 # decoder learning rate\n",
    "seed = 5 # random seed\n",
    "input_dim = 10 # dimension of input random vector\n",
    "latent_dim = 1 # dimension of representation space\n",
    "output_dim = 10 # dimension of input random vector\n",
    "p = 10 # base. i,j are integers in {0,1,2,...,p-1}.\n",
    "epochs = 1 # training iterations\n",
    "log = 100 # logging frequency\n",
    "dec_w = 200 # decoder width\n",
    "wd = 0 # decoder weight decay\n",
    "train_num = 45 # size of training set, no replacement (full dataset size=p(p+1)/2. 55 for p=10)\n",
    "modulo = False # If true, o=i+j(mod p); else o=i+j.\n",
    "natural_gradient = True # If true, use natural gradient; else use the common parameter gradient.\n",
    "\n",
    "################## Part II: Generate and Analyze the training set ##############\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "#------------training set-----------#\n",
    "D0_id = [] # D0 is the full dataset, D0_id=[(0,0),(0,1),...,(p-1,p-1)]. D0 contains p*(p-1)/2 samples.\n",
    "xx_id = [] # xx_id is the list of i in (i,j) in D0_id. xx_id = [0,0,...,p-1]\n",
    "yy_id = [] # yy_id is the list of j in (i,j) in D0_id. yy_id = [0,1,...,p-1]\n",
    "for i in range(p):\n",
    "    for j in range(i,p):\n",
    "        D0_id.append((i,j))\n",
    "        xx_id.append(i)\n",
    "        yy_id.append(j)\n",
    "        \n",
    "xx_id = np.array(xx_id)\n",
    "yy_id = np.array(yy_id)\n",
    "\n",
    "all_num = int(p*(p+1)/2)\n",
    "train_id = np.random.choice(all_num,train_num, replace=False) # select training set id\n",
    "test_id = np.array(list(set(np.arange(all_num)) - set(train_id))) # select testing set id\n",
    "\n",
    "#-----------Parallelogram set---------#\n",
    "P0 = [] # P0 is the set of all possible parallelograms\n",
    "P0_id = []\n",
    "\n",
    "ii = 0\n",
    "for i in range(all_num):\n",
    "    for j in range(i+1,all_num):\n",
    "        if np.sum(D0_id[i]) == np.sum(D0_id[j]):\n",
    "            P0.append(frozenset({D0_id[i], D0_id[j]}))\n",
    "            P0_id.append(ii)\n",
    "            ii += 1\n",
    "\n",
    "P0_num = len(P0_id) # P0_num is the number of elements in P0\n",
    "\n",
    "#---------Linear Equation set---------#\n",
    "A = []\n",
    "eq_id = 0\n",
    "\n",
    "for i1 in range(P0_num):\n",
    "    i,j = list(P0[i1])[0]\n",
    "    m,n = list(P0[i1])[1]\n",
    "    if i+j==m+n:\n",
    "        x = np.zeros(p,)\n",
    "        x[i] = x[i] + 1; \n",
    "        x[j] = x[j] + 1; \n",
    "        x[m] = x[m] - 1;\n",
    "        x[n] = x[n] - 1;\n",
    "        A.append(x)\n",
    "        eq_id = eq_id + 1\n",
    "\n",
    "A = np.array(A).astype(int)\n",
    "\n",
    "# P0(D)\n",
    "P0D_id = []\n",
    "\n",
    "\n",
    "#----------Predict testing accuracy with Parallelogram set-------------#\n",
    "ii = 0\n",
    "for i in range(all_num):\n",
    "    for j in range(i+1,all_num):\n",
    "        if np.sum(D0_id[i]) == np.sum(D0_id[j]):\n",
    "            if i in train_id and j in train_id:\n",
    "                P0D_id.append(ii)\n",
    "            ii += 1\n",
    "\n",
    "P0D = []\n",
    "for i in P0D_id:\n",
    "    P0D.append(P0[i])\n",
    "\n",
    "# P0D_c\n",
    "P0D_c_id = set(P0_id) - set(P0D_id)\n",
    "\n",
    "\n",
    "# PD\n",
    "PD_id = []\n",
    "\n",
    "for i in P0D_c_id:\n",
    "    P0D_id_aug = copy.deepcopy(P0D_id)\n",
    "    P0D_id_aug.append(i)\n",
    "    P0D_aug = []\n",
    "    for j in P0D_id_aug:\n",
    "        P0D_aug.append(P0[j])\n",
    "    null_dim_1 = np.sum(np.linalg.eigh(np.matmul(np.transpose(A[P0D_id]),A[P0D_id]))[0] < 1e-8)\n",
    "    null_dim_2 = np.sum(np.linalg.eigh(np.matmul(np.transpose(A[P0D_id_aug]),A[P0D_id_aug]))[0] < 1e-8)\n",
    "    if null_dim_1 == null_dim_2:\n",
    "        PD_id.append(i)\n",
    "\n",
    "PD_id = PD_id + P0D_id\n",
    "\n",
    "PD = []\n",
    "for i in PD_id:\n",
    "    PD.append(P0[i])\n",
    "\n",
    "\n",
    "RQI_ideal = len(PD)/P0_num # This is ideal RQI\n",
    "\n",
    "\n",
    "# Dbar(D)\n",
    "Dbar_id = list(copy.deepcopy(train_id))\n",
    "\n",
    "for i1 in test_id:\n",
    "    flag = 0\n",
    "    for j1 in train_id:\n",
    "        i, j = D0_id[i1]\n",
    "        m, n = D0_id[j1]\n",
    "        if {(i,j),(m,n)} in PD:\n",
    "            flag = 1\n",
    "            break\n",
    "    if flag == 1:\n",
    "        Dbar_id.append(i1)\n",
    "\n",
    "# accuracy in the ideal case. acc_ideal: the whole dataset; acc_ideal_test: only testing set.\n",
    "acc_ideal = len(Dbar_id)/all_num\n",
    "acc_ideal_test = (len(Dbar_id)-len(train_id))/len(test_id)\n",
    "\n",
    "print(\"acc_ideal_test={}/{}={}\".format((len(Dbar_id)-len(train_id)), len(test_id), (len(Dbar_id)-len(train_id))/len(test_id)))\n",
    "\n",
    "\n",
    "################### Part III: trainining neural networks ######################\n",
    "\n",
    "\n",
    "# inputs\n",
    "x_templates = np.random.normal(0,1,size=(p, output_dim)) # input random vectors\n",
    "if modulo == False:\n",
    "    y_templates = np.random.normal(0,1,size=(2*p-1, output_dim)) # output random vectors\n",
    "else:\n",
    "    y_templates = np.random.normal(0,1,size=(p, output_dim)) # output random vectors\n",
    "    \n",
    "x_templates = torch.tensor(x_templates, dtype=torch.float, requires_grad=True)\n",
    "y_templates = torch.tensor(y_templates, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "# labels\n",
    "inputs_id = np.transpose(np.array([xx_id,yy_id]))\n",
    "if modulo == False:\n",
    "    out_id = (xx_id + yy_id)\n",
    "else:\n",
    "    out_id = (xx_id + yy_id) % p\n",
    "    \n",
    "# training set\n",
    "labels_train = torch.tensor(y_templates[out_id[train_id]], dtype=torch.float, requires_grad=True)\n",
    "inputs_train = torch.cat([x_templates[xx_id[train_id]],x_templates[yy_id[train_id]]], dim=1)\n",
    "out_id_train = out_id[train_id]\n",
    "\n",
    "# testing set\n",
    "labels_test = torch.tensor(y_templates[out_id[test_id]], dtype=torch.float, requires_grad=True)\n",
    "inputs_test = torch.cat([x_templates[xx_id[test_id]],x_templates[yy_id[test_id]]], dim=1)\n",
    "out_id_test = out_id[test_id]\n",
    "\n",
    "# Define neural networks\n",
    "class NET(nn.Module): # base MLP model\n",
    "    def __init__(self, input_dim, output_dim, w=200):\n",
    "        super(NET, self).__init__()\n",
    "        self.l1 = nn.Linear(input_dim, w)\n",
    "        self.l2 = nn.Linear(w, w)\n",
    "        self.l3 = nn.Linear(w, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = torch.nn.Tanh()\n",
    "        self.x1 = f(self.l1(x))\n",
    "        self.x2 = f(self.l2(self.x1))\n",
    "        self.x3 = self.l3(self.x2)\n",
    "        return self.x3\n",
    "\n",
    "class DEC(nn.Module): # Decoder\n",
    "    def __init__(self, input_dim, output_dim, w=200):\n",
    "        super(DEC, self).__init__()\n",
    "        self.net = NET(input_dim, output_dim, w=dec_w)\n",
    "\n",
    "    def forward(self, latent, x_id):\n",
    "        self.add1 = latent[x_id[:,0]]\n",
    "        self.add2 = latent[x_id[:,1]]\n",
    "        self.add = self.add1 + self.add2 # addition in representation space\n",
    "        self.out = self.net(self.add)\n",
    "        return self.out\n",
    "\n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, w=200, input_dim=1, output_dim=1):\n",
    "        super(AE, self).__init__()\n",
    "        self.enc = NET(input_dim, latent_dim, w=w)\n",
    "        self.dec = DEC(latent_dim, output_dim, w=w)\n",
    "\n",
    "    def forward(self, x, x_id):\n",
    "        self.latent = self.enc(x)\n",
    "        self.out = self.dec(self.latent,x_id)\n",
    "\n",
    "        return self.out\n",
    "\n",
    "\n",
    "model = AE(input_dim=input_dim, output_dim=output_dim, w=200)\n",
    "\n",
    "\n",
    "if natural_gradient == True:\n",
    "    latent = torch.nn.parameter.Parameter(model.enc(x_templates).clone())\n",
    "    optimizer1 = torch.optim.Adam({latent}, lr=eta1)\n",
    "else:\n",
    "    optimizer1 = torch.optim.Adam(model.enc.parameters(), lr=eta1)\n",
    "    \n",
    "optimizer2 = torch.optim.AdamW(model.dec.parameters(), lr=eta2, weight_decay=wd)\n",
    "\n",
    "\n",
    "reach_acc_test = False\n",
    "reach_acc_train = False\n",
    "reach_rqi = False\n",
    "\n",
    "### training ###\n",
    "\n",
    "test_acc_epochs = []\n",
    "train_acc_epochs = []\n",
    "\n",
    "for epoch in range(epochs): \n",
    "\n",
    "    optimizer1.zero_grad()\n",
    "    optimizer2.zero_grad()\n",
    "\n",
    "    # updata model parameters\n",
    "    outputs_train = model.dec(latent, inputs_id[train_id])\n",
    "    outputs_test = model.dec(latent, inputs_id[test_id])\n",
    "    loss_train = torch.mean((outputs_train-labels_train)**2)\n",
    "    loss_train.backward()\n",
    "    optimizer1.step()\n",
    "    optimizer2.step()\n",
    "\n",
    "\n",
    "    # calculate accuracy based on nearest neighbor\n",
    "    pred_train_id = torch.argmin(torch.sum((outputs_train.unsqueeze(dim=1) - y_templates.unsqueeze(dim=0))**2, dim=2), dim=1)\n",
    "    pred_test_id = torch.argmin(torch.sum((outputs_test.unsqueeze(dim=1) - y_templates.unsqueeze(dim=0))**2, dim=2), dim=1)\n",
    "    acc_nn_train = np.mean(pred_train_id.detach().numpy() == out_id_train) # training acc\n",
    "    acc_nn_test = np.mean(pred_test_id.detach().numpy() == out_id_test) # testing acc\n",
    "    acc_nn = (acc_nn_train*train_id.shape[0] + acc_nn_test*test_id.shape[0])/all_num # whole accuract\n",
    "    test_acc_epochs.append(acc_nn_test)\n",
    "    train_acc_epochs.append(acc_nn_train)\n",
    "\n",
    "    # check if accuracy reaches a threshold (grokking time)\n",
    "    if not reach_acc_train: # train\n",
    "        if acc_nn_train >= 0.9:\n",
    "            reach_acc_train = True\n",
    "            iter_train = epoch\n",
    "            \n",
    "            \n",
    "    if not reach_acc_test: # test\n",
    "        if acc_nn_test >= 0.9:\n",
    "            reach_acc_test = True\n",
    "            iter_test = epoch\n",
    "\n",
    "    # Count parallelograms in representation \n",
    "    PR = []\n",
    "    PR_id = []\n",
    "    if natural_gradient == False:\n",
    "        latent = model.enc(x_templates).clone()\n",
    "    latent_scale = latent/torch.std(latent,dim=0).unsqueeze(dim=0)\n",
    "    \n",
    "    count = 0\n",
    "    for ii in range(P0_num):\n",
    "        i, j = list(P0[ii])[0]\n",
    "        m, n = list(P0[ii])[1]\n",
    "        dist = latent_scale[i] + latent_scale[j] - latent_scale[m] - latent_scale[n]\n",
    "        if (torch.mean(dist**2)<0.01):\n",
    "            PR_id.append(ii)\n",
    "            PR.append(P0[ii])\n",
    "\n",
    "    rqi = len(PR)/P0_num\n",
    "\n",
    "    # check if RQI reaches a high threshold (time scale of representation learning)\n",
    "    if not reach_rqi:\n",
    "        if rqi > 0.99:\n",
    "            reach_rqi = True\n",
    "            iter_rqi = epoch\n",
    "        \n",
    "    # logging\n",
    "    if epoch % log == 0:\n",
    "        print(\"epoch: %d  | loss: %.8f \"%(epoch, loss_train.detach().numpy()))\n",
    "\n",
    "# if fails within compute budget      \n",
    "if not reach_acc_test:\n",
    "    iter_test = epoch\n",
    "\n",
    "if not reach_acc_train:\n",
    "    iter_train = epoch\n",
    "\n",
    "if not reach_rqi:\n",
    "    iter_rqi = epoch\n",
    "\n",
    "\n",
    "################### Part IV: Postprocessing ######################\n",
    "# predict accuracy, based on the training set and the real representation\n",
    "eigs = np.linalg.eigh(np.matmul(np.transpose(A[PR_id]),A[PR_id]))[0]\n",
    "dof = np.sum(eigs < 1e-8) # degree of freedom of the representation\n",
    "if dof < p:\n",
    "    lambda_ = eigs[dof]\n",
    "    iter_pred = 1/lambda_\n",
    "    iter_pred_rqi = iter_pred\n",
    "else:\n",
    "    iter_pred_rqi = -1\n",
    "\n",
    "# Dbar(P)\n",
    "Dbar_id = list(copy.deepcopy(train_id))\n",
    "\n",
    "for i1 in test_id:\n",
    "    flag = 0\n",
    "    for j1 in train_id:\n",
    "        i, j = D0_id[i1]\n",
    "        m, n = D0_id[j1]\n",
    "        if {(i,j),(m,n)} in PR:\n",
    "            flag = 1\n",
    "            break\n",
    "    if flag == 1:\n",
    "        Dbar_id.append(i1)\n",
    "\n",
    "acc_pred = len(Dbar_id)/all_num\n",
    "acc_pred_test = (len(Dbar_id)-len(train_id))/len(test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "358d1a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fba7883d3d0>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(epochs), train_acc_epochs)\n",
    "plt.plot(np.arange(epochs), test_acc_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ecb03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
