{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b76035e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_75660/3426065576.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_train = torch.tensor(y_templates[out_id[train_id]], dtype=torch.float, requires_grad=True)\n",
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_75660/3426065576.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_test = torch.tensor(y_templates[out_id[test_id]], dtype=torch.float, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  | loss: 0.82493395 \n",
      "epoch: 100  | loss: 0.76302254 \n",
      "epoch: 200  | loss: 0.73361290 \n",
      "epoch: 300  | loss: 0.69287986 \n",
      "epoch: 400  | loss: 0.62817329 \n",
      "epoch: 500  | loss: 0.52927905 \n",
      "epoch: 600  | loss: 0.42162097 \n",
      "epoch: 700  | loss: 0.34450689 \n",
      "epoch: 800  | loss: 0.28353387 \n",
      "epoch: 900  | loss: 0.22333899 \n",
      "epoch: 1000  | loss: 0.17154217 \n",
      "epoch: 1100  | loss: 0.13550892 \n",
      "epoch: 1200  | loss: 0.10768575 \n",
      "epoch: 1300  | loss: 0.08296081 \n",
      "epoch: 1400  | loss: 0.06331909 \n",
      "epoch: 1500  | loss: 0.04918868 \n",
      "epoch: 1600  | loss: 0.03941762 \n",
      "epoch: 1700  | loss: 0.03237715 \n",
      "epoch: 1800  | loss: 0.02737385 \n",
      "epoch: 1900  | loss: 0.02388324 \n",
      "epoch: 2000  | loss: 0.02099634 \n",
      "epoch: 2100  | loss: 0.01801854 \n",
      "epoch: 2200  | loss: 0.01477067 \n",
      "epoch: 2300  | loss: 0.01096826 \n",
      "epoch: 2400  | loss: 0.00682222 \n",
      "epoch: 2500  | loss: 0.00335687 \n",
      "epoch: 2600  | loss: 0.00138202 \n",
      "epoch: 2700  | loss: 0.00056325 \n",
      "epoch: 2800  | loss: 0.00034914 \n",
      "epoch: 2900  | loss: 0.00020545 \n",
      "epoch: 3000  | loss: 0.00009560 \n",
      "epoch: 3100  | loss: 0.00010378 \n",
      "epoch: 3200  | loss: 0.00006334 \n",
      "epoch: 3300  | loss: 0.00004885 \n",
      "epoch: 3400  | loss: 0.00005212 \n",
      "epoch: 3500  | loss: 0.00006026 \n",
      "epoch: 3600  | loss: 0.00006678 \n",
      "epoch: 3700  | loss: 0.00004326 \n",
      "epoch: 3800  | loss: 0.00017327 \n",
      "epoch: 3900  | loss: 0.00003723 \n",
      "epoch: 4000  | loss: 0.00002228 \n",
      "epoch: 4100  | loss: 0.00003530 \n",
      "epoch: 4200  | loss: 0.00006453 \n",
      "epoch: 4300  | loss: 0.00009546 \n",
      "epoch: 4400  | loss: 0.00011534 \n",
      "epoch: 4500  | loss: 0.00009835 \n",
      "epoch: 4600  | loss: 0.00002290 \n",
      "epoch: 4700  | loss: 0.00001760 \n",
      "epoch: 4800  | loss: 0.00001291 \n",
      "epoch: 4900  | loss: 0.00004342 \n",
      "epoch: 5000  | loss: 0.00009684 \n",
      "epoch: 5100  | loss: 0.00003715 \n",
      "epoch: 5200  | loss: 0.00001588 \n",
      "epoch: 5300  | loss: 0.00005035 \n",
      "epoch: 5400  | loss: 0.00000951 \n",
      "epoch: 5500  | loss: 0.00006531 \n",
      "epoch: 5600  | loss: 0.00005089 \n",
      "epoch: 5700  | loss: 0.00003025 \n",
      "epoch: 5800  | loss: 0.00001353 \n",
      "epoch: 5900  | loss: 0.00002403 \n",
      "epoch: 6000  | loss: 0.00002816 \n",
      "epoch: 6100  | loss: 0.00000819 \n",
      "epoch: 6200  | loss: 0.00004707 \n",
      "epoch: 6300  | loss: 0.00001199 \n",
      "epoch: 6400  | loss: 0.00001449 \n",
      "epoch: 6500  | loss: 0.00001561 \n",
      "epoch: 6600  | loss: 0.00012021 \n",
      "epoch: 6700  | loss: 0.00001467 \n",
      "epoch: 6800  | loss: 0.00001177 \n",
      "epoch: 6900  | loss: 0.00000437 \n",
      "epoch: 7000  | loss: 0.00001169 \n",
      "epoch: 7100  | loss: 0.00004022 \n",
      "epoch: 7200  | loss: 0.00002036 \n",
      "epoch: 7300  | loss: 0.00001734 \n",
      "epoch: 7400  | loss: 0.00000624 \n",
      "epoch: 7500  | loss: 0.00006114 \n",
      "epoch: 7600  | loss: 0.00000318 \n",
      "epoch: 7700  | loss: 0.00003300 \n",
      "epoch: 7800  | loss: 0.00001188 \n",
      "epoch: 7900  | loss: 0.00000932 \n",
      "epoch: 8000  | loss: 0.00002975 \n",
      "epoch: 8100  | loss: 0.00000431 \n",
      "epoch: 8200  | loss: 0.00003687 \n",
      "epoch: 8300  | loss: 0.00001626 \n",
      "epoch: 8400  | loss: 0.00005260 \n",
      "epoch: 8500  | loss: 0.00002145 \n",
      "epoch: 8600  | loss: 0.00004356 \n",
      "epoch: 8700  | loss: 0.00000764 \n",
      "epoch: 8800  | loss: 0.00003196 \n",
      "epoch: 8900  | loss: 0.00002873 \n",
      "epoch: 9000  | loss: 0.00001220 \n",
      "epoch: 9100  | loss: 0.00001323 \n",
      "epoch: 9200  | loss: 0.00002103 \n",
      "epoch: 9300  | loss: 0.00007170 \n",
      "epoch: 9400  | loss: 0.00000516 \n",
      "epoch: 9500  | loss: 0.00001454 \n",
      "epoch: 9600  | loss: 0.00004218 \n",
      "epoch: 9700  | loss: 0.00001949 \n",
      "epoch: 9800  | loss: 0.00001917 \n",
      "epoch: 9900  | loss: 0.00004924 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import scipy.linalg\n",
    "import scipy\n",
    "import copy\n",
    "\n",
    "\n",
    "# This notebook is a simplified version of toy_model.ipynb, only focusing on neural network training.\n",
    "\n",
    "\n",
    "#################### Part I: Hyperparameters ####################\n",
    "eta1 = 1e-4 # encoder learning rate (gradient)/ representation learning rate (natural gradient)\n",
    "eta2 = 1e-3 # decoder learning rate\n",
    "seed = 4 # random seed\n",
    "input_dim = 10 # dimension of input random vector\n",
    "latent_dim = 1 # dimension of representation space\n",
    "output_dim = 10 # dimension of input random vector\n",
    "p = 10 # base. i,j are integers in {0,1,2,...,p-1}.\n",
    "epochs = 10000 # training iterations\n",
    "log = 100 # logging frequency\n",
    "dec_w = 200 # decoder width\n",
    "wd = 0 # decoder weight decay\n",
    "train_num = 45 # size of training set, no replacement (full dataset size=p(p+1)/2. 55 for p=10)\n",
    "modulo = False # If true, o=i+j(mod p); else o=i+j.\n",
    "natural_gradient = True # If true, use natural gradient; else use the common parameter gradient.\n",
    "\n",
    "\n",
    "\n",
    "################### Part III: trainining neural networks ######################\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# dataset\n",
    "D0_id = [] # D0 is the full dataset, D0_id=[(0,0),(0,1),...,(p-1,p-1)]. D0 contains p*(p-1)/2 samples.\n",
    "xx_id = [] # xx_id is the list of i in (i,j) in D0_id. xx_id = [0,0,...,p-1]\n",
    "yy_id = [] # yy_id is the list of j in (i,j) in D0_id. yy_id = [0,1,...,p-1]\n",
    "for i in range(p):\n",
    "    for j in range(i,p):\n",
    "        D0_id.append((i,j))\n",
    "        xx_id.append(i)\n",
    "        yy_id.append(j)\n",
    "        \n",
    "xx_id = np.array(xx_id)\n",
    "yy_id = np.array(yy_id)\n",
    "\n",
    "all_num = int(p*(p+1)/2)\n",
    "train_id = np.random.choice(all_num,train_num, replace=False) # select training set id\n",
    "test_id = np.array(list(set(np.arange(all_num)) - set(train_id))) # select testing set id\n",
    "\n",
    "# parallelogram set\n",
    "P0 = [] # P0 is the set of all possible parallelograms\n",
    "P0_id = []\n",
    "\n",
    "ii = 0\n",
    "for i in range(all_num):\n",
    "    for j in range(i+1,all_num):\n",
    "        if np.sum(D0_id[i]) == np.sum(D0_id[j]):\n",
    "            P0.append(frozenset({D0_id[i], D0_id[j]}))\n",
    "            P0_id.append(ii)\n",
    "            ii += 1\n",
    "\n",
    "P0_num = len(P0_id)\n",
    "        \n",
    "    \n",
    "# inputs\n",
    "x_templates = np.random.normal(0,1,size=(p, output_dim)) # input random vectors\n",
    "if modulo == False:\n",
    "    y_templates = np.random.normal(0,1,size=(2*p-1, output_dim)) # output random vectors\n",
    "else:\n",
    "    y_templates = np.random.normal(0,1,size=(p, output_dim)) # output random vectors\n",
    "    \n",
    "x_templates = torch.tensor(x_templates, dtype=torch.float, requires_grad=True)\n",
    "y_templates = torch.tensor(y_templates, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "# labels\n",
    "inputs_id = np.transpose(np.array([xx_id,yy_id]))\n",
    "if modulo == False:\n",
    "    out_id = (xx_id + yy_id)\n",
    "else:\n",
    "    out_id = (xx_id + yy_id) % p\n",
    "    \n",
    "# training set\n",
    "labels_train = torch.tensor(y_templates[out_id[train_id]], dtype=torch.float, requires_grad=True)\n",
    "inputs_train = torch.cat([x_templates[xx_id[train_id]],x_templates[yy_id[train_id]]], dim=1)\n",
    "out_id_train = out_id[train_id]\n",
    "\n",
    "# testing set\n",
    "labels_test = torch.tensor(y_templates[out_id[test_id]], dtype=torch.float, requires_grad=True)\n",
    "inputs_test = torch.cat([x_templates[xx_id[test_id]],x_templates[yy_id[test_id]]], dim=1)\n",
    "out_id_test = out_id[test_id]\n",
    "\n",
    "# Define neural networks\n",
    "class NET(nn.Module): # base MLP model\n",
    "    def __init__(self, input_dim, output_dim, w=200):\n",
    "        super(NET, self).__init__()\n",
    "        self.l1 = nn.Linear(input_dim, w)\n",
    "        self.l2 = nn.Linear(w, w)\n",
    "        self.l3 = nn.Linear(w, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = torch.nn.Tanh()\n",
    "        self.x1 = f(self.l1(x))\n",
    "        self.x2 = f(self.l2(self.x1))\n",
    "        self.x3 = self.l3(self.x2)\n",
    "        return self.x3\n",
    "\n",
    "class DEC(nn.Module): # Decoder\n",
    "    def __init__(self, input_dim, output_dim, w=200):\n",
    "        super(DEC, self).__init__()\n",
    "        self.net = NET(input_dim, output_dim, w=dec_w)\n",
    "\n",
    "    def forward(self, latent, x_id):\n",
    "        self.add1 = latent[x_id[:,0]]\n",
    "        self.add2 = latent[x_id[:,1]]\n",
    "        self.add = self.add1 + self.add2 # addition in representation space\n",
    "        self.out = self.net(self.add)\n",
    "        return self.out\n",
    "\n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, w=200, input_dim=1, output_dim=1):\n",
    "        super(AE, self).__init__()\n",
    "        self.enc = NET(input_dim, latent_dim, w=w)\n",
    "        self.dec = DEC(latent_dim, output_dim, w=w)\n",
    "\n",
    "    def forward(self, x, x_id):\n",
    "        self.latent = self.enc(x)\n",
    "        self.out = self.dec(self.latent,x_id)\n",
    "\n",
    "        return self.out\n",
    "\n",
    "\n",
    "model = AE(input_dim=input_dim, output_dim=output_dim, w=200)\n",
    "\n",
    "\n",
    "if natural_gradient == True:\n",
    "    latent = torch.nn.parameter.Parameter(model.enc(x_templates).clone())\n",
    "    optimizer1 = torch.optim.Adam({latent}, lr=eta1)\n",
    "else:\n",
    "    optimizer1 = torch.optim.Adam(model.enc.parameters(), lr=eta1)\n",
    "    \n",
    "optimizer2 = torch.optim.AdamW(model.dec.parameters(), lr=eta2, weight_decay=wd)\n",
    "\n",
    "\n",
    "reach_acc_test = False\n",
    "reach_acc_train = False\n",
    "reach_rqi = False\n",
    "\n",
    "### training ###\n",
    "\n",
    "test_acc_epochs = []\n",
    "train_acc_epochs = []\n",
    "\n",
    "for epoch in range(epochs): \n",
    "\n",
    "    optimizer1.zero_grad()\n",
    "    optimizer2.zero_grad()\n",
    "\n",
    "    # updata model parameters\n",
    "    outputs_train = model.dec(latent, inputs_id[train_id])\n",
    "    outputs_test = model.dec(latent, inputs_id[test_id])\n",
    "    loss_train = torch.mean((outputs_train-labels_train)**2)\n",
    "    loss_train.backward()\n",
    "    optimizer1.step()\n",
    "    optimizer2.step()\n",
    "\n",
    "\n",
    "    # calculate accuracy based on nearest neighbor\n",
    "    pred_train_id = torch.argmin(torch.sum((outputs_train.unsqueeze(dim=1) - y_templates.unsqueeze(dim=0))**2, dim=2), dim=1)\n",
    "    pred_test_id = torch.argmin(torch.sum((outputs_test.unsqueeze(dim=1) - y_templates.unsqueeze(dim=0))**2, dim=2), dim=1)\n",
    "    acc_nn_train = np.mean(pred_train_id.detach().numpy() == out_id_train) # training acc\n",
    "    acc_nn_test = np.mean(pred_test_id.detach().numpy() == out_id_test) # testing acc\n",
    "    acc_nn = (acc_nn_train*train_id.shape[0] + acc_nn_test*test_id.shape[0])/all_num # whole accuract\n",
    "    test_acc_epochs.append(acc_nn_test)\n",
    "    train_acc_epochs.append(acc_nn_train)\n",
    "\n",
    "    # check if accuracy reaches a threshold (grokking time)\n",
    "    if not reach_acc_train: # train\n",
    "        if acc_nn_train >= 0.9:\n",
    "            reach_acc_train = True\n",
    "            iter_train = epoch\n",
    "            \n",
    "            \n",
    "    if not reach_acc_test: # test\n",
    "        if acc_nn_test >= 0.9:\n",
    "            reach_acc_test = True\n",
    "            iter_test = epoch\n",
    "\n",
    "    # Count parallelograms in representation \n",
    "    PR = []\n",
    "    PR_id = []\n",
    "    if natural_gradient == False:\n",
    "        latent = model.enc(x_templates).clone()\n",
    "    latent_scale = latent/torch.std(latent,dim=0).unsqueeze(dim=0)\n",
    "    \n",
    "    count = 0\n",
    "    for ii in range(P0_num):\n",
    "        i, j = list(P0[ii])[0]\n",
    "        m, n = list(P0[ii])[1]\n",
    "        dist = latent_scale[i] + latent_scale[j] - latent_scale[m] - latent_scale[n]\n",
    "        if (torch.mean(dist**2)<0.01):\n",
    "            PR_id.append(ii)\n",
    "            PR.append(P0[ii])\n",
    "\n",
    "    rqi = len(PR)/P0_num\n",
    "\n",
    "    # check if RQI reaches a high threshold (time scale of representation learning)\n",
    "    if not reach_rqi:\n",
    "        if rqi > 0.99:\n",
    "            reach_rqi = True\n",
    "            iter_rqi = epoch\n",
    "        \n",
    "    # logging\n",
    "    if epoch % log == 0:\n",
    "        print(\"epoch: %d  | loss: %.8f \"%(epoch, loss_train.detach().numpy()))\n",
    "\n",
    "# if fails within compute budget      \n",
    "if not reach_acc_test:\n",
    "    iter_test = epoch\n",
    "\n",
    "if not reach_acc_train:\n",
    "    iter_train = epoch\n",
    "\n",
    "if not reach_rqi:\n",
    "    iter_rqi = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d1abe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
